"use strict";(globalThis.webpackChunkdocs_locki_io=globalThis.webpackChunkdocs_locki_io||[]).push([[6114],{8502(e){e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"meeting-ocapistaine-3","metadata":{"permalink":"/fr/blog/meeting-ocapistaine-3","editUrl":"https://github.com/locki-io/docs.locki.io/tree/main/blog/2026-01-24-catch-up-call.mdx","source":"@site/blog/2026-01-24-catch-up-call.mdx","title":"Catch-up Call: Deployment Strategy","description":"Here is the assessment of the catch-up call between @jnxmas and Victor regarding the \xd2 Capistaine project status and immediate priorities.","date":"2026-01-24T00:00:00.000Z","tags":[{"inline":false,"label":"Meeting","permalink":"/fr/blog/tags/meeting","description":"Meeting tag description"},{"inline":false,"label":"civitech","permalink":"/fr/blog/tags/civictech","description":"Citizen technologies and open source for the public good"},{"inline":false,"label":"AI and Machine Learning","permalink":"/fr/blog/tags/ai-ml","description":"Articles on AI, machine learning, and related technologies"}],"readingTime":3.61,"hasTruncateMarker":true,"authors":[{"name":"Jean-No\xebl Schilling","title":"Locki one / french maintainer","url":"https://github.com/jnschilling","socials":{"linkedin":"https://www.linkedin.com/in/jnschilling/","github":"https://github.com/jnschilling","newsletter":"https://www.stratej.fr"},"imageURL":"https://github.com/jnschilling.png","key":"jnxmas","page":null}],"frontMatter":{"slug":"meeting-ocapistaine-3","title":"Catch-up Call: Deployment Strategy","authors":["jnxmas"],"tags":["meeting","civictech","ai-ml"]},"unlisted":false,"nextItem":{"title":"Technical Strategy: Google Gemini Integration","permalink":"/fr/blog/gemini-workshop-1"}},"content":"Here is the assessment of the catch-up call between @jnxmas and Victor regarding the **\xd2 Capistaine** project status and immediate priorities.\\n\\n## **Summary of the Call**\\n\\n@jnxmas and Victor discussed the immediate roadmap for the **Opik/Commit to Change Hackathon MVP submission** (deadline: ~1 day, 14 hours). Victor has successfully downloaded approximately 4,000 PDFs (including ~3,965 deliberation documents), though he noted some potential duplicates and that the download process was synchronous and could be optimized later. He has committed changes to a development branch but not yet merged them, preferring to use GitHub as a medium to exchange the code while keeping the large PDF dataset local (or shared via a specific sub-directory).\\nThe team agreed on a strategy for the **Hackathon demo deployment**. Instead of using Vercel, which complicates environment variable management for their specific security setup (ngrok, multiple API keys for Opik, Firecrawl, Gemini, etc.), @jnxmas will run the demo from his local machine using a secure, paid ngrok tunnel (`ocapistaine.ngrok.app`). This setup allows the jury to interact with the Streamlit UI (restricted to Ollama for the external demo) while the team can continue testing other models (Gemini) locally. The architecture involves **Locki.io -> Vaettir Orchestration -> Local Machine (Ocapistaine Agent)**.\\n\\n\x3c!-- truncate --\x3e\\n\\n**Key Technical Decisions & Next Steps:**\\n\\n- **OCR & Database:** Victor is moving immediately to OCR processing. While `pdf2ocr` was discussed, they agreed that since most files are text-based PDFs (not scanned images), full image-to-text conversion might be overkill. The priority is text extraction and categorization. @jnxmas plans to implement a NoSQL database (MongoDB) to store OCR content alongside metadata (source, date) to support the RAG system.\\n- **Observability:** @jnxmas is finalizing the integration of Opik within **n8n workflows**. This ensures that if the Ocapistaine application triggers an n8n workflow involving an LLM, both systems report traces to the same Opik observability project, providing a unified view of checking the \\"Charter validity\\" and \\"Hallucination detection.\\"\\n- **Submission Prep:** @jnxmas will focus on writing the article/documentation to justify the technical choices and process for the submission, while Victor attempts to finish the OCR pipeline and potentially start the MongoDB implementation before he becomes unavailable for a few days.\\n\\n---\\n\\n## **Action Plan & Tasks**\\n\\n### **1. Data Engineering & Backend**\\n\\n- **Task:** Finalize PDF Text Extraction & OCR\\n  - **Owner:** @zcbtvag (Victor)\\n  - **Description:** Run the extraction on the downloaded ~4,000 PDFs. Focus on text-based extraction first, only using heavy OCR (image-to-text) if necessary. Commit the code logic (not the files themselves) to the dev branch.\\n  - **Deadline:** **Tonight / ASAP** (Before Victor travels).\\n  - **Success Criteria:** Text extracted from PDFs and ready for database ingestion.\\n- **Task:** Push Dev Branch Changes\\n  - **Owner:** @zcbtvag (Victor)\\n  - **Description:** Push the latest scraper and extraction logic to GitHub so @jnxmas can pull the code.\\n  - **Deadline:** **Tonight**.\\n  - **Success Criteria:** PR raised/Code available on the remote repository.\\n- **Task:** Implement MongoDB for Vector/Content Storage\\n  - **Owner:** @jnxmas [Secondary: Victor if time permits]\\n  - **Description:** Set up a NoSQL database (MongoDB) to store extracted PDF content + metadata (source URL, date, category). This is crucial for the RAG system to function efficiently.\\n  - **Deadline:** **Next 24 Hours**.\\n  - **Success Criteria:** DB instance running and successfully storing OCR output.\\n\\n### **2. Deployment & Hackathon Submission**\\n\\n- **Task:** Configure Local Demo Environment (ngrok)\\n  - **Owner:** @jnxmas\\n  - **Description:** Finalize the secure ngrok tunnel (`ocapistaine.ngrok.app`) pointing to the local Streamlit UI. Ensure the external facing demo is locked to Ollama, while internal dev builds can use Gemini.\\n  - **Deadline:** **Tomorrow Morning**.\\n  - **Success Criteria:** Live URL accessible to external users (jury) without exposing sensitive internal keys.\\n- **Task:** Draft Hackathon Submission Article\\n  - **Owner:** @jnxmas\\n  - **Description:** Write the required project description, work process, and key achievements for the hackathon platform. Focus on the \\"Charter Check\\" agent and Opik integration.\\n  - **Deadline:** **Tomorrow Mid-day**.\\n  - **Success Criteria:** Text ready for copy-paste into the submission form.\\n\\n### **3. Observability (Opik)**\\n\\n- **Task:** Verify Double-Tracing (App + N8N)\\n  - **Owner:** @jnxmas\\n  - **Description:** Confirm that Opik receives traces from both the Python app (Ocapistaine) and the n8n docker container when an LLM is triggered.\\n  - **Deadline:** **Tomorrow**.\\n  - **Success Criteria:** Unified dashboard in Opik showing traces from both sources.\\n\\n---\\n\\n### **Status Dashboard**\\n\\n- **Overall Progress:** \ud83d\udfe1 **Mixed** (Scraping done, OCR/DB pending, Deployment strategy fixed).\\n- **Open High-Priority Tasks:**\\n  1.  Run/Finish OCR on 4k PDFs (@zcbtvag).\\n  2.  Set up MongoDB for data ingestion (@jnschilling).\\n  3.  Finalize \\"Charter Check\\" Agent + Opik Tracing for Demo (@jnschilling).\\n- **Next Milestone:** **Hackathon MVP Submission** (Deadline: ~1 day 14 hours)."},{"id":"gemini-workshop-1","metadata":{"permalink":"/fr/blog/gemini-workshop-1","editUrl":"https://github.com/locki-io/docs.locki.io/tree/main/blog/2026-01-22-gemini-workshop.mdx","source":"@site/blog/2026-01-22-gemini-workshop.mdx","title":"Technical Strategy: Google Gemini Integration","description":"Context: Audierne 2026 Election Platform","date":"2026-01-22T00:00:00.000Z","tags":[{"inline":false,"label":"AI and Machine Learning","permalink":"/fr/blog/tags/ai-ml","description":"Articles on AI, machine learning, and related technologies"},{"inline":false,"label":"civitech","permalink":"/fr/blog/tags/civictech","description":"Citizen technologies and open source for the public good"},{"inline":false,"label":"RAG","permalink":"/fr/blog/tags/rag","description":"Retrieval-Augmented Generation topics"},{"inline":false,"label":"Encode hackathon","permalink":"/fr/blog/tags/encode","description":"Information from the Encode hackathon"}],"readingTime":4.54,"hasTruncateMarker":true,"authors":[{"name":"Jean-No\xebl Schilling","title":"Locki one / french maintainer","url":"https://github.com/jnschilling","socials":{"linkedin":"https://www.linkedin.com/in/jnschilling/","github":"https://github.com/jnschilling","newsletter":"https://www.stratej.fr"},"imageURL":"https://github.com/jnschilling.png","key":"jnxmas","page":null}],"frontMatter":{"slug":"gemini-workshop-1","title":"Technical Strategy: Google Gemini Integration","authors":["jnxmas"],"tags":["ai-ml","civictech","rag","encode"]},"unlisted":false,"prevItem":{"title":"Catch-up Call: Deployment Strategy","permalink":"/fr/blog/meeting-ocapistaine-3"},"nextItem":{"title":"Team Sync: Gemini Integration and Agent Workflows","permalink":"/fr/blog/meeting-ocapistaine-2"}},"content":"**Context:** Audierne 2026 Election Platform\\n\\nThis document outlines how we will leverage the Google Gemini ecosystem (AI Studio, Flash models, and Agentic workflows) to accelerate the development of the Locki project. By utilizing these tools, we aim to bridge the gap between human ideation and automated N8N workflows, specifically for the `Commit to Change` hackathon and the subsequent election period.\\n\\n\x3c!-- truncate --\x3e\\n\\n## 1. The `Human-to-Agent` Workflow Bridge\\n\\nA core strategy for our development is enabling our automated GitHub agents to `learn` from human-validated workflows. We will use Google AI Studio as the prototyping environment to define logic that is subsequently exported to our N8N instance.\\n\\n<iframe\\n  width=\\"100%\\"\\n  style={{ \\"aspect-ratio\\": \\"16 / 9\\" }}\\n  src=\\"https://www.encodeclub.com/programmes/comet-resolution-v2-hackathon/events/mastering-gemini-3-building-next-gen-ai-agents-in-google-ai-studio\\"\\n  title=\\"OPIK workshop 2\\"\\n  frameborder=\\"0\\"\\n  allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\"\\n  referrerpolicy=\\"strict-origin-when-cross-origin\\"\\n  allowfullscreen\\n></iframe>\\n\\n- **Prototyping in AI Studio:** Team members (jnxmas, etc.) can solve complex data extraction problems within the AI Studio interface (e.g., `Analyze this specific municipal PDF and extract the 2024 housing budget`).\\n- **The `Get Code` Feature:** Once the model successfully completes a task or fixes a bug, we utilize the **Get Code** feature. This provides the exact Python or cURL code required to replicate the action.\\n- **N8N Integration:** Instead of writing scrapers from scratch, we export this verified logic and integrate it directly into N8N Python nodes. This allows our automated agents to search the Audierne environment and context with logic already proven to work.\\n- **Workflow Orchestration:** We can prompt the model to stitch together disparate services (e.g., Search + Summarize + Store). The model designs the prompts and orchestrates the data flow, which we then convert into Dockerized N8N workflow nodes.\\n\\n## 2. Advanced Scraping & Contextual Search (Firecrawl Optimization)\\n\\nTo effectively crawl the ~150 links and municipal portals of Audierne, we will apply Gemini\'s `Anti-Gravity` and DOM-aware capabilities to our Firecrawl pipeline.\\n\\n- **DOM & Visual Navigation:** Unlike standard `curl` requests which may be blocked, Gemini-based logic can navigate the Document Object Model (DOM) and use visual cues (screenshots) to handle cookies, pop-ups, and complex navigation menus found on municipal sites.\\n- **Autonomous Planning:** We can equip our agents to formulate multi-step plans (Search -> Filter by `Audierne` -> Check Date -> Download PDF).\\n- **Code Repair:** Using the `Live` and `Annotate` features, developers can debug scraper errors in real-time. The model can identify logical breaks in the scraping code and suggest fixes while maintaining context.\\n\\n## 3. Multimodal Data Processing for Municipal Archives\\n\\nProcessing the dataset of 4,000+ PDFs and various multimedia links requires a high-volume, low-cost solution.\\n\\n- **Gemini Flash Models (1.5 / 3):** We will standardize on `Flash` models for the majority of our RAG pipeline. These models offer high speed and vast token windows at a fraction of the cost, making them sustainable for our project budget.\\n- **Video & Audio Analysis:** We can upload long-form video/audio recordings of Audierne municipal council meetings directly. Gemini can generate timestamps, summaries, and extract specific topic discussions (e.g., `Culture budget debates`) without requiring a separate, expensive Speech-to-Text pipeline.\\n- **Gemma 3 (Local/Open):** For local testing and zero-cost prototyping of OCR pipelines, we can utilize the open Gemma 3 model before deploying to the cloud.\\n\\n## 4. Accuracy & Sandbox Code Execution\\n\\nTo adhere to our goal of a `neutral, impartial RAG-based chatbot,` we must minimize hallucinations, especially regarding budget figures.\\n\\n- **Sandboxed Execution:** We will utilize Gemini\'s `Code Execution` tool. Instead of asking the LLM to _predict_ the sum of a budget list, we ask it to _write and run Python code_ to calculate it.\\n- **Data Visualization:** This feature allows us to ask the model to plot data (e.g., `Create a graph comparing the budget allocation of the 4 lists`) which can be displayed on the frontend.\\n- **Logic Verification:** This forces the model to use computational logic, ensuring that the comparisons provided to Audierne citizens are mathematically accurate.\\n\\n## 5. Frontend Acceleration & Multilingual Support\\n\\n- **The `Build` Feature:** For the upcoming Hackathon, we can use the `Build` feature to describe user interfaces (e.g., `Create a citizen contribution intake form with camera access`) and instantly generate deployed React/TypeScript code. This significantly reduces boilerplate work.\\n- **Gemini Live (Bilingual):** To support our EN/FR requirement, we leverage the model\'s native multilingual capabilities. It can seamlessly switch between French and English, ensuring the chatbot interacts fluently with all demographics in Audierne.\\n\\n---\\n\\n**Project Coordinator Status Update**\\n\\n- **Overall Progress:** \ud83d\udfe1 Firecrawl (Optimization via DOM-aware logic needed) \ud83d\udfe2 Docs (Gemini Strategy Added) \ud83d\udfe1 RAG (Integration of Flash models pending)\\n- **Open High-Priority Tasks:**\\n  - **Task:** Prototype PDF extraction in AI Studio & export Python code.\\n    - **Owner:** Victor / Meher\\n    - **Description:** Use AI Studio to solve a specific Audierne PDF parsing challenge, use `Get Code,` and integrate the Python script into a custom N8N node.\\n    - **Deadline:** Next call.\\n  - **Task:** Secure API keys for Gemini Flash.\\n    - **Owner:** jnxmas\\n    - **Description:** Provision keys to test cost-effective OCR and massive token processing for the 4k PDF dataset.\\n    - **Deadline:** ASAP.\\n  - **Task:** Refine Firecrawl Agents with DOM Logic.\\n    - **Owner:** Meher / jnxmas\\n    - **Description:** Use `Anti-Gravity` style logic to handle complex municipal site navigation (cookies/popups) where standard scraping fails.\\n    - **Deadline:** Hackathon Start.\\n  - **Task:** Hackathon Frontend Prototype.\\n    - **Owner:** Open (Frontend Contributor)\\n    - **Description:** Use Gemini `Build` feature to generate the base React code for the citizen contribution portal.\\n    - **Deadline:** Mid-February.\\n\\n- **Next Milestone:** Hackathon Prototype Delivery (~Mid-February)"},{"id":"meeting-ocapistaine-2","metadata":{"permalink":"/fr/blog/meeting-ocapistaine-2","editUrl":"https://github.com/locki-io/docs.locki.io/tree/main/blog/2026-01-22-team-sync.mdx","source":"@site/blog/2026-01-22-team-sync.mdx","title":"Team Sync: Gemini Integration and Agent Workflows","description":"Strategy Update: Leveraging Gemini & Agent Workflows","date":"2026-01-22T00:00:00.000Z","tags":[{"inline":false,"label":"Meeting","permalink":"/fr/blog/tags/meeting","description":"Meeting tag description"},{"inline":false,"label":"civitech","permalink":"/fr/blog/tags/civictech","description":"Citizen technologies and open source for the public good"},{"inline":false,"label":"AI and Machine Learning","permalink":"/fr/blog/tags/ai-ml","description":"Articles on AI, machine learning, and related technologies"}],"readingTime":3.33,"hasTruncateMarker":true,"authors":[{"name":"Jean-No\xebl Schilling","title":"Locki one / french maintainer","url":"https://github.com/jnschilling","socials":{"linkedin":"https://www.linkedin.com/in/jnschilling/","github":"https://github.com/jnschilling","newsletter":"https://www.stratej.fr"},"imageURL":"https://github.com/jnschilling.png","key":"jnxmas","page":null}],"frontMatter":{"slug":"meeting-ocapistaine-2","title":"Team Sync: Gemini Integration and Agent Workflows","authors":["jnxmas"],"tags":["meeting","civictech","ai-ml"]},"unlisted":false,"prevItem":{"title":"Technical Strategy: Google Gemini Integration","permalink":"/fr/blog/gemini-workshop-1"},"nextItem":{"title":"Let us choose the stack","permalink":"/fr/blog/meeting-ocapistaine-1"}},"content":"## Strategy Update: Leveraging Gemini & Agent Workflows\\n\\nThis document summarizes key insights from the recent team sync following Jean-No\xebl\'s attendance at the Google Gemini workshop. The focus is on accelerating the **Locki / \xd2 Capistaine** project by transitioning from manual human analysis to automated agent workflows.\\n\\n\x3c!-- truncate --\x3e\\n\\n### The Gemini Opportunity\\n\\njnxmas reported that despite initial skepticism, the Gemini workshop revealed significant opportunities for the project, particularly regarding scraping and rapid prototyping.\\n\\n- **Google AI Studio Capabilities:** The team discussed the power of Google AI Studio (specifically Gemini 1.5 Pro/Flash). It allows for multimodal input\u2014users can upload images or video recordings of a screen process, and the AI can generate the corresponding code (Python) to replicate that workflow.\\n- **Anti-Gravity & DOM Awareness:** The team is considering testing \\"Anti-Gravity\\" (potentially as a VS Code alternative or extension) to utilize Gemini\'s DOM-aware capabilities. This could allow for smarter scraping of the ~150 target links compared to the current \\"brute force\\" Firecrawl approach.\\n\\n### From Human Workflow to AI Agent\\n\\nA core strategic pivot discussed is the methodology for building the project\'s autonomous agents (specifically the **Ocapistaine** GitHub user/agent).\\n**The Proposed Workflow:**\\n\\n1. **Human Prototype:** A team member manually processes a citizen contribution using Gemini Studio.\\n   - _Example Action:_ Take a contribution about \\"rainwater taxes,\\" validate it against the charter, search Google for similar implementations in France, and check the `audierne2026` docs for local context.\\n2. **Code Generation:** Ask Gemini to \\"make a script of this search and interpretation process.\\"\\n3. **Agent Implementation:** This script is converted into a workflow stored in the repository.\\n4. **Execution via N8N:** The \\"Ocapistaine\\" agent (which has specific GitHub credentials) triggers these workflows. It acts as the owner of the process, running searches, documenting findings in the repo, and replying to issues using the context generated.\\n\\n### Integration Architecture: GitHub & N8N\\n\\nThe infrastructure is being updated to support this \\"Agent-driven\\" model:\\n\\n- **Cross-Repo Context:** Documentation from `docs.locki.io` is now set up as a submodule, allowing the AI to access context across different repositories (e.g., `vaettir` and `ocapistaine`) simultaneously.\\n- **Workflow Orchestration:** Python workflows within the `ocapistaine` repository are designed to call N8N workflows.\\n- **Automated Workflow Creation:** Experiments suggest that tools like Claude or Gemini can be used to write the JSON files required for N8N workflows, essentially allowing the AI to build its own integration pipelines using the project\'s existing API keys (Forseti/Ocapistaine).\\n\\n### Current Progress & Blockers\\n\\n- **Scraping (Firecrawl):** Victor has successfully scraped municipal considerations (PDFs/data), but there are file management issues. The files need to be renamed and committed to the `dev` branch properly to avoid needing complex cleanup later.\\n- **Opik Integration:** The integration is live. Jean-No\xebl demonstrated a trace where the system successfully checked the \\"contribution charter\\" and returned a confidence score and category (e.g., \\"Youth\\") using local LLMs (Ollama/Mistral) to save costs during dev.\\n- **Cost Management:** The team plans to use \\"Flash\\" models or free tiers (Gemini/Gemma) for the heavy lifting of RAG and video/audio analysis to keep the project sustainable.\\n\\n---\\n\\n### Project Coordinator Dashboard\\n\\n**Overall Progress:** \ud83d\udfe1 **Processing** (Scraping logic is working but needs cleanup; Agent workflow defined but not implemented).\\n**Open High-Priority Tasks:**\\n\\n- **Task:** Clean and Commit Scraped Data\\n  - **Owner:** @zcbtvag (Victor)\\n  - **Description:** Rename scraped municipal files/folders and commit to the `dev` branch so the team can access the dataset.\\n  - **Deadline:** ASAP\\n  - **Success Criteria:** Files visible in `dev` branch without directory errors.\\n- **Task:** Prototype \\"Human-to-Agent\\" Workflow\\n  - **Owner:** @jnxmas / @GurmeherSingh\\n  - **Description:** Record a manual analysis session in Gemini Studio, generate the Python script, and convert it into a GitHub Action/N8N trigger.\\n  - **Deadline:** Next Sync\\n  - **Success Criteria:** One functional automated workflow generated from a video/image input.\\n- **Task:** Test Anti-Gravity / Gemini DOM Scanning\\n  - **Owner:** @jnxmas\\n  - **Description:** Evaluate if Gemini\'s DOM-aware browsing is more efficient than the current Firecrawl setup for the 150 links.\\n  - **Deadline:** End of Week\\n  - **Success Criteria:** Decision on whether to switch scraping tools.\\n    **Next Milestone:**\\n- **Mid-February:** Hackathon Prototype Delivery (Functional RAG chatbot with automated contribution processing)."},{"id":"meeting-ocapistaine-1","metadata":{"permalink":"/fr/blog/meeting-ocapistaine-1","editUrl":"https://github.com/locki-io/docs.locki.io/tree/main/blog/2026-01-18-team-meeting.mdx","source":"@site/blog/2026-01-18-team-meeting.mdx","title":"Let us choose the stack","description":"Project Context Update (Mid-to-Late January 2026)","date":"2026-01-18T00:00:00.000Z","tags":[{"inline":false,"label":"Sprints","permalink":"/fr/blog/tags/sprints","description":"Articles on sprints and current status updates"},{"inline":false,"label":"civitech","permalink":"/fr/blog/tags/civictech","description":"Citizen technologies and open source for the public good"},{"inline":false,"label":"RAG","permalink":"/fr/blog/tags/rag","description":"Retrieval-Augmented Generation topics"},{"inline":false,"label":"Locki Labs","permalink":"/fr/blog/tags/locki-labs","description":"Startup adventures at Locki Labs still in formation"},{"inline":false,"label":"Meeting","permalink":"/fr/blog/tags/meeting","description":"Meeting tag description"},{"inline":false,"label":"Encode hackathon","permalink":"/fr/blog/tags/encode","description":"Information from the Encode hackathon"}],"readingTime":21.11,"hasTruncateMarker":true,"authors":[{"name":"Jean-No\xebl Schilling","title":"Locki one / french maintainer","url":"https://github.com/jnschilling","socials":{"linkedin":"https://www.linkedin.com/in/jnschilling/","github":"https://github.com/jnschilling","newsletter":"https://www.stratej.fr"},"imageURL":"https://github.com/jnschilling.png","key":"jnxmas","page":null}],"frontMatter":{"slug":"meeting-ocapistaine-1","title":"Let us choose the stack","authors":["jnxmas"],"tags":["sprints","civictech","rag","locki","meeting","encode"]},"unlisted":false,"prevItem":{"title":"Team Sync: Gemini Integration and Agent Workflows","permalink":"/fr/blog/meeting-ocapistaine-2"},"nextItem":{"title":"Lets us code","permalink":"/fr/blog/meher-jnxmas-letscode"}},"content":"## Project Context Update (Mid-to-Late January 2026)\\n\\n**Project**: Locki / \xd2 Capistaine (audierne2026) - AI-powered civic transparency & participatory democracy platform for Audierne 2026 local elections (France)\\n**Core Mission**: Build a neutral, source-based RAG chatbot to answer citizen questions about 4 municipal programs, automate contribution validation against charter rules, crawl/process municipal data (150+ links, 4,000+ PDFs), and showcase Opik integration for the \\"Commit to Change\\" Hackathon.\\n**Critical Timeline**:\\n\\n- Contributions deadline: ~January 31, 2026\\n- Hackathon prototype delivery: ~mid-February 2026 (4-week sprint)\\n- Election period: ~March 15-22, 2026\\n\\n\x3c!-- truncate --\x3e\\n\\n## Team Status & Availability\\n\\n**Active Members**:\\n\\n- **[jnxmas] aka Johnny Christmass (@jnxmas)**: Project lead, architecture, N8N deployment, Firecrawl testing, documentation, political outreach\\n- **Meher/Gurmukher/Guru (@GurmeherSingh)**: ML Engineer, RAG agent + Opik integration (recently arrived US East Coast, jet-lagged)\\n- **Victor (@zcbtvag)**: Backend Python, Firecrawl pipeline (intensive work Mon-Wed, unavailable Thu-Sun for France travel, returns following week)\\n\\n  **Team Experience Alignment**:\\n\\n- \u2705 **Shared expertise**: Streamlit, VectorStore, Python, Firecrawl, Gemini, Opik\\n- \u2705 **Guru\'s strengths**: NOMIC embeddings (Sentence Transformer), Azure, Ollama\\n- \u274c **Learning curve**: N8N (new for Guru + Victor)\\n\\n---\\n\\n## Political & Community Engagement Progress\\n\\n**Achievements**:\\n\\n- **Outgoing mayor**: Positive signal at New Year\'s event - mentioned \\"new paradigm in politics\\"\\n- **Email follow-up**: Mayor acknowledged project email (10+ days prior), interested in discussing Audierne2026\\n- **Electoral lists**: 3 of 4 lists contacted, very positive responses\\n- **Remaining**: 1 list still to contact (before Jan 31)\\n  **Current Challenge**:\\n- **Low citizen engagement**: Contributions/discussions system seeing minimal activity (\\"it\'s a bit too much right now\\")\\n- **Content workflow**: Issues validated \u2192 transferred to discussions \u2192 posted on website after community review\\n- **Validation stages**: Charter compliance + Contextualization (innovative agent finding solutions)\\n  **Strategic Shift**: Team needs to move from outreach to coding and delivery.\\n\\n---\\n\\n## Technical Architecture - Stack Decisions Finalized\\n\\n### Layer Structure (Bottom-Up ETL Pattern)\\n\\n**1. External Services Layer**\\n\\n- **Firecrawl API**: Primary web scraping/crawling service\\n- **Status**: Victor progressing, PR open, completion target Wednesday (before travel)\\n- **Blocker**: Pagination settings need tuning (tested with `max_pages=10`, returned only 1 page)\\n- **Priority source**: D\xe9lib\xe9rations du conseil municipal (municipal council deliberations)\\n  **2. Data Access Layer**\\n- **Vector Store**: Confirmed standard VectorStore approach (all team members familiar)\\n- **Embeddings Strategy**:\\n  - **Primary**: NOMIC model (Sentence Transformer) - Guru has experience\\n  - **Backup/Failsafe**: Mistral Studio - French language optimization + potential local sponsorship\\n- **Crawl Tracking Database**: SQL Lite/SQL Model chosen\\n  - Metadata: Crawl status, RAG inclusion, content category\\n  - SQL Model benefits: Type safety (Python \u2192 SQL columns), FastAPI/Pydantic compatibility, PostgreSQL support\\n  - Owner: [jnxmas] will build during Victor\'s absence\\n    **3. Business Logic Layer**\\n    **Agent 1: Charter Validation Agent** (PRIORITY)\\n- **Purpose**: Auto-validate citizen contributions against charter rules\\n- **Checks**: Respectful language, no personal naming, constructive content\\n- **Functionality**: Binary (green/red) conformance + auto-correct miscategorized submissions (7 categories)\\n- **Tech Stack**: Gemini (primary), Ollama (testing), Opik evaluation/tracing\\n- **Workflow**: Reads `audierne2026` repo issues \u2192 labels/updates in `ocapistaine` repo\\n- **Owner**: Guru (@GurmeherSingh)\\n- **Status**: Not started (blocked by Opik access + N8N deployment)\\n  **Agent 2: RAG Agent / Citizen Q&A Chatbot**\\n- **Purpose**: Answer citizen questions with neutral, contextual responses, cross-reference municipal programs\\n- **Requirements**: No hallucinations (Opik guardrails), source attribution, French language\\n- **Approach**: Start with available data, iterate as crawling progresses\\n- **Owner**: Guru (@GurmeherSingh)\\n- **Status**: Starting development, update expected in 1-2 days\\n- **Dependencies**: VectorStore populated, Charter agent outputs (optional)\\n  **Agent 3: Search Agent** (CONDITIONAL)\\n- **Decision**: Pending RAG completeness assessment\\n- **Logic**: If RAG sufficient \u2192 search agent unnecessary; if RAG insufficient \u2192 search agent as fallback\\n- **Status**: On hold pending RAG testing\\n  **Chat Service** (Already Working)\\n- **Functionality**: Multi-turn conversations with thread management (UUID per user)\\n- **Providers**: OpenAI, Ollama (Mistral, DeepSeek tested locally), Perplexity, Gemini\\n- **Features**: System message config, prompt engineering, thread history persistence, strategy selection (Charter mode, Q&A mode, etc.)\\n- **Opik Integration**: Full tracing via `workflow_chat` class\\n  **Document Service** (To Be Defined)\\n- **Proposed Purpose**:\\n  - Query documentation/source information\\n  - Scheduled re-crawling (detect new docs since last crawl)\\n  - Document categorization + metadata tagging\\n  - Change detection (compare current vs. new crawls, decide RAG updates)\\n- **Open Question**: How does this differ from Chat Service? (Clarified: Document = search/crawl management; Chat = conversational interface)\\n- **Status**: Requirements to be defined collaboratively\\n  **4. Application Layer**\\n- **Streamlit**: Primary presentation interface\\n- **FastAPI**: Routing layer\\n- **Debug Scripts**: One-click full stack launch (UV cone + Streamlit app automation)\\n- **Hot Reload**: Code modifications work without restart\\n- **Future**: Redis integration for scheduler + task memory persistence\\n  **5. External Orchestration Layer**\\n- **N8N**: Self-hosted Docker deployment on Contabo server (viety.loki.io)\\n- **Status**: ~2 hours from completion (as of last meeting)\\n- **Workflows**:\\n  - Pull issues from `audierne2026` GitHub repo\\n  - Trigger charter validation agent\\n  - Push validated/labeled issues back to repo\\n  - Modify Jekyll website content\\n  - Provide API endpoint for agent testing\\n- **Multi-project approach**: Separate workflows per project on shared dashboard\\n- **Future**: Pull data from Opik Cloud back into app (feedback loop via N8N API)\\n  **Supporting Infrastructure**:\\n- **APScheduler (cron-based)**: Time-based tasks (daily/weekly crawl updates, GitHub monitoring, Charter violation checks)\\n- **Opik**: Evaluation, tracing, prompt management (future), hallucination detection, metrics/dashboards\\n- **Poetry**: Dependency management (NOT pip) for library compatibility\\n\\n---\\n\\n## Opik Integration Strategy - MAXIMIZE USAGE\\n\\n**Team Consensus**: \\"Maximize Opik usage at every opportunity\\" - it\'s the hackathon focus.\\n**Dashboard Setup Decisions**:\\n\\n- **Environment**: Start with Opik Dev, migrate to Prod when ready\\n- **Dashboard Approach**: Single unified dashboard to monitor all agents (avoid complexity of separate dashboards per agent)\\n- **API Keys**: Likely one project with multiple keys for per-developer usage tracking\\n- **Access**:\\n  - [jnxmas] has access to `ocapistaine-dev` dashboard\\n  - Need to grant Guru and Victor access\\n  - Guru has existing Opik account (needs to share user ID/Discord username)\\n  - Victor\'s Opik status: Not confirmed\\n    **Integration Points**:\\n- Charter validation agent: Trace every decision, verify adherence to charter rules\\n- RAG agent: Hallucination prevention, prompt evaluation, guardrails\\n- N8N workflows: Plug into Opik for agent activity reporting\\n- Chat service: Already using Opik tracing via `workflow_chat` class\\n- Prompt management: Start hardcoded, potentially migrate to Opik prompt library later\\n- Evaluation judge: Verify LLM adherence to prompts\\n  **Team Familiarity**:\\n- Platform exploration phase: Team has not yet fully explored Opik capabilities\\n- Reference materials: Git examples available for implementation patterns\\n- Action needed: All team members familiarize with Opik features (dashboards, prompt library, evaluation)\\n\\n---\\n\\n## LLM Provider Strategy - Multi-Provider Approach\\n\\n**Confirmed Providers**:\\n\\n1. **Gemini**: Sponsored/free option (primary for Charter agent)\\n2. **Ollama**: Completely free (local testing) - Mistral, DeepSeek tested\\n3. **Mistral**: French language optimization + potential sponsorship target (French govt/local alignment, political risk mitigation)\\n4. **OpenAI**: Already working in Chat service\\n5. **Perplexity**: Available option\\n   **Strategic Rationale**:\\n\\n- **French language**: Critical for citizen-facing content - Mistral chosen for optimization\\n- **Political optics**: Using French LLM provider avoids criticism of not supporting French solutions\\n- **Cost management**: Prioritize free/sponsored options (Gemini, Ollama)\\n- **Failsafe approach**: NOMIC primary + Mistral backup = \\"two tracks maximum\\" ([jnxmas]\'s preference given 4-week timeline)\\n\\n---\\n\\n## Development Workflow & Version Control\\n\\n**Git Strategy**:\\n\\n- **Protected branch**: `dev` branch - no direct pushes\\n- **Feature branches**: Mandatory for all work\\n- **PR workflow**: Victor successfully resolved upstream branch issue, has open PR ready for review\\n- **Review process**: [jnxmas] reviews and merges into `dev`\\n  **Dependency Management**:\\n- **Poetry ONLY**: NOT pip - ensures library compatibility\\n- **Command**: `poetry add `\\n- **Action**: Victor + Guru ensure local environments use Poetry\\n  **Documentation**:\\n- **Docusaurus blog**: All meeting summaries posted to `blog/` for async collaboration + AI context (Cursor/Claude code context)\\n- **GitHub tasks**: Self-assignment encouraged, commenting for async feedback\\n- **Task board**: 3 tasks \\"In Progress\\" with all three team members assigned\\n\\n---\\n\\n## Critical Technical Decisions Summary\\n\\n| Component             | Decision                                        | Owner             | Status                 |\\n| --------------------- | ----------------------------------------------- | ----------------- | ---------------------- |\\n| **Vector DB**         | VectorStore (standard)                          | Team              | \u2705 Confirmed           |\\n| **Embeddings**        | NOMIC primary, Mistral backup                   | Guru              | \u2705 Confirmed           |\\n| **Orchestration**     | N8N (Docker, viety.loki.io)                     | [jnxmas]          | \ud83d\udfe1 ~2h from deployment |\\n| **Crawl Tracking**    | SQL Lite + SQL Model                            | [jnxmas]          | \ud83d\udd34 Design phase        |\\n| **LLM Providers**     | Gemini, Ollama, Mistral, OpenAI, Perplexity     | Team              | \u2705 Multi-provider      |\\n| **Opik Setup**        | Single dashboard, Dev \u2192 Prod, multiple API keys | [jnxmas]          | \ud83d\udfe1 Access pending      |\\n| **OCR Libraries**     | pdf2ocr, Tabula, pypdf (test all 3)             | Victor \u2192 [jnxmas] | \ud83d\udd34 Blocked on crawling |\\n| **Prompt Management** | Hardcoded \u2192 Opik library (future)               | Guru              | \ud83d\udd34 Not started         |\\n| **Search Agent**      | Conditional on RAG completeness                 | TBD               | \ud83d\udfe1 On hold             |\\n\\n---\\n\\n## Firecrawl Pipeline Progress (Victor\'s Focus)\\n\\n**Completed**:\\n\\n- \u2705 Debugged and fixed scraper component\\n- \u2705 Fixed crawler functionality (operational after \\"small but critical\\" config issues)\\n- \u2705 Open PR ready for review\\n- \u2705 Resolved upstream branch issue\\n  **Current Issues**:\\n- \u26a0\ufe0f Pagination settings need adjustment (tested `max_pages=10`, returned only 1 page)\\n- \u26a0\ufe0f Full document retrieval tuning required\\n  **Victor\'s Timeline**:\\n- **Mon-Wed (this week)**: Intensive work, complete Firecrawl pipeline\\n- **Thu-Sun**: Unavailable (travel to France - pre-planned holiday)\\n- **Following week**: Returns and continues contributions\\n- **Goal**: Finish Firecrawl work before Wednesday departure\\n  **Next Steps**:\\n\\n1. Victor finalizes additional commits to feature branch\\n2. Victor signals when PR ready for final review\\n3. [jnxmas] reviews and merges into `dev`\\n4. [jnxmas] tests merged pipeline on additional sources during Victor\'s absence\\n5. Once crawling complete \u2192 Apply OCR to PDFs (pdf2ocr, Tabula, pypdf testing)\\n\\n---\\n\\n## OCR Pipeline Strategy (Post-Firecrawl)\\n\\n**Target Documents**: Scanned PDFs in municipal archives (~4,000+ documents)\\n**Libraries to Test**:\\n\\n1. pdf2ocr\\n2. Tabula\\n3. pypdf\\n   **Approach**: Test all 3, compare outputs for quality\\n   **Owner**: Victor (primary), [jnxmas] (testing during Victor\'s absence)\\n   **Status**: Blocked on Firecrawl crawling completion\\n\\n---\\n\\n## Project Background & Context\\n\\n**Locki Project Evolution**:\\n\\n- Origin: Started from hackathon 2 years ago\\n- Built agents on top of Locki stack\\n- Participated in cohorts to improve functionality\\n- Previous projects:\\n  - 3D objects on-chain (paused - too early/complex, slow blockchain)\\n  - Horse racing betting strategy (15 years expert knowledge \u2192 code/data layer, not yet commercialized)\\n  - Multiverse Six / XRP experience (3D NFTs)\\n    **[jnxmas]\'s Work**:\\n- Freelancing: Apps, websites, e-commerce development\\n- Computer teacher for small children (Thursdays, 2 hours)\\n- Locki is primary passion project (\\"what makes me wake up in the morning\\")\\n- Successfully built Locki mobile app prototype\\n  **Dev Environment**:\\n- VS Code with debug scripts for one-click launch\\n- Team using Cursor IDE may need to adapt scripts\\n- Victor considering switch to VS Code for easier workflow\\n\\n---\\n\\n## Open Questions & Decisions Needed\\n\\n### For Guru (Flagged for Response):\\n\\n1. \u2705 **Vector store preference**: VectorStore confirmed, NOMIC embeddings confirmed\\n2. \u23f3 **Opik user ID/Discord username**: Needed for dashboard access grant\\n3. \u23f3 **Stack alignment review**: Read architecture blog post in repo, suggest improvements\\n4. \u23f3 **Charter agent design**: Begin development, provide update in 1-2 days\\n\\n### For Victor:\\n\\n1. \u23f3 **Opik account status**: Confirm if account exists or needs creation\\n2. \u23f3 **Firecrawl PR readiness**: Signal when ready for final review\\n3. \u23f3 **Poetry environment**: Verify local setup uses Poetry (not pip)\\n\\n### For [jnxmas]:\\n\\n1. \u23f3 **N8N deployment**: Complete final ~2 hours of setup\\n2. \u23f3 **Opik dashboard access**: Invite Guru + Victor (need usernames)\\n3. \u23f3 **Document Service requirements**: Define scope and distinction from Chat Service\\n4. \u23f3 **Crawl scheduling**: Decide frequency (daily vs. weekly) for production\\n\\n### For Team Discussion:\\n\\n1. \u23f3 **Search Agent decision**: Revisit after RAG testing complete\\n2. \u23f3 **Citizen engagement UX**: Consider simplifying contribution system if low engagement persists\\n3. \u23f3 **Effort attribution**: Finalize \\"all ores tracking template\\" for fair team contribution recognition\\n4. \u23f3 **Prompt management migration**: Timeline for moving hardcoded prompts to Opik library\\n\\n---\\n\\n## Comprehensive Task List\\n\\n### URGENT (Next 48 Hours)\\n\\n**Task 1: Complete N8N Deployment & GitHub Integration**\\n\\n- **Owner**: [jnxmas] (@[jnxmas])\\n- **Description**: Finish final ~2 hours of N8N Docker deployment on viety.loki.io, configure workflows to pull issues from `audierne2026` repo, trigger charter validation agent, push validated/labeled issues back, provide API endpoint for testing\\n- **Deadline**: Within 48 hours (before Guru needs to test agent)\\n- **Dependencies**: None (infrastructure task)\\n- **Success Criteria**: N8N accessible at viety.loki.io, test workflow successfully pulls/pushes GitHub issue, API endpoint documented\\n  **Task 2: Set Up Opik Dev Environment & Team Access**\\n- **Owner**: [jnxmas] (@[jnxmas])\\n- **Description**: Create/configure `Ocapistaine Dev` Opik project, generate separate API keys for [jnxmas]/Guru/Victor, grant dashboard access to all team members, document key assignment and usage tracking approach\\n- **Deadline**: Before Guru starts agent development (next 24 hours)\\n- **Dependencies**: Guru\'s Opik user ID/Discord username, Victor\'s Opik account confirmation\\n- **Success Criteria**: All team members can access dashboard, separate keys functional, usage trackable per developer\\n  **Task 3: Complete Firecrawl Pipeline Implementation**\\n- **Owner**: Victor (@zcbtvag)\\n- **Description**: Finalize additional commits to Firecrawl feature branch, fix pagination settings to ensure full document retrieval, test with multiple sources, signal when PR ready for final review\\n- **Deadline**: Wednesday (before France travel)\\n- **Dependencies**: Firecrawl API keys, crawler pagination tuning\\n- **Success Criteria**: PR merged without conflicts, Firecrawl successfully crawls full test dataset, documented in code\\n  **Task 4: Build Charter Validation Agent - Core Logic**\\n- **Owner**: Guru (@GurmeherSingh)\\n- **Description**: Develop charter validation agent that reads citizen contribution issues from `audierne2026` repo, validates against charter rules (respectful, no naming, constructive), auto-corrects category misclassifications (7 categories), returns binary validation + corrected category, integrates Opik evaluation at every decision point\\n- **Deadline**: Update in 1-2 days, full prototype ASAP (foundation for other workflows)\\n- **Dependencies**: Access to Opik dashboard, `audierne2026` repo read access, N8N API endpoint\\n- **Success Criteria**: Agent correctly validates 95%+ test cases, Opik traces show decision transparency, category corrections work\\n\\n### HIGH PRIORITY (This Week)\\n\\n**Task 5: Build RAG Agent with Opik Integration**\\n\\n- **Owner**: Guru (@GurmeherSingh)\\n- **Description**: Develop core RAG agent for citizen Q&A, integrate NOMIC embeddings (primary) + Mistral (backup), implement Opik for evaluation/tracing/hallucination prevention, generate neutral contextual responses with source attribution, cross-reference municipal program data\\n- **Deadline**: Update in 1-2 days, working prototype by end of week (Jan 30-31)\\n- **Dependencies**: VectorStore setup, crawled data (start with available), Opik access\\n- **Success Criteria**: Functional RAG agent with Opik tracing active, no hallucinations in test cases, proper French language responses\\n  **Task 6: Firecrawl PR Review & Merge**\\n- **Owner**: Victor (@zcbtvag) + [jnxmas] (review)\\n- **Description**: Victor signals PR ready, [jnxmas] reviews and merges into `dev` branch, test merged pipeline functionality\\n- **Deadline**: End of week (allow Victor time for improvements)\\n- **Dependencies**: Task 3 completion\\n- **Success Criteria**: PR merged, Firecrawl pipeline functional on test sources\\n  **Task 7: Test Firecrawl on Additional Sources & Build Crawl Tracking DB**\\n- **Owner**: [jnxmas] (@[jnxmas])\\n- **Description**: Intensively test Victor\'s Firecrawl code on ocapistaine repo PDFs/websites and other sources beyond current scope, begin SQL Lite/SQL Model database for tracking crawled content with metadata (crawl status, RAG inclusion, content category)\\n- **Deadline**: Sunday evening (status update)\\n- **Dependencies**: Task 6 completion (merged Firecrawl code)\\n- **Success Criteria**: Multiple sources tested successfully, tracking database prototype with crawl status and categorization functional\\n  **Task 8: Explore Opik Platform Features**\\n- **Owner**: All team members\\n- **Description**: Familiarize with Opik capabilities, dashboards, prompt library, evaluation features, tracing, metrics, experiment tracking\\n- **Deadline**: Next 2-3 days (parallel with development)\\n- **Dependencies**: Opik access granted (Task 2)\\n- **Success Criteria**: Team comfortable with platform, ready to maximize hackathon showcase, documented learnings\\n  **Task 9: Poetry Dependency Management Setup**\\n- **Owner**: Victor (@zcbtvag) + Guru (@GurmeherSingh)\\n- **Description**: Ensure local dev environments use Poetry (not pip) for all library additions, verify compatible with current workflows\\n- **Deadline**: Before next library additions\\n- **Dependencies**: Poetry installed locally\\n- **Success Criteria**: No pip-related dependency conflicts, `pyproject.toml` up to date, team using `poetry add ` exclusively\\n\\n### MEDIUM PRIORITY (Next Week)\\n\\n**Task 10: Test OCR Libraries on Scanned PDFs**\\n\\n- **Owner**: Victor (@zcbtvag) \u2192 [jnxmas] (if Victor unavailable)\\n- **Description**: Test pdf2ocr, Tabula, pypdf on sample scanned PDFs from municipal archives, compare outputs for quality, select best library/combination for production\\n- **Deadline**: Immediately after crawling complete (or during Victor\'s absence)\\n- **Dependencies**: Task 3/6 completion (crawling functional)\\n- **Success Criteria**: 3 libraries tested, quality comparison documented, production OCR library selected\\n  **Task 11: Architecture Documentation Review**\\n- **Owner**: Guru (@GurmeherSingh) + Victor (@zcbtvag)\\n- **Description**: Read architecture blog post [jnxmas] created (in `ocapistaine` repo), suggest additions specifically around NOMIC embedding implementation details, RAG service integration points, any conflicts/improvements from experience\\n- **Deadline**: Next 2-3 days (before architecture freeze)\\n- **Dependencies**: Access to blog in repo\\n- **Success Criteria**: Comments/suggestions posted, team aligned on architecture\\n  **Task 12: Define Document Service Requirements**\\n- **Owner**: [jnxmas] + Guru + Victor\\n- **Description**: Collaboratively define Document Service scope: query documentation/source info, scheduled re-crawling (detect new docs), document categorization + metadata tagging, change detection logic (compare current vs. new crawls), decide if separate DB needed, clarify distinction from Chat Service\\n- **Deadline**: Next sync meeting\\n- **Dependencies**: Vector store decision (confirmed), crawl tracking DB design (Task 7)\\n- **Success Criteria**: Document Service requirements documented, team aligned, ready for implementation\\n  **Task 13: Finish All Ores Tracking Template**\\n- **Owner**: [jnxmas] (@[jnxmas])\\n- **Description**: Complete effort attribution tracking system for fair team contribution recognition, share with team for feedback\\n- **Deadline**: ASAP (ongoing discussion on Discord)\\n- **Dependencies**: None\\n- **Success Criteria**: Template finalized, shared on Discord, team feedback incorporated\\n  **Task 14: Follow Up with Remaining Electoral List**\\n- **Owner**: [jnxmas] (@[jnxmas])\\n- **Description**: Contact 4th electoral list for outreach, gauge interest in Audierne2026 project\\n- **Deadline**: Before January 31 (contributions deadline)\\n- **Dependencies**: None\\n- **Success Criteria**: All 4 lists contacted, responses documented\\n\\n### FUTURE / CONDITIONAL\\n\\n**Task 15: Search Agent Design & Implementation** (CONDITIONAL)\\n\\n- **Owner**: TBD (likely Guru)\\n- **Description**: IF RAG agent proves insufficient during testing, design and implement Search Agent as fallback for citizen queries\\n- **Deadline**: Week 2 of sprint (after RAG testing complete)\\n- **Dependencies**: Task 5 completion (RAG agent functional), RAG sufficiency evaluation\\n- **Success Criteria**: Decision documented (Search Agent needed: yes/no), if yes \u2192 design document with Opik integration plan\\n  **Task 16: Migrate Prompts to Opik Library** (FUTURE)\\n- **Owner**: Guru (@GurmeherSingh)\\n- **Description**: After initial hardcoded prompts working, migrate to Opik prompt library for centralized management, versioning, experimentation\\n- **Deadline**: Post-hackathon or if time permits in sprint\\n- **Dependencies**: Opik platform familiarity (Task 8), working agents with hardcoded prompts\\n- **Success Criteria**: Prompts managed in Opik, agents use library-based prompts, versioning functional\\n  **Task 17: Redis Integration for Scheduler + Task Memory**\\n- **Owner**: [jnxmas] (@[jnxmas])\\n- **Description**: Integrate Redis for APScheduler persistence and task memory across restarts\\n- **Deadline**: Post-initial prototype (when scaling/production readiness needed)\\n- **Dependencies**: Working scheduler tasks (crawl updates, GitHub monitoring)\\n- **Success Criteria**: Scheduler tasks persist across app restarts, task memory functional\\n  **Task 18: Citizen Q&A RAG Agent - Planning** (After Charter Agent)\\n- **Owner**: Guru (@GurmeherSingh)\\n- **Description**: After charter agent working, design second RAG agent iteration for citizen questions: reads validated contributions from charter agent, generates neutral contextual responses, cross-references municipal program data, no hallucinations (Opik guardrails)\\n- **Deadline**: Week 2 of sprint (after charter agent complete)\\n- **Dependencies**: Charter agent functional (Task 4), N8N workflows ready, VectorStore populated\\n- **Success Criteria**: Design document with Opik integration plan, data flow diagram\\n\\n---\\n\\n## Risk Assessment & Mitigations\\n\\n| Risk                                                            | Impact   | Mitigation                                                                                                            | Owner             | Status                           |\\n| --------------------------------------------------------------- | -------- | --------------------------------------------------------------------------------------------------------------------- | ----------------- | -------------------------------- |\\n| **Guru missing from stack discussion**                          | Medium   | Record meetings, request written feedback on tasks, schedule follow-up calls                                          | [jnxmas]          | \ud83d\udfe2 Mitigated (recordings shared) |\\n| **Victor unavailable Thu-Sun**                                  | Medium   | Victor completes Firecrawl by Wed, [jnxmas] tests during absence                                                      | Victor + [jnxmas] | \ud83d\udfe2 Planned                       |\\n| **Crawler pagination not returning full results**               | High     | Victor debugging this week; if blocked, escalate to team                                                              | Victor            | \ud83d\udfe1 In progress                   |\\n| **N8N deployment blocking agent testing**                       | High     | [jnxmas] prioritizes ~2h completion ASAP                                                                              | [jnxmas]          | \ud83d\udfe1 In progress                   |\\n| **Opik access blocking Guru\'s development**                     | High     | [jnxmas] grants access within 24h (need Guru\'s username)                                                              | [jnxmas]          | \ud83d\udfe1 In progress                   |\\n| **Vector store decision blocking RAG**                          | Medium   | VectorStore + NOMIC confirmed; fallback = Mistral Studio                                                              | Guru + [jnxmas]   | \ud83d\udfe2 Resolved                      |\\n| **OCR quality unknown for scanned PDFs**                        | Medium   | Test 3 libraries (pdf2ocr, Tabula, pypdf), compare outputs                                                            | Victor \u2192 [jnxmas] | \ud83d\udfe1 Planned                       |\\n| **4-week sprint timeline pressure**                             | High     | Prioritize working prototype over perfection, ruthless task board management, limit to \\"two tracks\\" (NOMIC + Mistral) | All               | \ud83d\udfe1 Ongoing                       |\\n| **First agent not live before contributions deadline (~Feb 1)** | Critical | Fast-track Charter agent development this week, daily standups                                                        | Guru + [jnxmas]   | \ud83d\udd34 Active risk                   |\\n| **Low citizen engagement with contributions**                   | Medium   | Consider UX simplification, focus on chatbot quality over contribution volume                                         | [jnxmas]          | \ud83d\udfe1 Monitoring                    |\\n| **Team still in ideation phase despite deadline pressure**      | Medium   | Shift to execution mode immediately, lock architecture decisions                                                      | All               | \ud83d\udfe1 Transitioning                 |\\n| **French language hallucinations/bias in chatbot**              | Critical | Mistral backup, heavy Opik guardrails, manual testing in French                                                       | Guru + [jnxmas]   | \ud83d\udfe1 Design phase                  |\\n| **Hackathon Opik showcase insufficient**                        | Medium   | \\"Maximize Opik at every opportunity\\", document integration patterns, trace everything                                 | Guru              | \ud83d\udfe1 In progress                   |\\n\\n---\\n\\n## Alignment Confirmations & Strategic Notes\\n\\n**\u2705 Strong Alignments**:\\n\\n- Opik-first approach aligns perfectly with hackathon goals\\n- Multi-provider LLM strategy provides resilience and cost optimization\\n- French language focus (Mistral) addresses political optics and user needs\\n- Modular ETL architecture enables swapping components without rewrites\\n- Team expertise (NOMIC, Streamlit, VectorStore) matches stack decisions\\n- Task ownership clear and distributed well (Guru = agents, Victor = crawling, [jnxmas] = infrastructure/orchestration)\\n  **\u26a0\ufe0f Potential Misalignments**:\\n- **Timeline vs. scope**: 4 weeks for full pipeline (crawling + OCR + RAG + Charter agent + chatbot + Opik showcase) is aggressive\\n  - **Mitigation**: Limit to \\"two tracks\\" (NOMIC primary, Mistral backup), prioritize Charter agent + basic RAG over perfect Search Agent\\n- **Citizen engagement vs. effort**: Low contribution volume may not justify complex validation workflow\\n  - **Mitigation**: Focus on chatbot quality (broader citizen reach) over contribution automation\\n- **Team availability gaps**: Victor\'s Thu-Sun absence, Guru\'s jet lag, [jnxmas]\'s teaching commitment\\n  - **Mitigation**: Strong async collaboration (recordings, blog summaries, GitHub comments), clear handoffs\\n    **\ud83d\udd0d Clarifications Needed**:\\n- **Document Service vs. Chat Service**: Distinction still fuzzy (addressed as: Document = search/crawl management; Chat = conversational interface, but needs formal requirements doc)\\n- **Search Agent necessity**: Conditional decision pending RAG testing (good pragmatic approach)\\n- **Opik dashboard structure**: Single unified dashboard confirmed, but team needs to explore platform features to maximize value\\n  **\ud83d\udcc8 Recommended Focus Areas**:\\n\\n1. **This week**: Charter agent + RAG agent + Firecrawl completion + Opik access = critical path\\n2. **Next week**: OCR testing + Document Service definition + crawl tracking DB = data pipeline completion\\n3. **Week 3-4**: Integration, testing, French language validation, Opik showcase polish, citizen-facing chatbot MVP\\n\\n---\\n\\n## Documentation & Knowledge Management\\n\\n**Current Practices** (Strong):\\n\\n- \u2705 Docusaurus blog: Meeting summaries posted to `blog/` for async collaboration\\n- \u2705 GitHub tasks: Self-assignment, commenting for feedback\\n- \u2705 Task board: 3 tasks \\"In Progress\\" with assignments\\n- \u2705 Architecture blog post: Created for team review + AI context (Cursor/Claude)\\n- \u2705 Meeting recordings: Shared for async team members (Guru)\\n- \u2705 \\"Tell what we do and do what we tell\\": Commitment tracking across conversations\\n  **Improvements Suggested**:\\n- \ud83d\udcdd Document Opik integration patterns as you build (hackathon showcase value)\\n- \ud83d\udcdd Create crawl tracking DB schema diagram (avoid ambiguity)\\n- \ud83d\udcdd Formalize Document Service requirements doc (per Task 12)\\n- \ud83d\udcdd Keep citizen-facing deliverable timeline visible (dashboard/Gantt chart)\\n- \ud83d\udcdd Document OCR library comparison results (Task 10)\\n\\n---\\n\\n## Next Sync Meeting Agenda (Suggested)\\n\\n1. **N8N + Opik access**: Confirm both unblocked ([jnxmas] updates)\\n2. **Firecrawl PR status**: Review and merge if ready (Victor + [jnxmas])\\n3. **Charter agent progress**: Demo initial prototype, discuss Opik traces (Guru)\\n4. **RAG agent update**: Share approach, blockers, timeline (Guru)\\n5. **Crawl tracking DB**: Review schema design ([jnxmas])\\n6. **Document Service requirements**: Collaborative definition session (All)\\n7. **Task board review**: Close completed tasks, reprioritize open items (All)\\n8. **Opik showcase strategy**: What metrics/traces will impress hackathon judges? (All)\\n\\n---\\n\\n## Status Dashboard\\n\\n**Overall Progress**:\\n\\n- \ud83d\udfe2 **Architecture**: Finalized (VectorStore, NOMIC, Mistral, N8N, Opik strategy confirmed)\\n- \ud83d\udfe1 **Firecrawl Pipeline**: Victor progressing, PR open, completion target Wed\\n- \ud83d\udfe1 **N8N Deployment**: ~2 hours from completion (infrastructure blocker)\\n- \ud83d\udfe1 **Opik Setup**: Account exists, team access pending (need usernames)\\n- \ud83d\udd34 **Charter Agent**: Not started (blocked by Opik access + N8N)\\n- \ud83d\udfe1 **RAG Agent**: Starting development, update expected 1-2 days\\n- \ud83d\udd34 **OCR Pipeline**: Blocked on crawling completion\\n- \ud83d\udd34 **Crawl Tracking DB**: Design phase\\n- \ud83d\udfe2 **Team Coordination**: Strong communication, clear roles, async collaboration working\\n- \ud83d\udfe1 **Political Outreach**: 3/4 lists contacted, positive signals\\n- \ud83d\udd34 **Citizen Engagement**: Low contribution volume (UX consideration needed)\\n- \ud83d\udfe2 **Documentation**: Blog summaries active, recordings shared, task tracking functional\\n  **Open High-Priority Tasks** (Top 5):\\n\\n1. **\ud83d\udd34 URGENT**: Complete N8N deployment ([jnxmas], &lt;48h) - blocks agent testing\\n2. **\ud83d\udd34 URGENT**: Set up Opik team access ([jnxmas], &lt;24h) - blocks Guru\'s development\\n3. **\ud83d\udd34 URGENT**: Complete Firecrawl pipeline (Victor, by Wed) - data ingestion dependency\\n4. **\ud83d\udd34 HIGH**: Build Charter validation agent (Guru, update 1-2 days) - critical path for hackathon\\n5. **\ud83d\udd34 HIGH**: Build RAG agent with Opik (Guru, update 1-2 days) - citizen-facing deliverable\\n   **Next Milestones**:\\n\\n- **Week 1 Goal (by ~Jan 30-31)**: Charter validation agent functional with Opik tracing + Firecrawl pipeline merged + N8N orchestrating GitHub workflows + RAG agent prototype\\n- **Immediate Blockers**: N8N deployment (&lt;48h), Opik access (&lt;24h), Firecrawl pagination tuning (&lt;3 days)\\n- **Hackathon Prototype Delivery**: ~mid-February 2026 (2.5 weeks remaining from late Jan)\\n- **Critical Path**: Crawling \u2192 OCR \u2192 Vector DB \u2192 RAG + Opik \u2192 Chatbot MVP \u2192 French language validation\\n  **Team Availability This Week**:\\n- **Guru**: Available now (jet-lagged but active), development starting\\n- **Victor**: Intensive Mon-Wed, unavailable Thu-Sun (France travel)\\n- **[jnxmas]**: Full availability, prioritizing N8N/Opik/testing during Victor\'s absence\\n  **Risk Level**: \ud83d\udfe1 **MEDIUM-HIGH** - Timeline tight, first agent not yet live despite approaching contributions deadline, but team aligned and architecture solid. Immediate focus on unblocking N8N + Opik access will shift to \ud83d\udfe2 GREEN if completed within 48h.\\n\\n---\\n\\n## Action Summary - What Happens Next\\n\\n**Immediate (Next 24 Hours)**:\\n\\n1. [jnxmas] completes N8N deployment (~2 hours)\\n2. [jnxmas] sets up Opik team access (needs Guru + Victor usernames via Discord)\\n3. Guru shares Opik user ID/Discord username\\n4. Victor continues Firecrawl pagination debugging\\n5. Team explores Opik platform features once access granted\\n\\n**This Week (Next 3-7 Days)**:\\n\\n1. Victor completes Firecrawl PR by Wednesday\\n2. [jnxmas] reviews and merges Firecrawl PR\\n3. Guru develops Charter validation agent, provides update in 1-2 days\\n4. Guru develops RAG agent prototype, provides update in 1-2 days\\n5. [jnxmas] tests Firecrawl on additional sources during Victor\'s absence\\n6. [jnxmas] begins crawl tracking DB with SQL Model\\n7. Guru + Victor review architecture blog post, suggest improvements\\n8. All team members familiarize with Opik capabilities\\n\\n**Next Week (Week 2 of Sprint)**:\\n\\n1. OCR library testing begins (Victor returns, or [jnxmas] covers)\\n2. Document Service requirements defined collaboratively\\n3. RAG agent fully functional with Opik guardrails\\n4. Charter agent deployed to N8N workflows\\n5. Crawl tracking DB prototype operational\\n6. Architecture frozen, team shifts to integration phase\\n\\n**Week 3-4 (Final Sprint)**:\\n\\n1. Full pipeline integration testing\\n2. French language validation and bias checking\\n3. Citizen-facing chatbot MVP deployment\\n4. Opik showcase polish (metrics, traces, dashboards, experiment tracking)\\n5. Hackathon submission preparation\\n6. Final outreach to 4th electoral list\\n7. Documentation finalization\\n\\n---\\n\\n**Confidence Level**: \ud83d\udfe1 **CAUTIOUSLY OPTIMISTIC** - Strong team, solid architecture, clear tasks, but timeline aggressive and first agent not yet live. Success depends on unblocking N8N + Opik access within 48h and Charter agent prototype within 1 week. Victor\'s Wed deadline and Guru\'s jet lag are manageable with current plans.\\n**Recommendation**: Daily async standups (Discord) this week to track Charter agent + Firecrawl progress. Consider lightweight demo/review session Friday to validate week 1 progress before weekend."},{"id":"meher-jnxmas-letscode","metadata":{"permalink":"/fr/blog/meher-jnxmas-letscode","editUrl":"https://github.com/locki-io/docs.locki.io/tree/main/blog/2026-01-16-meher-jnxmas.mdx","source":"@site/blog/2026-01-16-meher-jnxmas.mdx","title":"Lets us code","description":"Project Overview & Vision","date":"2026-01-16T00:00:00.000Z","tags":[{"inline":false,"label":"Encode hackathon","permalink":"/fr/blog/tags/encode","description":"Information from the Encode hackathon"},{"inline":false,"label":"civitech","permalink":"/fr/blog/tags/civictech","description":"Citizen technologies and open source for the public good"}],"readingTime":2.28,"hasTruncateMarker":true,"authors":[{"name":"Jean-No\xebl Schilling","title":"Locki one / french maintainer","url":"https://github.com/jnschilling","socials":{"linkedin":"https://www.linkedin.com/in/jnschilling/","github":"https://github.com/jnschilling","newsletter":"https://www.stratej.fr"},"imageURL":"https://github.com/jnschilling.png","key":"jnxmas","page":null}],"frontMatter":{"slug":"meher-jnxmas-letscode","title":"Lets us code","authors":["jnxmas"],"tags":["encode","civictech"]},"unlisted":false,"prevItem":{"title":"Let us choose the stack","permalink":"/fr/blog/meeting-ocapistaine-1"},"nextItem":{"title":"\xd2 Capistaine Kick-off","permalink":"/fr/blog/ocapistaine-hackathon-kickoff"}},"content":"### Project Overview & Vision\\n\\n- The core idea is building an **AI-powered civic transparency platform** focused on local democracy, citizen engagement, and real-world issues (starting with inspiration from India, but with potential global scalability).\\n- Goal: Create a standardized, bullshit-free alternative to Twitter/X discussions for civic topics \u2014 avoiding noise and enabling structured transparency for citizens, investors, and \\"democracy islands\\" (e.g., places like Audierne).\\n- It\'s tied to the **\\"Commit to Change\\" AI Agents Hackathon** (powered by Opik / Comet), running ~4 weeks starting mid-January 2026, with categories like Community Impact.\\n\\n\x3c!-- truncate --\x3e\\n\\n### Hackathon Focus & Judging Criteria Emphasis\\n\\n- Heavy emphasis on **Opik integration** for observability, evaluation, metrics, dashboards, experiment tracking, and improving LLM/agent quality.\\n- Key criteria to nail: Functionality, real-world relevance (New Year\u2019s/civic goals), effective LLM/agent use, robust evaluation/monitoring, and deep Opik workflow integration (not just fluff).\\n- Plan: Automate Opik feedback into the platform for a \\"virtuous circle\\" of optimization. Showcase Opik at every stage (dev workflow, runtime monitoring, etc.).\\n- Strategy: Prioritize smooth, meaningful Opik use \u2192 even if some parts feel like \\"fluff,\\" lean into it for judging scores.\\n\\n### Team & Collaboration Setup\\n\\n- You created/invited Meher to the project using team join code: **0e10f89d** (valid until Jan 17, 2026).\\n- Repo: **Ocapistaine** (on GitHub) \u2014 you updated it with:\\n  - Clear license\\n  - Collaboration addendum / agreements\\n  - Docusaurus as submodule for docs\\n  - Project board with tasks (including Opik-specific ones)\\n- You proposed a fair structure for handling prize money / motivation (Meher called it \\"very clean\\" and liked it).\\n- Meher was pending \u2192 accepted invite; you both coordinated joining the hackathon portal/team.\\n- Another collaborator (Vic) self-assigned tasks.\\n- You shared the kickoff doc: https://docs.locki.io/blog/ocapistaine-hackathon-kickoff (titled \\"\xd2 Capistaine Kick-off | AI-Powered Civic Transparency for Local Democracy\\").\\n\\n### Monetization & Long-Term Ideas\\n\\n- You have ideas for monetizing / turning it into a full startup (trust-based, you\'ll share more).\\n- Potential global expansion discussed, but start focused (e.g., India as strong testbed).\\n- References to past hackathon success (you + Satish, 4th place with on-chain 3D objects + AI chatbot).\\n- Avoid overkill like Decidim platforms \u2014 keep it practical and thrilling.\\n\\n### Recent Status & Next Steps\\n\\n- Meher was occasionally busy/catching up (dinner, out, etc.) but engaged.\\n- As of last messages (~Jan 15\u201316, 2026):\\n  - Everyone\'s in the team/repo.\\n  - Ready to start **coding** (both excited: \\"tickles in fingers\\").\\n  - Emphasis on transparent task assignment (\\"tell what we do and do what we tell\\").\\n  - Call happened in Ocapistaine channel.\\n  - Brainstorming better Opik showcases ongoing.\\n\\n**Overall vibe**: Enthusiastic, trusting partnership (\\"I trust you!\\"). You\'re leading repo/docs/setup/strategy; Meher is on board, catching up, and eager to code + highlight Opik. Project is now properly set up and ready to build \u2014 focus on delivering a functional, Opik-heavy prototype for the hackathon deadline."},{"id":"ocapistaine-hackathon-kickoff","metadata":{"permalink":"/fr/blog/ocapistaine-hackathon-kickoff","editUrl":"https://github.com/locki-io/docs.locki.io/tree/main/blog/2026-01-15-let-the-journey-begin.mdx","source":"@site/blog/2026-01-15-let-the-journey-begin.mdx","title":"\xd2 Capistaine Kick-off","description":"AI-Powered Civic Transparency for Local Democracy","date":"2026-01-15T00:00:00.000Z","tags":[{"inline":false,"label":"Encode hackathon","permalink":"/fr/blog/tags/encode","description":"Information from the Encode hackathon"},{"inline":false,"label":"civitech","permalink":"/fr/blog/tags/civictech","description":"Citizen technologies and open source for the public good"}],"readingTime":2.17,"hasTruncateMarker":true,"authors":[{"name":"Jean-No\xebl Schilling","title":"Locki one / french maintainer","url":"https://github.com/jnschilling","socials":{"linkedin":"https://www.linkedin.com/in/jnschilling/","github":"https://github.com/jnschilling","newsletter":"https://www.stratej.fr"},"imageURL":"https://github.com/jnschilling.png","key":"jnxmas","page":null}],"frontMatter":{"slug":"ocapistaine-hackathon-kickoff","title":"\xd2 Capistaine Kick-off","authors":["jnxmas"],"tags":["encode","civictech"]},"unlisted":false,"prevItem":{"title":"Lets us code","permalink":"/fr/blog/meher-jnxmas-letscode"},"nextItem":{"title":"OPIK : Agent & Prompt Optimization for LLM Systems","permalink":"/fr/blog/opik-workshop-2"}},"content":"**AI-Powered Civic Transparency for Local Democracy**\\n\\n## My 2026 Resolution\\n\\n:::tip The Promise\\nThis year, I will finally understand my local elections and get involved as a citizen.\\n:::\\n\\nSound familiar? Every election cycle, millions of citizens want to participate but face the same wall: scattered documents, administrative jargon, and no time to dig through years of municipal decisions.\\n\\n**This January, I stopped just wishing \u2014 and started building.**\\n\\n\x3c!-- truncate --\x3e\\n\\n## What We\'re Building\\n\\n\xd2 Capistaine is my answer to the civic engagement resolution we\'ve all made (and broken). It\'s an AI-powered transparency tool that:\\n\\n| Feature               | Description                                               |\\n| --------------------- | --------------------------------------------------------- |\\n| **Document Crawling** | Index 4,000+ municipal documents with Firecrawl + OCR     |\\n| **Citizen Q&A**       | Answer questions in plain language via RAG                |\\n| **Quality Tracking**  | Monitor LLM accuracy with Opik (hallucination detection)  |\\n| **Multi-Channel**     | N8N workflows for Facebook, email, and chatbot engagement |\\n\\n:::info Live Platform\\nSupporting [audierne2026.fr](https://audierne2026.fr) \u2014 a participatory democracy platform where citizens co-create the 2026 municipal program.\\n:::\\n\\n## Progress Update (Checkpoint 1)\\n\\n### Status Overview\\n\\n| Component              | Status | Details                                                        |\\n| ---------------------- | :----: | -------------------------------------------------------------- |\\n| **audierne2026.fr**    |   \u2705   | Jekyll platform live, citizens contributing                    |\\n| **Document Corpus**    |   \ud83d\udfe1   | 42 Gwaien bulletins collected, 4,000+ arr\xeat\xe9s identified       |\\n| **Firecrawl Pipeline** |   \ud83d\udd34   | Infrastructure designed, crawling not yet operational          |\\n| **Opik Integration**   |   \ud83d\udfe1   | Tracing architecture planned, awaiting RAG implementation      |\\n| **N8N Workflows**      |   \ud83d\udfe1   | Vaettir repo created, FB/email integration designed            |\\n| **RAG System**         |   \ud83d\udd34   | Vector store + retrieval pipeline pending                      |\\n| **Documentation**      |   \u2705   | [docs.locki.io](https://docs.locki.io) live with methodologies |\\n\\n### What\'s Working\\n\\n- Live participation platform with real citizen contributions\\n- Dual-license structure (Apache 2.0 + ELv2) for open collaboration\\n- Bilingual documentation (EN/FR)\\n- Project planning and task tracking on GitHub\\n\\n### Next Steps\\n\\n1. **Fix Firecrawl pipeline** \u2014 Get municipal document crawling operational\\n2. **Deploy Opik tracing** \u2014 LLM observability from day one\\n3. **Build RAG retrieval** \u2014 With hallucination guardrails\\n4. **Launch citizen Q&A** \u2014 First chatbot interactions\\n\\n## Hackathon Tracks\\n\\n| Track                         | Why We Qualify                                                 |\\n| ----------------------------- | -------------------------------------------------------------- |\\n| **Social & Community Impact** | Civic transparency tool enabling local democracy participation |\\n| **Best Use of Opik**          | LLM-as-judge evaluations + tracing for RAG quality assurance   |\\n\\n:::note Democracy Can\'t Afford Hallucinations\\nWhen citizens ask \\"What happened with the school budget?\\", the answer must be accurate and sourced. Opik helps us guarantee that.\\n:::\\n\\n## Team\\n\\n| Name                      | Role                          | GitHub                                             |\\n| ------------------------- | ----------------------------- | -------------------------------------------------- |\\n| Jean-No\xebl Schilling       | Project Lead / Backend        | [@jnschilling](https://github.com/jnschilling)     |\\n| Victor A                  | Backend Python                | [@zcbtvag](https://github.com/zcbtvag)             |\\n| GurmeherSingh             | ML Engineer                   | [@GurmeherSingh](https://github.com/GurmeherSingh) |\\n| _(open for contributors)_ | Frontend / UX / Communication | \u2014                                                  |\\n\\n## Links\\n\\n| Resource      | URL                                                             | Public/private |\\n| ------------- | --------------------------------------------------------------- | -------------- |\\n| Live Platform | [audierne2026.fr](https://audierne2026.fr)                      | public         |\\n| Documentation | [docs.locki.io](https://docs.locki.io)                          | public         |\\n| GitHub        | [locki-io/ocapistaine](https://github.com/locki-io/ocapistaine) | private ATM    |\\n| Project Board | [GitHub Projects](https://github.com/orgs/locki-io/projects/2)  | public         |\\n\\n---\\n\\n_If AI can help us keep our New Year\'s resolutions, maybe the most impactful one is: becoming a better citizen._"},{"id":"opik-workshop-2","metadata":{"permalink":"/fr/blog/opik-workshop-2","editUrl":"https://github.com/locki-io/docs.locki.io/tree/main/blog/2026-01-15-opik-workshop-2.mdx","source":"@site/blog/2026-01-15-opik-workshop-2.mdx","title":"OPIK : Agent & Prompt Optimization for LLM Systems","description":"This training consolidates the operational and technical foundations needed to run and execute agent/prompt optimization in team settings (e.g., hackathons and internal workshops).","date":"2026-01-15T00:00:00.000Z","tags":[{"inline":false,"label":"AI and Machine Learning","permalink":"/fr/blog/tags/ai-ml","description":"Articles on AI, machine learning, and related technologies"},{"inline":false,"label":"RAG","permalink":"/fr/blog/tags/rag","description":"Retrieval-Augmented Generation topics"},{"inline":false,"label":"Encode hackathon","permalink":"/fr/blog/tags/encode","description":"Information from the Encode hackathon"}],"readingTime":15.53,"hasTruncateMarker":true,"authors":[{"name":"Jean-No\xebl Schilling","title":"Locki one / french maintainer","url":"https://github.com/jnschilling","socials":{"linkedin":"https://www.linkedin.com/in/jnschilling/","github":"https://github.com/jnschilling","newsletter":"https://www.stratej.fr"},"imageURL":"https://github.com/jnschilling.png","key":"jnxmas","page":null}],"frontMatter":{"slug":"opik-workshop-2","title":"OPIK : Agent & Prompt Optimization for LLM Systems","authors":["jnxmas"],"tags":["ai-ml","rag","encode"]},"unlisted":false,"prevItem":{"title":"\xd2 Capistaine Kick-off","permalink":"/fr/blog/ocapistaine-hackathon-kickoff"},"nextItem":{"title":"OPIK : AI Evaluation and Observability","permalink":"/fr/blog/opik-workshop-1"}},"content":"This training consolidates the operational and technical foundations needed to run and execute **agent/prompt optimization** in team settings (e.g., hackathons and internal workshops).\\n\\nIt includes :\\n\\n- **eval-driven optimization** of LLM agent prompts using measurable metrics and iterative loops,\\n- including **meta-prompting**, **genetic/evolutionary methods**, **hierarchical/reflective optimizers (HRPO)**, **few-shot Bayesian selection**, and **parameter tuning**.\\n\\n\x3c!-- truncate --\x3e\\n\\n:::tip\\nIt is important because prompt iteration without datasets and metrics devolves into subjective \u201cdoom wordsmithing,\u201d leading to unreliable, expensive, and non-reproducible agents.\\n:::\\n\\nThis material applies to **AI/ML engineers, LLM practitioners, platform and DevEx teams, facilitators/mentors**, and anyone responsible for building and improving **tool-using agents (RAG/MCP), multimodal agents, or production chatbots** under constraints of **speed, accuracy, and cost**.\\n\\n<iframe\\n  width=\\"100%\\"\\n  style={{ \\"aspect-ratio\\": \\"16 / 9\\" }}\\n  src=\\"https://www.encodeclub.com/programmes/comet-resolution-v2-hackathon/events/agent-optimization-with-opik\\"\\n  title=\\"OPIK workshop 2\\"\\n  frameborder=\\"0\\"\\n  allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\"\\n  referrerpolicy=\\"strict-origin-when-cross-origin\\"\\n  allowfullscreen\\n></iframe>\\n\\n## 1) Development of the Content (Chronological and Logical Sequence)\\n\\n### A. Concepts Fundamental\\n\\n#### A1) Workshop mechanics for \u201cagent optimization\u201d sessions (hackathon-ready)\\n\\nIn interactive workshops, success depends on **facilitation + operational readiness** as much as technical depth.\\n\\n- **Live attendance** enables:\\n  - Real-time **Q&A**\\n  - Higher engagement in hands-on segments\\n  - Faster alignment on team objectives and constraints\\n- **Recording** is essential for:\\n  - Participants across incompatible time zones\\n  - Rewatching technical steps and setup instructions later\\n- **Hands-on sessions** increase operational sensitivity:\\n  - Frequent tool switching\\n  - Screen sharing and multi-monitor complexity\\n  - High cognitive load for participants following along\\n    **Conceptual grounding before coding**\\n- Start with a short theory block to align terminology, scope, and outcomes.\\n- Prevents early derailment into deep technical detail without a shared baseline.\\n  **Analogy**\\n- Conceptual grounding is like agreeing on a **map legend** before navigating; without it, people interpret instructions differently even if they follow the same steps.\\n\\n---\\n\\n#### A2) The optimization target: prompts, context, demonstrations, and parameters\\n\\n\u201cPrompt optimization\u201d is broader than rewriting a single system message. In practice, teams optimize multiple levers:\\n\\n- **System prompt / developer prompt**: core behavior, constraints, safety, formatting.\\n- **Context engineering**: retrieval and tools (RAG, MCP servers/clients), memory, long-context strategies.\\n- **Intent engineering**: define \u201cwhat good looks like\u201d using examples of desired conversations/outputs, then optimize toward them.\\n- **Few-shot demonstrations**: which examples to include, how many, and in what order.\\n- **Sampling parameters**: temperature, top_p, top_k (reduce variance, tune style/creativity).\\n  **Prompt optimization vs. fine-tuning**\\n- **Prompt optimization** changes instructions/context/examples/parameters without changing model weights.\\n  - Faster iteration, lower operational burden, often cheaper.\\n- **Fine-tuning** changes model weights.\\n  - Higher complexity and governance; sometimes necessary, but not the default path.\\n\\n---\\n\\n#### A3) Evals: the required feedback signal\\n\\nAn **eval** is an automated test harness that:\\n\\n- Runs an agent/prompt on a **dataset**\\n- Scores outputs via a **metric**\\n- Produces repeatable feedback for iteration\\n  Common metric dimensions:\\n- **Accuracy / task success**\\n- **Hallucination rate / factuality**\\n- **Format compliance** (schemas, JSON, bullet structure)\\n- **Latency** (end-to-end runtime)\\n- **Cost** (tokens, tool calls, infra)\\n\\n---\\n\\n#### A4) The \u201cImpossible Triangle\u201d: Speed \xd7 Accuracy \xd7 Cost\\n\\nAgent design and optimization inevitably trade off:\\n\\n- **Speed**: latency, time-to-first-token, end-to-end runtime\\n- **Accuracy**: correctness, task completion, policy compliance\\n- **Cost**: token usage, model pricing, tool calls, infrastructure\\n  Key implications:\\n- You typically can\u2019t maximize all three simultaneously.\\n- Optimization must explicitly measure and constrain all three.\\n  Common architecture pattern:\\n- **Fast front agent** for interactive UX\\n- **Slower or cheaper back agents** for verification, retrieval, or deeper reasoning\\n\\n---\\n\\n#### A5) Optimizer families (what they do and when to use)\\n\\nThis training consolidates multiple optimizer approaches discussed across sessions:\\n\\n1. **Meta-prompting (Meta-reasoner)**\\n   - An LLM rewrites your prompt to produce candidate variants.\\n   - You evaluate candidates and keep the best.\\n2. **Genetic / Evolutionary optimization**\\n   - Maintains a population of prompts.\\n   - Applies **mutations** (remove/reorder/rewrite sections) and **selection** (keep winners).\\n   - Often includes \u201cfresh gene injection\u201d and a **Hall of Fame** of best prompts/components.\\n3. **Hierarchical / Reflective optimization (HRPO)**\\n   - Diagnoses failure clusters, forms hypotheses, proposes targeted prompt changes, re-evaluates.\\n   - Requires a clear metric + explanation so it can reason about failures.\\n4. **Few-shot Bayesian optimization**\\n   - Selects:\\n     - Which few-shot examples,\\n     - How many,\\n     - In which order,\\n   - Then inserts them into the prompt for stronger in-context guidance.\\n5. **Parameter optimization**\\n   - Tunes generation parameters (temperature/top_p/top_k) after prompt structure is solid.\\n   - Useful for controlling non-determinism and output variance.\\n     **Chaining optimizers (recommended workflow)**\\n\\n- A practical pipeline:\\n  1. Reflective/hierarchical (HRPO) or evolutionary to improve core instructions\\n  2. Few-shot Bayesian to optimize demonstrations\\n  3. Parameter tuning to stabilize style/variance and meet latency/cost constraints\\n\\n---\\n\\n#### A6) Multimodal prompt optimization (images + text)\\n\\nMultimodal agents take:\\n\\n- **Image inputs** (e.g., dashcam frames)\\n- **Text inputs** (questions/instructions)\\n- Produce **text outputs** (e.g., hazard descriptions)\\n  Multimodality raises difficulty because correctness depends on accurately grounding outputs in visual evidence and describing it consistently.\\n\\n---\\n\\n### B. Principal Pains and Practical Problems Encountered\\n\\n#### B1) Workshop/session problems (operational + facilitation)\\n\\n- Time zone mismatch reduces live attendance quality.\\n- Startup friction:\\n  - Zoom connection issues\\n  - Screen sharing failures (multi-monitor confusion)\\n- Logistics questions interrupt technical flow.\\n- Early \u201cdeep technical\u201d derailments due to lack of conceptual alignment.\\n- High cognitive load in hands-on segments (tools + steps + Q&A at once).\\n\\n#### B2) Optimization and engineering problems (technical)\\n\\n- **No dataset \u2192 no optimization**: teams want improvement without ground truth.\\n- **No metric \u2192 no signal**: changes are judged by \u201cvibes.\u201d\\n- Hidden tradeoffs: gains in accuracy may increase cost/latency.\\n- Overfitting to small/clean datasets that don\u2019t represent production.\\n- Tooling complexity:\\n  - Provider setup (API keys)\\n  - Version/environment issues\\n  - Multimodal data formatting and cost\\n- Non-determinism: outputs vary across runs; naive evals are unstable.\\n- Multilingual drift: improvements in one language/style can degrade perceived quality for non-native speakers.\\n\\n---\\n\\n### C. Causes of Common Errors and Deviations (Technical Diagnosis)\\n\\n#### C1) Process errors\\n\\n- Starting implementation before defining:\\n  - \u201cWhat does success look like?\u201d\\n  - \u201cHow will we measure it?\u201d\\n- Iterating prompts manually without repeatable evals (\u201cdoom wordsmithing\u201d).\\n- Changing too many variables at once (prompt + model + dataset + metric), breaking attribution.\\n\\n#### C2) Data and evaluation errors\\n\\n- Non-representative datasets (too small, too idealized, not production-like).\\n- No holdout/validation: training improvements don\u2019t generalize.\\n- Overfitting to dataset phrasing rather than task intent.\\n\\n#### C3) Metric errors\\n\\n- Metric-task mismatch:\\n  - Character-level similarity (e.g., Levenshtein) penalizes correct paraphrases.\\n  - Fast metrics can reward superficial closeness rather than semantic correctness.\\n- Missing metric explanations:\\n  - Reflective optimizers need \u201cwhy this score\u201d to hypothesize and fix failure modes.\\n\\n#### C4) Operational and facilitation errors\\n\\n- No pre-flight checks for recording/screen share.\\n- Letting logistics Q&A repeatedly interrupt technical depth.\\n- Skipping conceptual grounding and jumping into highly technical steps.\\n\\n---\\n\\n### D. Solutions, Best Practices, and Strategies\\n\\n#### D1) Facilitation strategy for optimization workshops\\n\\nUse a structured agenda:\\n\\n1. **Buffer + housekeeping** (2\u20135 min)\\n2. **Conceptual grounding** (5\u201310 min)\\n3. **Hands-on optimization** (primary block)\\n4. **Q&A checkpoints** (periodic)\\n5. **Closeout**: follow-ups and resources channel (e.g., Discord)\\n   Logistics handling:\\n\\n- Allow limited buffer for urgent logistics at the start.\\n- Route ongoing logistics to a dedicated channel (Discord/moderators).\\n  Operational readiness:\\n- Test recording and screen share before the session begins.\\n- Prepare backup plan (rejoin meeting, share window instead of full screen, alternate host).\\n\\n---\\n\\n#### D2) Build an eval-driven optimization loop (core technical workflow)\\n\\nA repeatable loop:\\n\\n1. Define a **dataset** (inputs + expected behavior)\\n2. Define a **metric** (scoring rubric and constraints)\\n3. Run a **baseline** prompt/agent and record results\\n4. Generate candidate prompts (human + optimizer)\\n5. Evaluate candidates on the same harness\\n6. Select the best under constraints (accuracy + speed + cost)\\n7. Iterate until target reached or budget exhausted\\n8. Validate on **holdout/validation** to detect overfitting\\n\\n---\\n\\n#### D3) Optimizer selection guidance\\n\\n- **Meta-prompting**: fast setup, quick baseline improvements.\\n- **Genetic/evolutionary**: broad exploration; good when small phrasing changes matter.\\n- **Reflective/HRPO**: best for nuanced failure clusters and \u201clast mile\u201d improvements.\\n- **Few-shot Bayesian**: when selection and ordering of examples is a major performance lever.\\n- **Parameter tuning**: stabilize variance and style after instructions/examples are strong.\\n  Budget warning:\\n- More sophisticated optimizers typically increase API calls, tokens, and wall-clock time.\\n\\n---\\n\\n#### D4) Metric strategy (fast iteration vs semantic correctness)\\n\\n- Use **fast metrics** early:\\n  - regression detection\\n  - formatting/stability checks\\n- Upgrade to **semantic evals** for correctness:\\n  - **LLM-as-a-judge** with a strict rubric\\n  - hybrid metrics (semantic + format + safety)\\n- Consider **multi-metric objectives**:\\n  - accuracy + cost + latency to prevent \u201caccurate but too expensive\u201d prompts\\n\\n---\\n\\n#### D5) Production readiness strategy (safety and reproducibility)\\n\\nTreat prompt optimization as controlled production change:\\n\\n- Use staging, monitoring, and rollback prompts.\\n- Keep prompt versioning, changelogs, and eval results history.\\n- Build datasets from **production traces**:\\n  - convert traces into eval datasets or annotation queues\\n- Add human-in-the-loop review for safety-critical workflows.\\n  Model strategy:\\n- Use a **larger model** to generate candidate prompts (\u201cauthoring model\u201d).\\n- Evaluate on the **target deployment model** (smaller/cheaper) for parity.\\n  Multilingual concerns:\\n- Ensure evals cover target languages.\\n- Use judges that understand the language or add human evaluation.\\n\\n---\\n\\n### E. What Should Be Done (Do\u2019s)\\n\\n- **Start every workshop** with conceptual grounding (terms, scope, intended outcome).\\n- **Enable and test recording**; ensure rewatchability for time zones.\\n- **Pre-flight check** (5\u201310 minutes before start):\\n  - Zoom audio/video\\n  - correct monitor/window sharing\\n  - recording status\\n- **Define success metrics upfront** (rubric, pass/fail rules, partial credit).\\n- **Build/curate a representative dataset** (typical + edge + adversarial cases).\\n- **Run and document a baseline** before optimization.\\n- **Use train + validation (+ test)** splits; add a holdout set to detect overfitting.\\n- **Select an optimizer** based on:\\n  - budget, failure complexity, exploration vs targeted fixes\\n- **Log and version everything**:\\n  - prompt versions, scores, dataset versions, model/provider settings\\n- **Constrain outputs** when consistency matters (schemas, bullet limits, citation rules).\\n- **Use production traces** to keep evaluation realistic; build an annotation queue when needed.\\n- **Apply multi-metric gates** (accuracy must improve without breaking cost/latency).\\n\\n---\\n\\n### F. What Should NOT Be Done (Don\u2019ts)\\n\\n- **Don\u2019t** jump into technical implementation without aligning key concepts and goals.\\n- **Don\u2019t** rely only on live attendance; always provide recordings.\\n- **Don\u2019t** allow logistics Q&A to repeatedly interrupt technical flow (route it elsewhere).\\n- **Don\u2019t** optimize without a dataset or without a metric.\\n- **Don\u2019t** rely only on \u201cdoom wordsmithing\u201d (random manual edits) without evals.\\n- **Don\u2019t** ignore the **speed\u2013accuracy\u2013cost** triangle.\\n- **Don\u2019t** let optimizers run without guardrails:\\n  - they may exploit metric loopholes or create brittle prompts\\n- **Don\u2019t** assume gains on tiny samples generalize to production.\\n- **Don\u2019t** use only character-level similarity for semantic tasks in production-critical scenarios.\\n- **Don\u2019t** change prompt + model + metric + dataset simultaneously without tracking; you lose causal understanding.\\n- **Don\u2019t** treat experiment tracking as optional; without it, improvements are not reproducible.\\n\\n---\\n\\n### G. Tools, Tasks, and Recommended Actions\\n\\n#### G1) Tools (explicitly referenced or strongly implied)\\n\\n- **Zoom**: live delivery, screen sharing, recording\\n- **Discord**: async logistics, follow-ups, announcements\\n- **Comet**: experiment tracking UI, dataset management, optimization studio\\n- **OPIK SDK** (Python): optimization/evaluation loops (multimodal demo)\\n- **OPIC SDK / OPIC GitHub monorepo**: optimizer suite and algorithms\\n- **LLM providers**: OpenAI, Gemini, local models (e.g., Ollama)\\n- **RAG + MCP servers/clients**: context/tool ecosystem patterns\\n\\n#### G2) Recommended team routines (practical)\\n\\n1. **Workshop runbook**\\n   - setup checklist + agenda template + escalation channel for logistics\\n2. **Minimal eval harness**\\n   - dataset loader\\n   - prompt runner\\n   - metric function (with explanation)\\n   - reporting dashboard\\n3. **Optimization experiments**\\n   - meta-prompting run (N candidates \xd7 M trials)\\n   - genetic/evolutionary run (population, mutation operators, selection)\\n   - reflective/HRPO run (failure clustering, hypothesis generation)\\n4. **Budget controls**\\n   - max trials, max tokens, max wall-clock time\\n5. **Acceptance gates**\\n   - \u201cno merge if validation score drops\u201d\\n   - enforce latency/cost ceilings\\n6. **Trace-to-dataset pipeline**\\n   - export 100\u2013500 production traces\\n   - label via annotation queue\\n   - iterate dataset quality before optimizing further\\n7. **Release checklist**\\n   - staging rollout, monitoring KPIs, rollback prompt\\n\\n#### G3) Multimodal data preparation (when applicable)\\n\\n- Images/audio/video may require **Base64 encoding** in datasets.\\n- Video can be token-heavy; plan budget and limits accordingly.\\n\\n---\\n\\n## 2) Practical Examples (Reviewed and Rewritten)\\n\\n### Example 1 \u2014 Handling a delayed workshop start (screen sharing issue)\\n\\n- **Scenario:** Presenter cannot share the correct screen due to multi-monitor setup.\\n- **Recommended response:**\\n  - Announce a brief delay and the cause (\u201cscreen share setup\u201d).\\n  - Use the time for housekeeping:\\n    - confirm recording is on,\\n    - ask attendees to post time zones in chat,\\n    - redirect logistics questions to Discord.\\n  - Resume with **conceptual grounding** once stable.\\n\\n---\\n\\n### Example 2 \u2014 Preventing early technical derailment\\n\\n- **Scenario:** Participants immediately ask deep implementation questions (tool calling, optimizer internals) before definitions are aligned.\\n- **Recommended response:**\\n  - Pause and define:\\n    - agent vs prompt vs context vs eval,\\n    - the target metric and dataset,\\n    - success criteria and constraints.\\n  - Explicitly defer deep dives:\\n    - \u201cWe\u2019ll go deep in a moment\u2014first we align on concepts and the evaluation loop.\u201d\\n\\n---\\n\\n### Example 3 \u2014 Meta-prompting to fix inconsistent formatting\\n\\n- **Scenario:** A support chatbot fails JSON schema compliance ~30% of the time.\\n- **Approach:**\\n  - Create an eval set of real formatting cases.\\n  - Use a meta-reasoner prompt to generate ~6 system-prompt variants:\\n    - stricter schema instructions,\\n    - explicit formatting steps,\\n    - refusal conditions for missing fields.\\n  - Evaluate all candidates; select the best and iterate until compliance stabilizes.\\n\\n---\\n\\n### Example 4 \u2014 Genetic optimization for a sensitive classification task\\n\\n- **Scenario:** Ticket triage accuracy varies heavily with small prompt phrasing differences.\\n- **Approach:**\\n  - Start from a parent prompt.\\n  - Generate a population of children via mutations:\\n    - reorder priorities (labels first, then rules),\\n    - remove ambiguous wording,\\n    - tighten label definitions.\\n  - Evaluate all prompts; keep winners and discard losers.\\n  - Inject fresh prompts periodically (including the original) to avoid premature convergence.\\n  - Maintain a **Hall of Fame** of best prompts/components.\\n\\n---\\n\\n### Example 5 \u2014 Reflective/HRPO optimizer to reduce hallucinations in incomplete context\\n\\n- **Scenario:** An agent hallucinates when retrieval returns partial or irrelevant documents.\\n- **Approach:**\\n  - Define a metric that penalizes hallucination and rewards:\\n    - uncertainty statements,\\n    - citations/quotes from sources.\\n  - Reflective optimizer clusters failures:\\n    - missing citations,\\n    - overconfident claims with no evidence.\\n  - Prompt changes:\\n    - \u201cIf the answer is not supported by context, say \u2018I don\u2019t know\u2019 and ask for clarification.\u201d\\n    - require quoting or citing retrieved passages.\\n  - Re-run eval until hallucination rate drops on validation.\\n\\n---\\n\\n### Example 6 \u2014 Multimodal hazard detection optimization (Comet + OPIK)\\n\\n- **Scenario:** Dashcam hazard detector gives generic driving advice instead of image-grounded hazards.\\n- **Dataset pattern:**\\n  - image + question \u2192 reference hazard annotation\\n- **Metric approach (fast demo metric):**\\n  - Levenshtein ratio vs reference text (fast but not semantic)\\n- **Reflective prompt improvements:**\\n  - \u201cOnly describe hazards visible or strongly implied by the image; do not provide general driving advice.\u201d\\n  - Output constraint: \u201cReturn 1\u20133 bullet points, each a hazard statement.\u201d\\n- **Validation caution:**\\n  - Small subsets can cause high variance and overfitting; confirm improvements on validation/test and consider LLM-judge for semantic correctness.\\n\\n---\\n\\n### Example 7 \u2014 Intent engineering for a safety-sensitive conversational assistant\\n\\n- **Scenario:** Build an empathetic therapy-style assistant with safety protocols.\\n- **Intent-first approach:**\\n  - Create a dataset of ideal conversations:\\n    - anxiety \u2192 validation + grounding exercise\\n    - self-harm \u2192 safety protocol and resources\\n    - diagnosis request \u2192 refuse diagnosis, provide guidance/resources\\n  - Define eval rubric:\\n    - empathy markers, safety compliance, refusal correctness\\n  - Optimize prompts:\\n    - meta-prompting for quick gains\\n    - reflective optimizer for recurring failures (too clinical, insufficient validation)\\n  - Add holdout eval set to avoid overfitting to scripted examples.\\n\\n---\\n\\n## 3) Conclusion and Practical Application\\n\\nAgent/prompt optimization becomes reliable only when treated as an engineering discipline: **dataset + metric + iterative eval loop + controlled changes**. Operationally, successful workshops require **conceptual grounding**, tested **recording/screen-sharing**, and clear boundaries between logistics and technical work. Technically, improvements should be driven by **evals** and chosen optimizers (meta-prompting, genetic/evolutionary, reflective/HRPO, few-shot Bayesian, parameter tuning), with explicit management of the **speed\u2013accuracy\u2013cost** tradeoff. In day-to-day practice, start with a minimal eval harness and baseline, run low-budget optimization, validate on holdout data, and ship through a controlled release process with monitoring and rollback.\\n**Next steps**\\n\\n1. Build a reusable **workshop runbook** (checklist + agenda + support channels).\\n2. Implement a minimal **eval harness** for one real agent flow and record baseline metrics.\\n3. Run **meta-prompting** to capture quick wins, then escalate to **reflective/HRPO** or **genetic** as needed.\\n4. Add **few-shot selection** and **parameter tuning** once prompt structure is stable.\\n5. Upgrade metrics to **LLM-judge + rubric** (and human review where required), and enforce **multi-metric gates** (accuracy + cost + latency).\\n\\n---\\n\\n## Glossary (Key Terms)\\n\\n- **Agent**: An LLM-based system that may use tools (APIs, retrieval, MCP) to complete tasks.\\n- **Agent optimization**: Improving an agent\u2019s effectiveness through systematic iteration using datasets and metrics.\\n- **Conceptual grounding**: A short theory-based alignment step to ensure shared definitions, goals, and assumptions.\\n- **Context engineering**: Supplying relevant context via retrieval/tools/memory/long context windows (e.g., RAG, MCP).\\n- **Intent engineering**: Defining desired outputs/behaviors first (examples), then optimizing backward toward them.\\n- **Eval (evaluation)**: Automated testing of prompts/agents against a dataset using a metric/rubric.\\n- **Metric / reward function**: A scoring function used to judge outputs (accuracy, hallucination, cost, latency, etc.).\\n- **Meta-prompting (meta-reasoner)**: Using an LLM to generate improved prompts for another task.\\n- **Genetic / evolutionary optimization**: Population-based search over prompts via mutation and selection.\\n- **Mutation**: A change applied to a prompt to create variation (remove/reorder/replace text).\\n- **Hall of Fame**: A retained set of top-performing prompts/components across generations.\\n- **Reflective / hierarchical optimizer (HRPO)**: Optimizer that diagnoses failure patterns, forms hypotheses, and proposes targeted fixes.\\n- **Few-shot Bayesian optimizer**: Method for selecting and ordering few-shot examples to include in prompts.\\n- **Parameter optimization**: Tuning inference parameters like temperature/top_p/top_k to manage variability and style.\\n- **LLM-as-a-judge**: Using an LLM to grade another model\u2019s output against a rubric/reference.\\n- **Levenshtein ratio**: Character-level similarity score based on edit distance (fast, but not semantic).\\n- **Overfitting**: Improvements on training data that don\u2019t generalize to validation/test.\\n- **MCP (Model Context Protocol)**: Tool ecosystem pattern (servers/clients) enabling models to access external capabilities.\\n- **RAG (Retrieval-Augmented Generation)**: Retrieval system that injects documents into context to ground responses.\\n\\n---\\n\\n## Review Questions (Knowledge Check)\\n\\n1. Why is **conceptual grounding** important before hands-on optimization in a workshop?\\n2. What are the three constraints in the **speed\u2013accuracy\u2013cost** triangle, and why do they conflict?\\n3. What are the minimum components of an **eval-driven optimization loop**?\\n4. When would you choose **meta-prompting** vs **genetic optimization** vs **reflective/HRPO**?\\n5. Why can **Levenshtein ratio** be misleading for semantic tasks, and what should you use instead?\\n6. What does it mean to **chain optimizers**, and what reminder should guide the order (instructions \u2192 examples \u2192 parameters)?\\n7. Why are **validation/holdout sets** necessary during prompt optimization?\\n8. What operational checklist items should be validated before running a live interactive workshop?\\n\\n---\\n\\n## Suggested Further Reading / Resources\\n\\n- **Facilitation & workshop design**\\n  - Atlassian Team Playbook (facilitation patterns and team rituals): https://www.atlassian.com/team-playbook\\n- **Evals and LLM testing**\\n  - OpenAI Evals (conceptual reference): https://github.com/openai/evals\\n  - LangChain evaluation guides: https://python.langchain.com/docs/guides/evaluation/\\n- **Agent systems and tool use**\\n  - OpenAI documentation (agents/tooling concepts): https://platform.openai.com/docs/\\n  - Anthropic documentation (tool use and agent patterns): https://docs.anthropic.com/\\n  - LangChain agent patterns and evaluation tooling: https://python.langchain.com/docs/\\n- **RAG learning resources**\\n  - Pinecone RAG guides: https://www.pinecone.io/learn/\\n  - Weaviate RAG resources: https://weaviate.io/developers/weaviate\\n- **Experiment tracking**\\n  - Comet platform: https://www.comet.com/"},{"id":"opik-workshop-1","metadata":{"permalink":"/fr/blog/opik-workshop-1","editUrl":"https://github.com/locki-io/docs.locki.io/tree/main/blog/2026-01-14-OPIK-workshop.mdx","source":"@site/blog/2026-01-14-OPIK-workshop.mdx","title":"OPIK : AI Evaluation and Observability","description":"This lecture, led by Abby Morgan, an AI Research Engineer, introduces AI evaluation as a systematic feedback loop for transitioning prototypes to production-ready systems. It outlines the four key components of a useful evaluation: a target capability, a test set, a scoring method, and decision rules. The session differentiates between general benchmarks and specific product evaluations, emphasizing the need for observability in agent evaluation. It demonstrates using OPIK, an open-source tool, to track, debug, and evaluate LLM agents through features like traces, spans, \'LM as a judge\', and regression testing datasets.","date":"2026-01-14T00:00:00.000Z","tags":[{"inline":false,"label":"AI and Machine Learning","permalink":"/fr/blog/tags/ai-ml","description":"Articles on AI, machine learning, and related technologies"},{"inline":false,"label":"civitech","permalink":"/fr/blog/tags/civictech","description":"Citizen technologies and open source for the public good"},{"inline":false,"label":"Encode hackathon","permalink":"/fr/blog/tags/encode","description":"Information from the Encode hackathon"},{"inline":false,"label":"Observability","permalink":"/fr/blog/tags/observability","description":"Articles on observability practices and tools"}],"readingTime":17.25,"hasTruncateMarker":true,"authors":[{"name":"Jean-No\xebl Schilling","title":"Locki one / french maintainer","url":"https://github.com/jnschilling","socials":{"linkedin":"https://www.linkedin.com/in/jnschilling/","github":"https://github.com/jnschilling","newsletter":"https://www.stratej.fr"},"imageURL":"https://github.com/jnschilling.png","key":"jnxmas","page":null}],"frontMatter":{"slug":"opik-workshop-1","title":"OPIK : AI Evaluation and Observability","authors":["jnxmas"],"tags":["ai-ml","civictech","encode","observability"]},"unlisted":false,"prevItem":{"title":"OPIK : Agent & Prompt Optimization for LLM Systems","permalink":"/fr/blog/opik-workshop-2"},"nextItem":{"title":"Project scope - Victor + JNS","permalink":"/fr/blog/victor-intro-project-description"}},"content":"This lecture, led by Abby Morgan, an AI Research Engineer, introduces AI evaluation as a systematic feedback loop for transitioning prototypes to production-ready systems. It outlines the four key components of a useful evaluation: a target capability, a test set, a scoring method, and decision rules. The session differentiates between general benchmarks and specific product evaluations, emphasizing the need for observability in agent evaluation. It demonstrates using OPIK, an open-source tool, to track, debug, and evaluate LLM agents through features like traces, spans, \'LM as a judge\', and regression testing datasets.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Takeaways\\n\\n1.  Introduction to Abby Morgan, an AI Research Engineer and Developer Advocate at Comet.\\n2.  Housekeeping rules for the hackathon: use the public Discord channel for specific questions to ensure fairness and help others.\\n3.  Recap of the previous session on AI evaluations: they turn prototypes into production-ready systems.\\n4.  Evaluation is the feedback loop that enables systematic improvement, turning guesswork into a scientific process.\\n5.  Evaluations help in making decisions (ship or roll back), debugging failures, and building trustworthy systems.\\n6.  An evaluation is defined as a structured, repeatable measurement of system behavior against specific criteria.\\n7.  Four key ingredients of a useful evaluation: a target capability, a test set reflecting the relevant world, a scoring method, and decision rules.\\n8.  Evaluation outputs are not just numbers; they can include concrete examples, categorical slices, and error taxonomies.\\n9.  Distinction between benchmarks and product evaluations: Benchmarks are for broad comparisons, while product evals are specific to your use case, tools, and workflows.\\n10. The importance of observability in evaluating agents: the full trace (context, prompts, tool calls) is crucial as agents don\'t fail like traditional software.\\n\\n<iframe\\n  width=\\"100%\\"\\n  style={{ \\"aspect-ratio\\": \\"16 / 9\\" }}\\n  src=\\"https://www.encodeclub.com/programmes/comet-resolution-v2-hackathon/events/intro-to-opik\\"\\n  title=\\"OPIK workshop 1\\"\\n  frameborder=\\"0\\"\\n  allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\"\\n  referrerpolicy=\\"strict-origin-when-cross-origin\\"\\n  allowfullscreen\\n></iframe>\\n\\n## Highlights\\n\\n- `\\"Evaluation is essentially just the feedback loop that makes improvement thematic.\\"-- Annie Morgan`\\n- `\\"Evals are what turn random iteration or guesswork into more of a scientific process that allows you to improve in a systematic way.\\"-- Annie Morgan`\\n\\n## Chapters & Topics\\n\\n### The Role and Definition of AI Evaluation\\n\\n> An evaluation is a structured, repeatable measurement of system behavior against criteria we care about. This structured approach is key to distinguishing a true evaluation from a mere product demo, as it must be consistently runnable over time to interpret changes.\\n\\n- **Keypoints**\\n  - Evaluation turns a prototype into a production-ready system.\\n  - It provides a feedback loop for systematic improvement.\\n  - It helps make decisions like shipping or rolling back features.\\n  - An evaluation must be structured and repeatable.\\n  - It\'s the difference between a product demo and a scientific process.\\n  - It helps build systems that people can trust in the real world.\\n- **Explanation**\\n  The process of evaluation is what transforms a cool but unreliable prototype into something that can be confidently shipped and iterated upon in a real-world production environment. It provides the necessary feedback loop to make improvements systematic rather than random. It helps decide whether to ship new features, roll back changes, debug failures, and ultimately, build systems that users can trust. Without a structured evaluation process, it\'s difficult to know if a system is actually improving or if observed successes are just cherry-picked examples or irrelevant outliers.\\n\\n### Four Ingredients of a Useful Evaluation\\n\\n> Most useful evaluations are composed of four essential ingredients: a target capability, a test set, a scoring method, and decision rules.\\n\\n- **Keypoints**\\n  - A target capability (e.g., fluency, relevance, toxicity).\\n  - A test set that reflects the specific world and edge cases you care about.\\n  - A scoring method, which can be human annotation or an automated metric.\\n  - Decision rules that dictate actions based on the evaluation scores (e.g., ship, roll back).\\n- **Explanation**\\n  To create a useful evaluation, you need four components. First, a \'target capability\' which could be things like fluency, relevance, or toxicity. Second, a \'test set\' that accurately reflects the real-world scenarios you care about, including edge cases. This is where product evals differ from general benchmarks. Third, a \'scoring method\', which can be either human annotation or an automated metric. Finally, \'decision rules\' which define what actions to take based on the evaluation scores, such as deploying a new feature or rolling it back.\\n\\n### Benchmarks vs. Product Evals\\n\\n> Benchmarks are standardized evaluations for broad comparisons across a wide range of use cases, whereas product evaluations are tailored to a specific use case, including its unique tools and workflows. Benchmarks are helpful for general comparisons, but are not a substitute for product-specific evaluations.\\n\\n- **Keypoints**\\n  - Benchmarks are standardized and cover a wide range of use cases.\\n  - Product evals are specific to your use case, tools, and workflows.\\n  - Benchmarks are for broader comparisons.\\n  - Product evals are for seriously evaluating your system for its specific purpose.\\n  - Product evals should use a test set that reflects the particular world you care about, including edge cases.\\n- **Explanation**\\n  While the term \'benchmark\' is often used in AI evals, it\'s important to distinguish it from a \'product eval\'. Benchmarks are generalized and standardized, designed to test a wide range of examples and use cases. They are useful for broader comparisons. On the other hand, a product evaluation is specific to your product\'s use case. It should use a test set that reflects the world you care about, including specific edge cases, and incorporate your particular tools and workflows. For serious evaluation of your own system\'s performance for its intended purpose, a product-specific eval is necessary.\\n\\n### Observability as the Foundation for Agent Evaluation\\n\\n> For AI agents, observability is the foundation of evaluation. Since agents don\'t fail like traditional software, it\'s crucial to look beyond the final output and observe the full trace of their operation. This includes the context retrieved, prompts used, tools called, and all intermediate steps. This observation is the first step in a cycle of improvement: observe, understand, evaluate, and then improve.\\n\\n- **Keypoints**\\n  - The final output of an agent is never the full story.\\n  - Observing the full trace is critical for evaluation.\\n  - The trace includes retrieved context, prompts, tool calls, and intermediate steps.\\n  - Agents do not fail like traditional software, so observation is key.\\n  - The improvement cycle is: observe -&gt; understand -&gt; evaluate -&gt; improve.\\n  - OPIK is designed to make observability and evaluation practical and simple.\\n- **Explanation**\\n  Evaluating agents requires starting with observability because their final output alone doesn\'t tell the whole story. Unlike traditional software, an agent\'s failure or success is determined by a complex series of steps. Therefore, you must be able to see the full trace of its actions. This includes what context was retrieved, what prompts were generated and used, which tools were called, and all the intermediate results. Once you can observe this entire process, you can begin to understand what is happening. With understanding, you can then evaluate the agent\'s performance against your criteria. Finally, with these evaluations and measurements of success, you can systematically begin to improve the agent. OPIC is a tool specifically designed to facilitate this process by making observability and evaluation practical and easy.\\n\\n### Integrating OPIK into Your Code\\n\\n> OPIK is an observability tool that can be easily integrated into your code to track, monitor, and evaluate the performance of LLM agents. It requires minimal code, often just one to three lines, to start logging agent interactions. The integration method might vary slightly depending on whether you\'re using a direct integration like with OpenAI Agents or a more general method like the track decorator.\\n\\n- **Keypoints**\\n  - Import OPIK at the top of your file.\\n  - Integration can be as simple as one to three lines of code.\\n  - Direct integrations, like with OpenAI Agents, might have a specific syntax.\\n  - The \'@track\' decorator is another common method for integration.\\n- **Explanation**\\n  To integrate OPIK, you first import it at the top of your script. Then, depending on the framework, you might use a specific call like the one shown for OpenAI Agents, or use a track decorator. For the demonstrated recipe generator agent, which uses OpenAI Agents, the integration was slightly different but still very simple. This minimal setup allows OPIK to automatically capture detailed information about each agent run.\\n- **Examples**\\n  > A basic agent acting as a recipe generator. The user provides a list of ingredients, and the agent performs two LLM calls. The first LLM call suggests a recipe based on the ingredients (e.g., creamy orange tomato soup from tomatoes, cream, and oranges). The second LLM call researches the steps to create that specific recipe.\\n  - The user runs the script and is prompted for ingredients.\\n  - The user enters \'tomatoes, cream, oranges\'.\\n  - The first LLM processes these ingredients and suggests \'creamy orange tomato soup\'.\\n  - This recipe name is then passed to the second LLM.\\n  - The second LLM researches and outputs the detailed steps for making the soup.\\n  - All of this activity is logged as a single trace in the OPIC dashboard.\\n\\n### Understanding Traces and Spans\\n\\n> OPIC organizes logged data into traces and spans. A trace represents a single, complete end-to-end process or execution of your agent. A span is a smaller, individual step or operation within that trace. This hierarchical structure allows for both a high-level overview and a granular look at each component of the agent\'s execution, which is crucial for debugging and evaluation.\\n\\n- **Keypoints**\\n  - A trace is the entire end-to-end agentic call.\\n  - A span is an individual step within the trace.\\n  - The dashboard allows you to view the overall trace and drill down into individual spans.\\n  - This structure helps isolate where failures or issues occur in a complex agent.\\n- **Explanation**\\n  When an agent runs, the entire operation is captured as one trace. Within this trace, you can see individual spans corresponding to specific actions, like LLM calls or tool usage. The OPIC dashboard clearly displays the input and output for the entire agent call (the trace) and for each individual step (the spans). This helps identify exactly where in the process a failure or unexpected behavior occurs. For example, if an agent fails, you can inspect the spans leading up to the failure to understand the root cause.\\n- **Examples**\\n  > In the recipe generator agent, the entire process from taking user ingredients to outputting a full recipe is one trace. Within that trace, the first LLM call (suggesting \'chicken parmesan\') is one span, and the second LLM call (researching how to make it) is another span.\\n  - User interacts with the agent, triggering a run. This entire run is a \'trace\'.\\n  - The agent calls the first LLM to generate a recipe idea. This call is a \'span\'.\\n  - The agent then calls the second LLM to get the recipe steps. This second call is another \'span\'.\\n  - The dashboard shows the full trace and allows clicking into it to see the individual spans, each with its own inputs, outputs, and metadata.\\n\\n### Online Evaluations and \'LM as a Judge\'\\n\\n> OPIC allows for creating online evaluations that automatically score agent performance against predefined criteria every time an agent runs. A key feature is \'LM as a judge,\' where one LLM is used to evaluate the output of your agent\'s LLM based on a custom prompt and scoring scale you provide. This enables automated, qualitative assessment of agent outputs.\\n\\n- **Keypoints**\\n  - Online evaluations automatically score agent calls based on created rules.\\n  - \'LM as a judge\' uses an external LLM to evaluate your agent\'s output.\\n  - You must provide a detailed prompt to guide the judge LLM.\\n  - Users can manually annotate traces with their own scores to calibrate the \'LM as a judge\' evaluator.\\n  - You can compare human scores with LLM-generated scores to improve the evaluation prompt.\\n- **Explanation**\\n  To set up an online evaluation, you navigate to the \'Online Evaluation\' section in the OPIC dashboard and create a new rule. You name the rule, provide API keys for the external judging LLM (e.g., from OpenAI, Anthropic, or OpenRouter), and select the model. You then write a detailed prompt that instructs the judge LLM on how to score the output, including the context, scoring scale, and criteria. Once the rule is created, every new trace will be automatically scored against this metric, and the scores will appear in your trace list.\\n- **Examples**\\n  > For the recipe agent, three \'LM as a judge\' evaluation metrics were created to assess the generated recipes. The user set up a rule by providing a prompt to a judge LLM, defining a scoring scale and the context for what constitutes a good recipe suggestion. After setup, every time the recipe agent ran, it was automatically scored by these metrics.\\n  - Go to \'Online Evaluation\' and click \'Create New Rule\'.\\n  - Name the rule (e.g., \'Recipe Coherence\').\\n  - Choose a provider and model for the judge LLM (e.g., OpenAI\'s GPT-4).\\n  - Write a prompt for the judge: \'You are an evaluator. Rate the following recipe on a scale of 1-5 for coherence...\'\\n  - Save the rule. Now, all subsequent runs of the recipe agent will have a \'Recipe Coherence\' score.\\n  - You can then go into a trace and manually add a human score to compare against the \'LM as a judge\' score, which helps in refining the evaluation prompt.\\n- **Considerations**\\n- It\'s good practice to use a healthy mix of heuristic evaluation metrics and \'LLM as a judge\' metrics.\\n- You may need to iterate on the prompt given to the judging LLM to ensure its evaluations align with your definition of success.\\n- Using one LLM to judge another can be problematic, so human oversight and comparison are valuable.\\n- **Special Circumstances**\\n- If you suspect the \'LM as a judge\' is providing inaccurate scores, you should go into the traces, manually score them yourself, and compare your scores to the AI\'s. Based on the discrepancies, you can tweak the prompt given to the judge LLM to make its ratings more closely aligned with human judgment.\\n\\n### Managing and Testing with Problematic Samples\\n\\n> OPIC provides features to isolate problematic agent runs and use them for regression testing. When an agent produces an error or an undesirable output, you can select these \'problematic samples\' from your traces and add them to a named dataset. This dataset can then be used to repeatedly test your agent after making code changes, ensuring that your fixes are effective and don\'t introduce new issues.\\n\\n- **Keypoints**\\n  - Isolate traces where the agent fails or performs poorly.\\n  - Select these problematic samples in the dashboard.\\n  - Add them to a new or existing dataset.\\n  - Use this dataset to run regression tests on your agent after making code changes.\\n  - This helps verify that improvements are effective and don\'t cause regressions.\\n- **Explanation**\\n  To do this, you would go through your list of traces in the OPIC dashboard. You can filter or sort by low evaluation scores or errors to find the problematic runs. Select the checkboxes next to these traces. Then, use the option to \'add to a dataset\'. You can create a new dataset (e.g., \'problematic samples\') or add to an existing one. Later, when you\'ve modified your agent\'s code, you can run the agent specifically against the inputs from this dataset to see if the outputs have improved.\\n\\n### Getting Started with OPIC and Basic Tracing\\n\\n> OPIC (Open-Source Project for Instrumenting and Correcting AI) is an open-source tool for LLM observability. It can be self-hosted locally for free, providing high customization, or used via a 100% free cloud tier for quick setup. The demonstration and initial setup guide focus on the free cloud tier.\\n\\n- **Keypoints**\\n  - OPIC is 100% open source and offers a 100% free cloud tier.\\n  - Setup involves installing the package and configuring it with an API key and workspace.\\n  - The \'@track\' decorator is the simplest way to add tracing to any Python function.\\n  - Direct integrations for frameworks like OpenAI provide deeper observability by wrapping the client object.\\n- **Explanation**\\n  To get started with the free cloud tier, first install OPIC using \'pip install opic\'. Then, configure your account by running \'open configure\'. This command will prompt you for your API key, which can be found in your Comet account settings. You will also confirm the workspace you want to use. You can also set these as environment variables to avoid entering them manually each time. A project name can be specified using a code snippet provided in the quick start guide.\\n- **Examples**\\n  > A simple \'Hello World\' trace example can be created using the \'@track\' decorator from OPIC. You import \'track\' from OPIC and place the decorator above the definition of the function you want to monitor, such as an LLM call. This allows you to track the function\'s execution without rewriting your code. Every time the decorated function is run, it will automatically log to Comet, and a link to view the trace will be provided in the output.\\n  - Import the \'track\' decorator: `from opic import track`\\n  - Place the decorator above your function definition: `@track\\ndef my_function(): ...`\\n- Run the function. It will automatically log its execution, inputs, and outputs to OPIC.\\n  > OPIC offers direct integrations with popular frameworks like OpenAI. Instead of using the \'@track\' decorator, you import the specific integration module (e.g., `opic.integrations.openai`). Then, you wrap your framework client (e.g., the OpenAI client) in an OPIC track function. This provides deep observability into all system metrics and information from the LLM call with just one or two lines of code.\\n- Import the integration: `import opic.integrations.openai`\\n- Instantiate your original client: `client = OpenAI()`\\n- Wrap the client with the OPIC function: `client = opic.integrations.openai.patch(client)`\\n- Now, all calls made using this `client` object will be automatically tracked.\\n\\n### Debugging and Evaluation with OPIC\\n\\n> OPIC provides visibility into an agent\'s execution flow, helping users debug errors and identify areas for improvement. It allows you to see exactly which step in a multi-step agent failed, avoiding the need to manually sift through code or error traces.\\n\\n- **Keypoints**\\n  - OPIC\'s trace visualization helps pinpoint the exact step where an error occurred in an agent.\\n  - This reduces debugging time by narrowing the search space from the entire codebase to a specific component (e.g., one LLM call).\\n  - Automatic evaluation metrics can be set up to flag issues like toxicity that are hard to catch manually.\\n  - The UI allows filtering the entire dataset based on evaluation flags, aggregating all relevant examples for analysis.\\n- **Explanation**\\n  When an agent call fails, OPIC\'s trace view shows the sequence of operations (e.g., LLM calls, tool calls). If a step fails, the trace stops there. For example, if an agent fails at the \'recipe suggester\' step, you know the problem lies within that specific LLM call. This narrows down the debugging scope significantly. For more subtle issues, you can create evaluation metrics for things like toxicity. These metrics can automatically flag problematic runs, which you can then filter and analyze in the OPIC UI. This makes it much easier to consume a lot of information quickly and distill it down to find where things are going wrong, why, and how to fix them.\\n\\n### Managing Evaluation Datasets and Metrics\\n\\n> To maintain a relevant dataset for evaluations as an application evolves, OPIC allows for dynamic management of evaluation datasets. It is also highly recommended to use a combination of evaluation methods rather than relying on a single one.\\n\\n- **Keypoints**\\n  - Evaluation datasets in OPIC can be dynamically updated by adding or removing traces.\\n  - If a dataset becomes irrelevant due to application changes, you can edit it or create a new one from scratch.\\n  - It is recommended to use multiple evaluation metrics, such as combining heuristic logic with an LLM-as-a-judge, for more robust evaluation.\\n  - Users are not limited to one evaluation dataset and can maintain several for different testing scenarios.\\n- **Explanation**\\n  As your application changes (e.g., tool call names, parameters), some traces in your evaluation dataset may become obsolete. In the OPIC UI, you can easily remove irrelevant traces from an existing dataset. You can also add new, more relevant traces. If a dataset has largely lost its relevance, you can create a completely new one from scratch. Users are not limited to a single evaluation dataset and can have dozens for different purposes. For the evaluation itself, you can and should use multiple metrics. For example, you can evaluate a model using a combination of heuristic logic and an LLM-as-a-judge.\\n\\n## Assignments & Suggestions\\n\\n- Include the source code file (like a cursor) or basic setup steps in your final submission if you use AI tools like n8n, so judges can confirm the project works as intended.\\n- For detailed or \'deep divey\' questions about OPIC, post them on the Discord server for a more thorough answer.\\n- For questions about specific SDKs and the functional differences between various OPIC integration methods, post them on Discord."},{"id":"victor-intro-project-description","metadata":{"permalink":"/fr/blog/victor-intro-project-description","editUrl":"https://github.com/locki-io/docs.locki.io/tree/main/blog/2026-01-14-meeting.md","source":"@site/blog/2026-01-14-meeting.md","title":"Project scope - Victor + JNS","description":"Location: Discord voice chat","date":"2026-01-14T00:00:00.000Z","tags":[{"inline":false,"label":"Meeting","permalink":"/fr/blog/tags/meeting","description":"Meeting tag description"}],"readingTime":9.05,"hasTruncateMarker":true,"authors":[{"name":"Jean-No\xebl Schilling","title":"Locki one / french maintainer","url":"https://github.com/jnschilling","socials":{"linkedin":"https://www.linkedin.com/in/jnschilling/","github":"https://github.com/jnschilling","newsletter":"https://www.stratej.fr"},"imageURL":"https://github.com/jnschilling.png","key":"jnxmas","page":null}],"frontMatter":{"slug":"victor-intro-project-description","title":"Project scope - Victor + JNS","authors":["jnxmas"],"tags":["meeting"]},"unlisted":false,"prevItem":{"title":"OPIK : AI Evaluation and Observability","permalink":"/fr/blog/opik-workshop-1"},"nextItem":{"title":"Hackathon kickoff meeting","permalink":"/fr/blog/encode-kickoff"}},"content":"> Location: Discord voice chat\\n> Attendees: jnxmas, Victor\\n\\n## Overview\\n\\nThis document summarizes a series of project meetings focused on building a community-focused AI application for a local election. The discussions cover team composition and recruitment, defining the project\'s scope for a hackathon, and outlining the technical architecture. Key activities include automating the processing of community contributions, developing a neutral chatbot to compare political programs, and initiating web crawling operations to gather data. The plan involves using technologies like Firecrawl, N8N, Pydantic, and a Retrieval-Augmented Generation (RAG) system, with a strong emphasis on collaborative development practices via GitHub.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Key Topics\\n\\n- Two new potential members from Audierne, France, were identified. They are developers focused on websites but are new to AI and Python, so they are considered to be starting from scratch.\\n- Their primary value is their familiarity with the local context of the project (audierne2026), making them a potential bridge to the local population. They are seen as good candidates for an \\"observer\\" role within the hackathon.\\n- They are connected to one of four local political lists, which is interested in participative community movements.\\n- An Indian developer named Satish, with whom a speaker previously worked on an AWS and Next.js project in a 2023 hackathon, is a potential collaborator. However, he is currently cautious about joining due to being busy with his job.\\n- A French individual named Max, who specializes in SEO and social media strategy, was mentioned but is not a coder.\\n- An Indian machine learning specialist, referred to as \\"Meher,\\" is considering joining. He is seen as a key \\"third guy\\" ML for the application architecture.\\n- There was a concern that the project\'s connection to a local election might disqualify it from the hackathon.\\n- A team member named Rebecca clarified in a general chat that the project is eligible to be submitted under the \\"social community impact\\" category.\\n- The team was advised that all questions should be posted in the general chat, as Rebecca will not be replying to DMs.\\n- Progress has been made on the project\'s GitHub Kanban board, with P0 tasks and some unprioritized tickets added.\\n- The data crawling phase (\\"filecrawl\\") is ready to start with an initial dataset of 150 links and 4,000 PDFs that require OCR. The speaker has paused this work to incorporate more contributions.\\n- The N8N orchestration workflow is nearly complete and ready for deployment.\\n  - It will be hosted on a server with a 6-core CPU and 16 gigabytes of RAM.\\n  - The team will consider moving to Vercel if the server cannot handle the load.\\n- N8N is a low-code/no-code platform for building workflows, and an example was shown for automating posts based on GitHub repository issues.\\n- The setup is encapsulated in Docker, making it easier to run.\\n- The project\'s code and tasks will be managed in the Ocapistan repository on GitHub.\\n- A key initial task is to push the ideation file (`ideation 13.1.2026`) to the main branch to serve as a foundation for future work.\\n- The team will use a feature branch workflow:\\n  - For each task, a developer will create a new feature branch from the main branch.\\n  - Once the task is complete, the branch will be pushed to the repository for review by others.\\n  - After review and debugging, the branch will be merged and closed.\\n- The team agreed to prioritize the Firecrawl operation as the starting point, despite its potential difficulty.\\n- Two team members will conduct parallel trials on different fire crawling tasks to gain experience and share learnings.\\n- Each member should get their own free Firecrawl API key, as they may need to use multiple free accounts by registering with different emails to maximize free API calls.\\n- The initial scraping task will target a list of PDFs from a specific URL (`script marie arete mary`).\\n- PDF processing will require testing various libraries like `pdf2ocr`, Tabula, and `pypdf` to find the most effective one.\\n- The team needs to develop a method to identify and set aside documents that only contain signatures, as this information has no value for the LLM.\\n- The project will use Python with type hinting and Pydantic for data handling to improve code quality.\\n- A modular \\"separation of concerns\\" approach will be used for the ETL (Extract, Transform, Load) process.\\n  - Extraction, transformation, and loading will be handled by separate workflows, likely implemented as three distinct classes.\\n  - This modularity will allow for different extraction methods (e.g., for plain text, HTML with Beautiful Soup, or OCR for PDFs) to be developed and called as needed.\\n- The project will use Ocapistan for code management, N8N for workflows, and a flexible AI provider handled by OPIC.\\n- Team members do not need to work at the same time but must communicate effectively. Progress will be tracked through changes in the project repository.\\n- Developers should assign tasks to themselves and break them down into smaller sub-tasks.\\n- Direct discussions will be necessary when merging work on the same files to resolve conflicts.\\n- **Current Process (Manual):**\\n  - Contributions are received via email.\\n  - They are manually reviewed against a \\"chart of contribution\\" to ensure they meet the criteria.\\n  - If approved, they are manually copy-pasted into a GitHub issue.\\n  - A daily summary of contributions is automatically posted to Facebook via M8N, but this post is anonymous and lacks detail.\\n- **Proposed Automated Process:**\\n  - The goal is to automate the entire workflow from email receipt to GitHub issue creation.\\n  - An AI agent will be developed to judge whether an incoming contribution respects the \\"chart of contribution.\\"\\n  - This agent will be part of the \\"OKAPI stand,\\" which will house all agents and the RAG system for the project.\\n- **Purpose:** After a contribution is validated and becomes a GitHub issue, a \\"creative agent\\" (also called the Okapi Sten proper) will process it.\\n- **Functionality:**\\n  - The agent generates an AI-made reply that contextualizes the new contribution.\\n  - It cross-references the submission with previous contributions in the same category to find echoes and avoid repetition.\\n  - The reply includes links to the sources used to construct the contextualization.\\n- **Handling New vs. Existing Topics:**\\n  - The workflow must handle two cases: when a contribution is for a brand-new category, and when it relates to a previously discussed topic.\\n  - In the latter case, the system will search existing issues and the RAG system to build a comprehensive answer.\\n- **LLM Testing:**\\n  - Grok has been used for initial testing. It was effective at searching for context online (including the audierne2026 project) and generating relevant, though coincidental, replies.\\n  - The team discussed that Grok\'s ability to search and synthesize information acts similarly to a basic RAG system.\\n- **RAG System Development:**\\n  - A dedicated RAG (Retrieval-Augmented Generation) system will be built to avoid repetitive outputs and manage context efficiently.\\n  - The team needs to decide how to store source links and other data for the RAG system, considering a NoSQL database like MongoDB for flexibility. A vector store will be used for training on gathered data.\\n- **Key Dates:**\\n  - The election preparation period is currently underway.\\n  - Contribution collection will continue until at least January 31.\\n  - The election day is around March 15-22.\\n- **February Focus:**\\n  - Work in February will focus purely on developing the chatbot.\\n  - The chatbot will be used to compare the programs of the four enlisted municipal lists.\\n  - It will be designed to provide neutral, impartial comparisons on topics like lodging, culture, and budget realism.\\n- **Using OPIK:**\\n  - The OPIK framework will be used to evaluate every AI interaction to ensure quality and impartiality.\\n  - The team is considering creating a feedback loop where OPIK\'s evaluations could automatically improve the prompts in N8N.\\n- **Maintaining Neutrality:**\\n  - A major challenge is ensuring the chatbot remains neutral and does not generate sycophantic or biased responses based on leading questions from users.\\n  - The system may need multiple prompts to check for constraints like budget, realism, and political neutrality before generating a reply.\\n\\n## Open Issues & Risks\\n\\n- It is unclear how the new team members from Audierne, who have limited technical experience in AI, will be integrated into the project.\\n- The availability of a key potential collaborator, Satish, is uncertain due to his current work commitments.\\n- The machine learning specialist, Meher has not yet confirmed if he will join the team.\\n- It is undecided how to best store source links and data for the RAG system, though a NoSQL database is being considered.\\n- It is unclear which LLMs (e.g., Gemini) will be chosen for the final implementation.\\n- A key challenge will be designing the chatbot to remain neutral and avoid generating biased responses to leading questions.\\n- The project\'s success depends on receiving a sufficient number of community contributions, which requires incentivizing and motivating people to participate.\\n\\n## Action Items\\n\\n- [ ] Prioritize project tasks.\\n- [ ] Set up a server for the Vaettir repo so the team can access it with a password.\\n- [ ] Start by building workflows, with more coding to begin next week.\\n- [ ] Set up an ollama platform to experiment with local LLMs.\\n- [ ] Push the `ideation 13.1.2026` file to the main Ocapistaine repository.\\n- [ ] Each team member to get a free Firecrawl API key.\\n- [ ] Begin experimenting with Firecrawl by creating a new branch on the Ocapistaine repository.\\n- [ ] Start working on scraping the documents from the \\"script marie arret\xe9 maririe\\" task.\\n- [ ] Test different PDF reading libraries (`pdf2ocr`, Tabula, `pypdf`) to determine the best option for the project.\\n\\n> **AI Suggestion**\\n> AI has identified the following issues that were not concluded in the meeting or lack clear action items; please pay attention:\\n>\\n> 1. **Critical Staffing and Team Formation Risk:** The project faces a significant risk of stalling due to unresolved team composition. Two key experts, developer Satish and machine learning specialist \\"Mayer,\\" have not committed to the project, leaving critical skill gaps. A clear action plan is needed to secure their participation or find qualified alternatives immediately to ensure the project can proceed.\\n> 2. **Unresolved Core Chatbot Neutrality:** A fundamental and unresolved challenge is how to technically implement and guarantee the election chatbot\'s neutrality and impartiality. There is no defined strategy for preventing the AI from giving biased responses, especially when faced with leading or manipulative user questions, which poses a major reputational and functional risk to the project\'s core objective.\\n> 3. **Lack of a Defined Community Contribution Workflow:** The entire process for receiving, validating, and integrating community-submitted content is undefined. This includes the creation of an AI agent to automate judging submissions and a clear workflow for handling both new and existing topics. Without this, the project cannot scale or effectively leverage community input, which is stated as a dependency for success.\\n> 4. **Undefined Technical Foundation for Data Processing and Storage:** Key decisions about the project\'s technical architecture are still pending. The team has not selected a database for the RAG system (e.g., NoSQL/MongoDB) or the specific Large Language Models (LLMs) for the final implementation. Furthermore, the method for processing varied document types like PDFs remains uncertain. These foundational decisions must be made to avoid delays in development."},{"id":"encode-kickoff","metadata":{"permalink":"/fr/blog/encode-kickoff","editUrl":"https://github.com/locki-io/docs.locki.io/tree/main/blog/2026-01-13-encode_hackathon.md","source":"@site/blog/2026-01-13-encode_hackathon.md","title":"Hackathon kickoff meeting","description":"Date & Time04:06","date":"2026-01-13T00:00:00.000Z","tags":[{"inline":false,"label":"Meeting","permalink":"/fr/blog/tags/meeting","description":"Meeting tag description"}],"readingTime":16.75,"hasTruncateMarker":true,"authors":[{"name":"Jean-No\xebl Schilling","title":"Locki one / french maintainer","url":"https://github.com/jnschilling","socials":{"linkedin":"https://www.linkedin.com/in/jnschilling/","github":"https://github.com/jnschilling","newsletter":"https://www.stratej.fr"},"imageURL":"https://github.com/jnschilling.png","key":"jnxmas","page":null}],"frontMatter":{"slug":"encode-kickoff","title":"Hackathon kickoff meeting","authors":["jnxmas"],"tags":["meeting"]},"unlisted":false,"prevItem":{"title":"Project scope - Victor + JNS","permalink":"/fr/blog/victor-intro-project-description"},"nextItem":{"title":"Locki Labs in 2025 - Introducing Valkyria","permalink":"/fr/blog/locki-in-2025"}},"content":"> Date & Time: 2026-01-13 19:04:06\\n> Location: online presentation\\n> `AI Agent Hackathon` `New Year\'s Resolutions` `AI Evaluation`\\n\\n## Theme\\n\\nThis lecture introduces the \\"Commit to Change AI Agent Hackathon,\\" a four-week online event challenging participants to build AI agents that help users stick to their New Year\'s resolutions. The event offers $30,000 in prizes across five tracks: Productivity, Personal Growth, Social Impact, Health/Wellness, and Financial Health. It emphasizes the importance of AI evaluations, defining them as structured measurements of system behavior. The lecture details the hackathon\'s timeline, submission requirements using the ENCODE platform, and the mandatory use of the OPIC tool for evaluation, guiding participants from ideation to final project submission.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Takeaways\\n\\n1.  Introduction to the Commit to Change AI Agent Hackathon.\\n2.  The hackathon\'s goal is to build AI that turns New Year\'s resolutions into real results.\\n3.  Hackathon registration is required on the Encode Cloud platform for all participants, including those from Luma.\\n4.  The hackathon is sponsored by Comet, with supporting partners Basel and Google DeepMind.\\n5.  This is a four-week online hackathon with up to $30,000 in prizes.\\n6.  Statistics on New Year\'s resolutions: 23% give up within 13 days, and 43% give up by the end of January.\\n7.  Hackathon advice: Start with a problem you have faced and build a unique solution.\\n8.  The hackathon features five thematic tracks: Productivity and Work Habits, Personal Growth and Learning, Social and Community Impact, Health, Fitness and Wellness, and Financial Health.\\n9.  It\'s recommended to focus on one category to create a high-quality, specialized app.\\n10. There is an overall challenge for the \'Best use of OPIC\' for projects showcasing excellent evaluation and observability.\\n\\n## Highlights\\n\\n- `\\"AI evals are what turn that sort of really cool, fun prototype into a system that you can actually iterate on with confidence and start to think about exposing to the real world and to real people.\\"-- Abby`\\n- `\\"What\'s even cooler than a really cool prototype is, in my opinion, a cool tool that you can actually use in the real world on real data. And for that, you need some sort of system of evaluations.\\"-- Abby`\\n- `\\"Evaluation turns guesswork into science.\\"-- Abby`\\n- `\\"AI evaluations are how we turn random iteration into progress, and they give us a repeatable way to measure behavior, compare changes, and improve systematically instead of guessing.\\"-- Abby`\\n\\n## Chapters & Topics\\n\\n### Hackathon Overview and Goal\\n\\n> The Commit to Change AI Agent Hackathon is a four-week online event starting in January 2026, aimed at building AI agents and LLM-powered apps that help people stick to their New Year\'s resolutions and goals. It offers up to $30,000 in prizes.\\n\\n- **Keypoints**\\n  - The hackathon runs for four weeks online.\\n  - The goal is to build AI/LLM apps to help users stick to their goals.\\n  - There\'s a prize pool of up to $30,000.\\n  - It is sponsored by Comet, with support from Basel and Google DeepMind.\\n  - Participants must register on the Encode Cloud platform to get all updates and access resources.\\n- **Explanation**\\n  The hackathon is structured to guide participants from ideation to final submission. It includes workshops, deadlines to ensure progress, and resources provided by sponsors like Comet and partners like Google DeepMind. The central theme is leveraging AI to address the common problem of people giving up on their resolutions. Statistics show 23% of people give up 13 days into January, and 43% give up by the end of the month, highlighting the target audience for the projects.\\n\\n### Hackathon Tracks and Challenges\\n\\n> The hackathon is structured around five thematic tracks for projects, plus an overall challenge. The tracks are: Productivity and Work Habits, Personal Growth and Learning, Social and Community Impact, Health, Fitness and Wellness, and Financial Health. The overall challenge is for the \'Best Use of OPIC\', rewarding projects with excellent evaluation and observability.\\n\\n- **Keypoints**\\n  - Productivity and Work Habits: Build tools for smarter work and better routines.\\n  - Personal Growth and Learning: Build apps for learning new skills or developing self-awareness.\\n  - Social and Community Impact: Build tools for organizing communities or supporting environmental/social action.\\n  - Health, Fitness and Wellness: Build solutions for fitness, mental health, or general well-being.\\n  - Financial Health: Build AI/LLM apps for budgeting, saving, or understanding money.\\n  - Best Use of OPIC: A special category for projects with excellent evaluation and observability.\\n- **Explanation**\\n  Participants should choose one of the five themes to focus their project on. The themes are broad to accommodate a wide range of ideas. The \'Health, Fitness and Wellness\' category is noted as a particularly common area for New Year\'s resolutions. In addition to the thematic challenges, all projects are eligible for the \'Best Use of OPIC\' challenge, sponsored by Comet. This special prize can be won in conjunction with a thematic prize.\\n- **Considerations**\\n- Focus on one category rather than trying to build an app that spans multiple, as an app that does one thing well is better than one that does four things less well.\\n- Start with a problem that you yourself have faced to understand it well.\\n- Come up with a solution that sets you apart from everyone else; build something interesting and unique.\\n- Don\'t just do the bare minimum base idea; build on top of it.\\n- The best use of OPIC category can be won alongside a theme prize, which is the only two-in-one win possible.\\n\\n### Hackathon Timeline and Submission Requirements\\n\\n> The hackathon has a structured timeline with key deadlines and specific submission requirements for the final project. The timeline spans four weeks, starting with ideation, moving to building, and culminating in a final submission.\\n\\n- **Keypoints**\\n  - Week 1: Ideation and Project Creation Deadline.\\n  - Week 2: Building and a workshop on Gemini 3.\\n  - Week 2/3: Mid-hackathon Deadline (submit GitHub repo and description).\\n  - Final Submission Deadline: February 8th, 23:59 UTC-12.\\n  - Required final submission items: Video pitch with demo and public code base.\\n  - Recommended submission items: Hosted site and a presentation.\\n  - The mid-hackathon deadline forces early and regular code commits.\\n- **Explanation**\\n  The timeline is designed to keep participants on track.\\n\\n- Week 1: Launch, ideation, and a \'Project Creation Deadline\' to commit to an idea and team.\\n- Week 2: Start building, with a workshop from Google DeepMind on Gemini 3.\\n- End of Week 2/Start of Week 3: \'Mid-hackathon Deadline\' requiring submission of a GitHub repo and project description. This is not judged but ensures progress.\\n- Final Submission Deadline: February 8th at one minute to midnight, UTC-12. This means you can submit if it\'s still before midnight anywhere in the world.\\n  Final submissions must include a video pitch with a demo, a public codebase, and optionally a hosted site and presentation slides. These requirements are designed to give judges a comprehensive view of the project.\\n- **Considerations**\\n- Do not leave commits until the last minute; make regular commits from the start.\\n- The video pitch is the first impression for judges and needs to capture their attention immediately.\\n- While AI can be used for presentations, it is not recommended to use AI to generate the entire video pitch as it can be \'soul destroying\' for judges.\\n- A hosted site and presentation are optional but highly recommended to strengthen your submission.\\n- Do not submit at the last minute to avoid technical issues or mistakes.\\n\\n### Using the ENCODE Platform\\n\\n> The ENCODE platform is the central hub for the hackathon. It contains all necessary information, resources, and submission portals for participants.\\n\\n- **Keypoints**\\n  - The ENCODE platform is the home for the hackathon.\\n  - Participants must register on the platform to get all emails and updates.\\n  - To create a project and team, use the \'create project\' and \'join team code\' features.\\n  - The platform has a \'Lecture\' section with helpful videos.\\n  - The \'Events\' page lists all workshop links.\\n  - Challenge descriptions and partner resources (Comet docs, Gemini credits, etc.) are available on the platform.\\n  - All submissions are made through the participant\'s personal hackathon page on the platform.\\n- **Explanation**\\n  Participants will use the ENCODE platform for all hackathon-related activities. After the introductory session, participants who joined from Luma must register for the hackathon on the platform to receive emails and updates. The platform is where you create your project, add team members using a \'join code\', find lecture videos, access event links for workshops, read detailed challenge descriptions, and find partner resources like the Comet developer documentation and OPIC quick start guide. All submissions, from the mid-hackathon check-in to the final project, will be done through this platform.\\n\\n### Definition of an AI Evaluation\\n\\n> An evaluation is a structured measurement of a system\'s behavior against criteria we care about. This involves defining the vision of success and failure for the application. The process is a structured way to measure an AI system\'s behavior against defined criteria on representative tasks to support decisions and improve product development.\\n\\n- **Keypoints**\\n  - It\'s a structured measurement of a system\'s behavior.\\n  - It\'s measured against predefined criteria that define success.\\n  - The hardest part is often figuring out the criteria you care about.\\n  - It\'s important to think about success, failure, and potential failure modes from the beginning.\\n  - Failure modes can vary, from harmful content and incorrect outputs to having the wrong tone for a brand.\\n  - An evaluation system is often a set of many individual metrics.\\n  - These individual metrics are aggregated into an overall success score.\\n- **Explanation**\\n  To unpack the definition: \'Structure\' means repeatable and explicit, allowing for comparisons over time. \'Behavior\' refers to what the system does, such as responses, tool calls, latency, or cost. \'Criteria\' is how success is predefined, including accuracy, helpfulness, safety, or product-specific outcomes. An evaluation system is typically a set of many metrics aggregated into an overall success score.\\n\\n### The Four Ingredients of a Good Evaluation\\n\\n> A good evaluation generally consists of four main ingredients: a target, a test set/task, a scoring method, and a decision rule. These components work together to form a repeatable and useful evaluation process.\\n\\n- **Keypoints**\\n  - A target: The specific capability or outcome being tested (e.g., factual QA, customer support resolution).\\n  - A test set or task: Examples representing the real world, including edge cases and outliers.\\n  - A scoring method: A metric, rubric, or judge (human or LLM) to score performance, including how individual scores are aggregated (e.g., math equation, voting system).\\n  - A decision rule: Determines what to do with the results, such as shipping a feature, rolling it back, or retraining. It often includes a threshold (e.g., \'if the success rate improves by at least 20%, then ship it\').\\n- **Explanation**\\n  First, the \'target\' specifies the capability or outcome being tested (e.g., factual QA, tool use). Second, the \'test set\' includes real-world examples and edge cases. Third, the \'scoring method\' defines how performance is measured (e.g., a metric, rubric, LLM judge) and how scores are aggregated. Finally, the \'decision rule\' dictates the action to be taken based on the evaluation score, such as shipping a feature or rolling it back, often based on a threshold or comparison to a baseline.\\n\\n### Difference Between Benchmarks and Application Evals\\n\\n> Benchmarks are standardized test sets, often from academia, used for broad comparisons across different models. Application evaluations are specific to a product, matching its real distribution of prompts, workflows, and constraints, and are used to determine if a system is good enough to ship.\\n\\n- **Keypoints**\\n  - Benchmarks are standardized tests for comparing models.\\n  - Application evals are product-specific tests for system performance.\\n  - Benchmarks assess general ability; application evals assess behavior in a specific context.\\n  - Benchmarks score the model in isolation; application evals test the entire system (prompts, RAG, tools).\\n  - The biggest gap is distribution: benchmarks rarely match real traffic, edge cases, or domain language.\\n  - A model can score high on a benchmark but fail in your specific application context.\\n  - Benchmarks help narrow model choices; product evals tell you if the system is ready to ship.\\n- **Explanation**\\n  Benchmarks are useful for getting a quick read on a model\'s general capabilities and comparing model families (e.g., comparing one foundation model to another). However, a model can perform well on a benchmark and still fail in production because production success depends on behavior in a specific context. Application evals test the entire system\u2014including prompts, RAG, and tool use\u2014against your specific definition of success. The main gap is that benchmarks rarely match the distribution of real-world traffic, including edge cases and domain-specific language.\\n\\n### Challenges in Evaluating LLMs\\n\\n> Evaluating LLMs and agentic systems is inherently difficult due to several unique challenges that distinguish them from traditional software. These challenges include non-determinism, the subjectivity of tasks, high sensitivity to inputs, and the possibility of silent failures.\\n\\n- **Keypoints**\\n  - LLMs are non-deterministic: The same input does not guarantee the same output, making traditional monitoring difficult.\\n  - Many AI tasks lack a single correct answer: Tasks like summarization or content generation can have multiple valid outputs, making simple match comparisons insufficient.\\n  - Heuristic methods are largely ineffective: Simple heuristics like regex or pattern matching don\'t consider semantic meaning.\\n  - Evaluation metrics are subjective: Concepts like relevance, coherence, and conciseness are open to interpretation.\\n  - Human feedback is imperfect: It is expensive, can be inconsistent, and is difficult to scale.\\n  - LLMs are extremely sensitive to prompts and context: Small changes can drastically alter the output.\\n  - LLMs have silent failures: A system can produce a correct output but use flawed or unsafe reasoning to get there, which is a failure that isn\'t immediately obvious.\\n  - There\'s no standard set of evaluation metrics applicable to all products.\\n- **Explanation**\\n  LLMs are non-deterministic, meaning the same input can produce different outputs, making hard-coded logic and simple error handling ineffective. Many tasks like summarization have no single correct answer, so simple matching (like regex) fails; semantic meaning must be considered. Metrics like \'relevance\' are subjective, and even human annotators may disagree. LLMs are also very sensitive to small changes in prompts or context. Finally, they can have \'silent failures,\' where the final output appears correct, but the underlying reasoning was flawed or discriminatory, which can only be caught by observing the entire process.\\n- **Examples**\\n  > Traditional software monitoring uses tools like try-and-accept clauses to handle anticipated errors. However, with LLMs, we cannot anticipate every single possible output they might produce due to their non-deterministic nature. Therefore, a simple accept clause is not a scalable or effective solution for handling LLM failures.\\n\\n### Challenges in Monitoring AI Agents\\n\\n> Monitoring AI agents is significantly more complex than monitoring standalone Large Language Models (LLMs) because agents are composed of multiple LLMs and external tools, leading to compounded variability and numerous failure modes.\\n\\n- **Keypoints**\\n  - Agents are built on top of LLMs, inheriting their non-deterministic nature.\\n  - Chaining multiple LLM calls compounds variability and potential for error.\\n  - Agents are multi-step workflows with more moving parts and thus more failure modes.\\n  - The use of external tools introduces external dependencies, potential silent failures, and added unpredictability.\\n  - Dynamic memory and context in agents can lead to intent drift and performance degradation over time.\\n  - Evaluation of agents must include not just the final output, but also the reasoning, tool choice, and every step along the way.\\n- **Explanation**\\n  Agents are built on top of LLMs, which are non-deterministic systems. When you chain multiple LLM calls together, as is common in agents, the variability and potential for error at the beginning of the chain can be amplified through subsequent steps. Agents are also multi-step workflows with more moving parts, external tool dependencies (which can have silent failures), and dynamic memory/context. This complexity increases the number of failure modes and necessitates a more thorough evaluation process that goes beyond just the final output to include reasoning, tool choice, and each intermediate step. As agents are deployed in real-world scenarios with more complex workflows and higher usage frequency, tracking all these aspects becomes extremely difficult.\\n- **Examples**\\n  > A user engaged with a dealership\'s chatbot and managed to get it to agree to sell them a Chevy Tahoe for one US dollar. The chatbot even stated \'no takesies backsies\'. This agreement was legally upheld in court, forcing the dealership to sell the car for one dollar.\\n  - This example illustrates a real-world consequence of an AI agent (chatbot) going wrong.\\n  - The non-deterministic and unconstrained nature of the LLM powering the chatbot led to an unintended and costly outcome for the dealership.\\n  - It highlights the critical need for robust monitoring and evaluation of agent behavior to prevent such incidents.\\n  - The phrase \'no takesies backsies\' being considered legally binding underscores how interactions with AI can have unforeseen legal ramifications.\\n\\n### Hackathon Logistics and Rules\\n\\n> The \'Resolve to Evolve\' Hackathon is an event where participants build AI or LLM-powered applications to help people maintain their New Year\'s resolutions. The event has specific rules regarding submissions, team formation, judging, and tooling.\\n\\n- **Keypoints**\\n  - Objective: Build an AI/LLM-powered app to help with New Year\'s resolutions.\\n  - Team Formation: Solo or teams of any size are permitted (recommendation: 5 or less).\\n  - Submissions: You can submit to multiple tracks but win at most one.\\n  - Prizing: $5,000 for each of the five themes, plus a $5,000 special prize for the best use of OPIC.\\n  - Required Tooling: OPIC must be used for evaluation.\\n  - Allowed Tooling: Any LLM (e.g., Gemini, etc.) can be used.\\n  - Submission deliverable: A video demo is a mandatory and critical part of the submission, presenting the project and its functionality.\\n  - Project Scope: Build an MVP. Full production-ready apps are not expected.\\n  - Timeline: The hackathon has started, and coding can begin now. It lasts approximately 28 days.\\n  - Prior Work: You can build on a pre-existing project, but only functionality added during the hackathon will be judged.\\n- **Explanation**\\n  Participants are tasked with creating an MVP (Minimum Viable Product) of a web or mobile app. They can work solo or in teams of any size, though teams of 5 or fewer are recommended. Submissions can target multiple prize categories, but a project can only win one. Judging criteria are available on the hackathon platform. A key component of the submission is a video demo that serves as both a product pitch and a functional walkthrough. While participants can use any LLM, they are required to use OPIC for evaluation. The hackathon provides access to partner services with generous free tiers instead of specific credits. Participants can start coding immediately and can even build upon existing projects, but only work done during the hackathon period will be judged.\\n- **Special Circumstances**\\n- If building a mobile app that cannot be easily shared or hosted for judging, a very thorough demo video showing all functionality is sufficient.\\n- If you want to work on a project you have already started, you can, but you will only be judged on the new functionality and features built during the hackathon period.\\n\\n## Assignments & Suggestions\\n\\n- If you are joining from Luma, go into your programs page and register for the hackathon after the session.\\n- If you have questions that aren\'t answered live, put them in the Q&A to be answered in the Discord.\\n- Decide on the project you want to build, the theme to focus on, the solution to come up with, and what makes it special during the ideation stage in week one.\\n- By the project creation deadline at the end of week one, create and commit to the project idea, the challenge theme, and add team members.\\n- Start the building process in week two.\\n- For the mid-hackathon deadline, submit your publicly available GitHub repo and a fuller description of what you\'re building.\\n- For the final submission deadline on February 8th, submit a video pitch including a product demo, your public code base, a hosted site (optional but recommended), and a presentation (optional but recommended).\\n- Watch the helpful, short lecture videos on the platform to prepare for workshops and the wider hackathon.\\n- Run the code from the QR code provided to see how evaluations are created and look in OPIC. The QR code leads to a simple recipe generator agent in a GitHub gist. You need to copy the script, follow the directions for creating online evaluations, create a Comet account if you don\'t have one (it\'s free), and add your API key."},{"id":"locki-in-2025","metadata":{"permalink":"/fr/blog/locki-in-2025","editUrl":"https://github.com/locki-io/docs.locki.io/tree/main/blog/2025-01-15-locki-in-2025.mdx","source":"@site/blog/2025-01-15-locki-in-2025.mdx","title":"Locki Labs in 2025 - Introducing Valkyria","description":"After months of development on our horse racing analysis platform, we\'re taking a bold step forward. In 2025, Locki Labs shifts focus from real-time data aggregation to something far more ambitious: Valkyria \u2014 a temporal prediction system designed for scientific rigor and reproducible results.","date":"2025-01-15T00:00:00.000Z","tags":[{"inline":false,"label":"Locki Labs","permalink":"/fr/blog/tags/locki-labs","description":"Startup adventures at Locki Labs still in formation"},{"inline":false,"label":"AI and Machine Learning","permalink":"/fr/blog/tags/ai-ml","description":"Articles on AI, machine learning, and related technologies"},{"inline":false,"label":"Temporal Predictions","permalink":"/fr/blog/tags/temporal-predictions","description":"Insights into temporal prediction models and applications"},{"inline":false,"label":"Machine Learning","permalink":"/fr/blog/tags/machine-learning","description":"Articles on machine learning and related technologies"}],"readingTime":3.94,"hasTruncateMarker":true,"authors":[{"name":"Jean-No\xebl Schilling","title":"Locki one / french maintainer","url":"https://github.com/jnschilling","socials":{"linkedin":"https://www.linkedin.com/in/jnschilling/","github":"https://github.com/jnschilling","newsletter":"https://www.stratej.fr"},"imageURL":"https://github.com/jnschilling.png","key":"jnxmas","page":null}],"frontMatter":{"slug":"locki-in-2025","title":"Locki Labs in 2025 - Introducing Valkyria","authors":["jnxmas"],"tags":["locki","ai-ml","temporal-predictions","machine-learning"]},"unlisted":false,"prevItem":{"title":"Hackathon kickoff meeting","permalink":"/fr/blog/encode-kickoff"},"nextItem":{"title":"Locki Labs: From Hackathon to Production - Navigating Blockchain Evolution and Data Challenges","permalink":"/fr/blog/locki-journey-2023-2025"}},"content":"After months of development on our horse racing analysis platform, we\'re taking a bold step forward. In 2025, Locki Labs shifts focus from real-time data aggregation to something far more ambitious: **Valkyria** \u2014 a temporal prediction system designed for scientific rigor and reproducible results.\\n\\n\x3c!-- truncate --\x3e\\n\\n## The Problem We Set Out to Solve\\n\\nOur existing platform excels at real-time race analysis. But we discovered a fundamental limitation when it came to developing prediction models: **temporal blindness**.\\n\\nWhen analyzing a historical race from, say, November 16th, our system would use horse data from _today_ \u2014 including races that happened _after_ the race we\'re analyzing. This is called **data leakage**, and it makes prediction models unreliable.\\n\\n```\\nScenario: Analyzing a race from November 16, 2025\\n\\n\u274c Current approach:\\n   Uses horse career data as of TODAY\\n   \u2192 Includes future races (data leakage)\\n   \u2192 Predictions are scientifically invalid\\n\\n\u2705 Valkyria approach:\\n   Uses horse career data as of November 16\\n   \u2192 Only past information available\\n   \u2192 Predictions are reproducible and valid\\n```\\n\\n## What is Valkyria?\\n\\nValkyria is a **temporal prediction laboratory** \u2014 a system that can reproduce predictions with historical accuracy. The core principle is simple but powerful:\\n\\n> **Analyze any race using only the data that would have been available at that exact moment in time.**\\n\\nThis enables:\\n\\n- **Time-Travel Analysis**: Query any race from the past 18 months with temporally-accurate data\\n- **Model Training**: Train prediction algorithms on clean, causally-valid datasets\\n- **Backtesting**: Validate strategies on historical races without information leakage\\n- **Reproducibility**: Same race, same analysis, same results \u2014 every time\\n\\n## The Three-Tier Architecture\\n\\nValkyria operates on a three-tier temporal system:\\n\\n### Tier 1: Real-Time (48 hours)\\n\\nLive operations for current and upcoming races. Odds tracking, race analysis, chat features \u2014 all powered by Redis cache with 15-minute refresh cycles.\\n\\n### Tier 2: Recent History (7 days)\\n\\nFast access to recently finished races. Runner snapshots stored in Redis with file backup for durability.\\n\\n### Tier 3: Historical Archive (18 months)\\n\\n**This is the innovation.** A SQLite-based temporal database containing runner snapshots \u2014 immutable records of each horse\'s state at race time. No future information, ever.\\n\\n## Campaign-Based Population\\n\\nRather than fetching all historical data upfront, Valkyria uses a **campaign-based approach**:\\n\\n| Campaign | Period       | Estimated Races | Status   |\\n| -------- | ------------ | --------------- | -------- |\\n| Q4-2025  | Oct-Dec 2025 | ~4,500          | Priority |\\n| Q3-2025  | Jul-Sep 2025 | ~4,500          | Next     |\\n| Q2-2025  | Apr-Jun 2025 | ~4,500          | Planned  |\\n\\nEach campaign represents a 3-month slice of racing data, populated incrementally during off-peak hours. The database grows organically while maintaining full temporal accuracy.\\n\\n## Model Unification: One Model, Multiple Lifecycles\\n\\nA key architectural decision: **CanonicalRunner** becomes the single source of truth across all lifecycle stages:\\n\\n1. **Pre-Race**: Upcoming participant with updating odds\\n2. **Post-Race**: Finished runner with final results\\n3. **Career History**: Stored snapshot for temporal queries\\n\\nThis eliminates 88% of field duplication from our previous dual-model system and ensures consistency across the entire platform.\\n\\n## What This Means for Predictions\\n\\nWith Valkyria, we can finally build prediction models with scientific integrity:\\n\\n```\\nHistorical snapshots (18 months)\\n    \u2193\\nFeature engineering (career metrics, confrontations)\\n    \u2193\\nModel training (XGBoost, Neural Networks)\\n    \u2193\\nBacktesting (temporal validation)\\n    \u2193\\nProduction deployment (high confidence)\\n```\\n\\nWe\'ll be able to:\\n\\n- Compare multiple models on the same historical data\\n- Calculate accuracy by race type (HARNESS, FLAT, Quint\xe9)\\n- Calibrate confidence scores based on actual performance\\n- Generate algorithmic selections with transparent methodology\\n\\n## The Road Ahead\\n\\nValkyria development follows a phased approach:\\n\\n**Foundation** \u2014 Database schema, storage functions, basic snapshot capabilities\\n\\n**Daily Population** \u2014 Automated jobs to capture yesterday\'s races every night\\n\\n**Workflow Integration** \u2014 Career workflows query the temporal database for historical analysis\\n\\n**Campaign Population** \u2014 Bulk population of 3-month historical periods\\n\\n**Model Unification** \u2014 Full migration to CanonicalRunner across all analyzers\\n\\nEach phase delivers value independently. If we complete only the foundation and daily population, we still have a working temporal snapshot system. The full vision builds incrementally.\\n\\n## Storage & Retention\\n\\nThe numbers are reassuring:\\n\\n- **18 months of racing**: ~27,000 races, ~324,000 runner snapshots\\n- **Storage requirement**: ~486 MB\\n- **Cleanup policy**: Monthly removal of data older than 18 months\\n\\nSQLite handles this efficiently, and the system remains lightweight.\\n\\n## Why This Matters\\n\\nWithout temporal accuracy, predictions are anecdotal. With Valkyria:\\n\\n- \u2705 **Causal validity**: Only past data influences predictions\\n- \u2705 **Reproducibility**: Results can be verified and replicated\\n- \u2705 **Testability**: Objective performance metrics across historical data\\n- \u2705 **Confidence**: Know when and where models perform reliably\\n\\nThis isn\'t just a technical improvement \u2014 it\'s the foundation for next-generation prediction algorithms built on scientific rigor rather than intuition.\\n\\n## Looking Forward\\n\\n2025 marks a transition for Locki Labs. We\'re moving from \\"what does the data say right now?\\" to \\"what would we have known then, and how can we learn from it?\\"\\n\\nValkyria represents our commitment to building prediction systems we can trust, test, and continuously improve. One snapshot at a time.\\n\\n---\\n\\n_Stay tuned for updates as we progress through the Valkyria implementation phases._"},{"id":"locki-journey-2023-2025","metadata":{"permalink":"/fr/blog/locki-journey-2023-2025","editUrl":"https://github.com/locki-io/docs.locki.io/tree/main/blog/2023-09-29-locki-labs-journey.mdx","source":"@site/blog/2023-09-29-locki-labs-journey.mdx","title":"Locki Labs: From Hackathon to Production - Navigating Blockchain Evolution and Data Challenges","description":"Locki started as a vision during the Encode MultiversX Hackathon in October 2023, with the ambitious goal of creating a decentralized platform for minting, viewing, and interacting with 3D Data NFTs using the Itheum protocol.","date":"2024-01-15T00:00:00.000Z","tags":[{"inline":false,"label":"Locki Labs","permalink":"/fr/blog/tags/locki-labs","description":"Startup adventures at Locki Labs still in formation"},{"inline":false,"label":"Blockchain","permalink":"/fr/blog/tags/blockchain","description":"Articles related to blockchain technology"},{"inline":false,"label":"MultiversX","permalink":"/fr/blog/tags/multiversx","description":"Content about the MultiversX ecosystem"},{"inline":false,"label":"Itheum","permalink":"/fr/blog/tags/itheum","description":"Posts regarding Itheum platform"},{"inline":false,"label":"RAG","permalink":"/fr/blog/tags/rag","description":"Retrieval-Augmented Generation topics"},{"inline":false,"label":"Web3 Technologies","permalink":"/fr/blog/tags/web3","description":"Discussions on Web3 technologies and trends"}],"readingTime":3.43,"hasTruncateMarker":true,"authors":[{"name":"Jean-No\xebl Schilling","title":"Locki one / french maintainer","url":"https://github.com/jnschilling","socials":{"linkedin":"https://www.linkedin.com/in/jnschilling/","github":"https://github.com/jnschilling","newsletter":"https://www.stratej.fr"},"imageURL":"https://github.com/jnschilling.png","key":"jnxmas","page":null}],"frontMatter":{"slug":"locki-journey-2023-2025","title":"Locki Labs: From Hackathon to Production - Navigating Blockchain Evolution and Data Challenges","authors":["jnxmas"],"tags":["locki","blockchain","multiversx","itheum","rag","web3"],"date":"2024-01-15T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Locki Labs in 2025 - Introducing Valkyria","permalink":"/fr/blog/locki-in-2025"},"nextItem":{"title":"Locki blog introduction","permalink":"/fr/blog/welcome"}},"content":"Locki started as a vision during the **Encode MultiversX Hackathon in October 2023**, with the ambitious goal of creating a decentralized platform for minting, viewing, and interacting with 3D Data NFTs using the Itheum protocol.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Current Status\\n\\n:::info Project Status\\n\\nLocki dApp is currently in **alpha phase** on MultiversX devnet, and has been supported by the **xPand Itheum program**.\\n\\n:::\\n\\nThe platform has evolved significantly since its inception, now featuring:\\n\\n- **3D Data NFT minting and visualization** using Three.js and React Three Fiber\\n- **AI-powered chat assistant** for Blender Python scripting guidance\\n- **Whitelist management system** for controlled access\\n- **Integration with MultiversX wallets** including xAlias support\\n\\n## The Challenge of Collecting RAG Data and Intellectual Property\\n\\nOne of the most significant hurdles we\'ve encountered is **building a quality dataset for our RAG (Retrieval-Augmented Generation) system** while respecting intellectual property rights.\\n\\n### The IP Dilemma\\n\\nCreating an effective AI assistant for 3D modeling and Blender scripting requires extensive training data:\\n\\n- **Documentation licensing**: Much of the best Blender documentation exists under various licenses that complicate commercial use\\n- **Community content**: Forum posts, tutorials, and Stack Exchange answers have complex attribution requirements\\n- **Proprietary scripts**: Many advanced Blender scripts are commercial products with restrictive licenses\\n- **Version fragmentation**: Blender\'s rapid release cycle means documentation quickly becomes outdated\\n\\n### Our Approach\\n\\nWe\'ve had to carefully curate our data sources, focusing on:\\n\\n1. **Open-source documentation** with permissive licenses\\n2. **Original content creation** to fill knowledge gaps\\n3. **User-generated content** with explicit consent\\n4. **Partnerships** with content creators in the Blender ecosystem\\n\\n:::tip Locki Companion Status\\n\\nThe Locki companion AI feature is currently **on pause** while we navigate these data collection challenges and ensure full compliance with IP rights.\\n\\n:::\\n\\n## Technical Challenges: The Blockchain Moving Target\\n\\nPerhaps the most demanding aspect of maintaining Locki has been keeping pace with the **rapid evolution of blockchain infrastructure**.\\n\\n### SDK Version Churn\\n\\nThe MultiversX ecosystem has undergone substantial changes:\\n\\n```json\\n// Our current dependency snapshot\\n\\"@multiversx/sdk-core\\": \\"12.18.0\\",\\n\\"@multiversx/sdk-dapp\\": \\"2.28.7\\",\\n\\"@multiversx/sdk-network-providers\\": \\"2.3.0\\",\\n\\"@itheum/sdk-mx-data-nft\\": \\"2.7.0-beta.4\\"\\n```\\n\\nEach major version brings:\\n\\n- **Breaking API changes** requiring code refactoring\\n- **New authentication patterns** (we\'ve integrated xAlias, native auth tokens)\\n- **Smart contract interface updates** affecting our ABI integrations\\n- **Wallet provider deprecations** forcing migration to new connection methods\\n\\n### Specific Challenges We\'ve Faced\\n\\n1. **SSR Compatibility**: Next.js App Router with blockchain SDKs requires careful dynamic imports to avoid server-side rendering issues\\n\\n2. **Transaction Handling**: The move from legacy transaction signing to the new `signAndSendTransactions` utility required substantial refactoring\\n\\n3. **Smart Contract ABIs**: Contract updates on devnet mean our local ABIs must be continuously synchronized\\n\\n4. **Network Provider Changes**: API endpoint changes and rate limiting adjustments have required multiple backend adaptations\\n\\n### The Maintenance Burden\\n\\n```\\nRecent commits reflecting ongoing maintenance:\\n- \\"Upgrade: Status locki companion\\"\\n- \\"fix vulnerabilities\\"\\n- \\"Fixed build issues\\"\\n- \\"Fixes smart contract dependencies and code refactoring\\"\\n```\\n\\nEach vulnerability fix in upstream dependencies cascades through our codebase. The security-conscious nature of blockchain applications means we cannot defer these updates.\\n\\n## Lessons Learned\\n\\n### For Blockchain Developers\\n\\n1. **Abstract your SDK interactions**: Create wrapper classes (like our `baseSmartContract.ts`) to isolate breaking changes\\n2. **Pin versions carefully**: Use exact versions in production, but test against latest regularly\\n3. **Monitor ecosystem announcements**: Join Discord channels and follow GitHub releases\\n4. **Build with deprecation in mind**: What works today may be obsolete in 6 months\\n\\n### For AI/RAG Builders\\n\\n1. **Document your data sources**: Track provenance from day one\\n2. **Build consent mechanisms**: Make it easy for users to contribute data with clear licensing\\n3. **Plan for re-training**: Your model will need updates as source material evolves\\n4. **Consider synthetic data**: Generate training examples where real data is restricted\\n\\n## What\'s Next for Locki\\n\\nDespite these challenges, we remain committed to the vision:\\n\\n- **Mainnet deployment** once alpha testing is complete\\n- **Enhanced 3D NFT features** with improved GLTF/GLB support\\n- **Expanded AI capabilities** with properly licensed training data\\n- **Community governance** for whitelist and platform decisions\\n\\n---\\n\\n_Locki is an open-source project. We welcome contributors who share our passion for decentralized 3D asset ownership and AI-assisted creativity._\\n\\n**Special Collections on MultiversX Devnet:**\\n\\n- `DATANFTFT-e0b917`\\n- `I3TICKER-03e5c2`\\n- `COLNAMA-539838`"},{"id":"welcome","metadata":{"permalink":"/fr/blog/welcome","editUrl":"https://github.com/locki-io/docs.locki.io/tree/main/blog/2021-08-26-welcome/index.md","source":"@site/blog/2021-08-26-welcome/index.md","title":"Locki blog introduction","description":"Docusaurus blogging features are powered by the blog plugin.","date":"2021-08-26T00:00:00.000Z","tags":[{"inline":false,"label":"Facebook","permalink":"/fr/blog/tags/facebook","description":"Facebook tag description"},{"inline":false,"label":"Hello","permalink":"/fr/blog/tags/hello","description":"Hello tag description"}],"readingTime":0.85,"hasTruncateMarker":true,"authors":[{"name":"Jean-No\xebl Schilling","title":"Locki one / french maintainer","url":"https://github.com/jnschilling","socials":{"linkedin":"https://www.linkedin.com/in/jnschilling/","github":"https://github.com/jnschilling","newsletter":"https://www.stratej.fr"},"imageURL":"https://github.com/jnschilling.png","key":"jnxmas","page":null}],"frontMatter":{"slug":"welcome","title":"Locki blog introduction","authors":["jnxmas"],"tags":["facebook","hello"]},"unlisted":false,"prevItem":{"title":"Locki Labs: From Hackathon to Production - Navigating Blockchain Evolution and Data Challenges","permalink":"/fr/blog/locki-journey-2023-2025"}},"content":"[Docusaurus blogging features](https://docusaurus.io/docs/blog) are powered by the [blog plugin](https://docusaurus.io/docs/api/plugins/@docusaurus/plugin-content-blog).\\n\\n:::tip\\nThe blog is interesting because it allows our agent in cursor and claude code to dive into our reflexion to have a set of the status and the priorities of the project.\\nUse it referencing our discussion for your feature branches.\\n:::\\n\\n\x3c!-- truncate --\x3e\\n\\n:::tip\\nThe blog is interesting because it allows our agent in cursor and claude code to dive into our reflexion to have a set of the status and the priorities of the project.\\nUse it referencing our discussion for your feature branches.\\n:::\\nSimply add Markdown files (or folders) to the `blog` directory.\\n\\nRegular blog authors can be added to `authors.yml`.\\n\\nThe blog post date can be extracted from filenames, such as:\\n\\n- `2019-05-30-welcome.md`\\n- `2019-05-30-welcome/index.md`\\n\\nA blog post folder can be convenient to co-locate blog post images:\\n\\n![Docusaurus Plushie](./docusaurus-plushie-banner.jpeg)\\n\\nThe blog supports tags as well!"}]}}')}}]);