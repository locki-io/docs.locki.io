"use strict";(globalThis.webpackChunkdocs_locki_io=globalThis.webpackChunkdocs_locki_io||[]).push([[9419],{8245(e,n,i){i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>d,default:()=>h,frontMatter:()=>l,metadata:()=>r,toc:()=>a});const r=JSON.parse('{"id":"app/opik/EXPERIMENT_WORKFLOW","title":"Experiment Workflow","description":"This document describes the prompt optimization experiment workflow using Opik\'s native evaluation API.","source":"@site/docs/app/opik/EXPERIMENT_WORKFLOW.md","sourceDirName":"app/opik","slug":"/app/opik/EXPERIMENT_WORKFLOW","permalink":"/fr/docs/app/opik/EXPERIMENT_WORKFLOW","draft":false,"unlisted":false,"editUrl":"https://github.com/locki-io/docs.locki.io/tree/main/docs/app/opik/EXPERIMENT_WORKFLOW.md","tags":[],"version":"current","frontMatter":{}}');var s=i(4848),t=i(8453);const l={},d="Experiment Workflow",c={},a=[{value:"Overview",id:"overview",level:2},{value:"Architecture",id:"architecture",level:2},{value:"Two LLMs",id:"two-llms",level:2},{value:"Task LLM (Sidebar)",id:"task-llm-sidebar",level:3},{value:"Judge LLM (Opik)",id:"judge-llm-opik",level:3},{value:"Available Metrics",id:"available-metrics",level:2},{value:"Opik Built-in Metrics (LLM Judges)",id:"opik-built-in-metrics-llm-judges",level:3},{value:"Custom Metrics",id:"custom-metrics",level:3},{value:"Output Format Metric",id:"output-format-metric",level:3},{value:"Workflow Components",id:"workflow-components",level:2},{value:"1. workflow_dataset.py",id:"1-workflow_datasetpy",level:3},{value:"2. workflow_experiment.py",id:"2-workflow_experimentpy",level:3},{value:"Dataset Item Format",id:"dataset-item-format",level:2},{value:"Agent Feature Registry",id:"agent-feature-registry",level:2},{value:"Pre-experiment Cleanup",id:"pre-experiment-cleanup",level:2},{value:"Error Patterns Detected",id:"error-patterns-detected",level:3},{value:"Using Cleanup",id:"using-cleanup",level:3},{value:"Cleanup in Experiments",id:"cleanup-in-experiments",level:3},{value:"Evaluation Flow",id:"evaluation-flow",level:2},{value:"Admin Dashboard Integration",id:"admin-dashboard-integration",level:2},{value:"Scheduled Task",id:"scheduled-task",level:2},{value:"Files",id:"files",level:2},{value:"References",id:"references",level:2}];function o(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"experiment-workflow",children:"Experiment Workflow"})}),"\n",(0,s.jsx)(n.p,{children:"This document describes the prompt optimization experiment workflow using Opik's native evaluation API."}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"The experiment workflow enables systematic evaluation and optimization of LLM prompts used by OCapistaine agents. It leverages Opik's built-in LLM judge metrics to assess prompt quality."}),"\n",(0,s.jsx)(n.h2,{id:"architecture",children:"Architecture"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Experiment Workflow                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502   Dataset   \u2502\u2500\u2500\u2500\u25b6\u2502  Evaluation \u2502\u2500\u2500\u2500\u25b6\u2502   Opik Platform     \u2502 \u2502\n\u2502  \u2502   (Opik)    \u2502    \u2502    Task     \u2502    \u2502   (Results)         \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                            \u2502                                    \u2502\n\u2502                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510                            \u2502\n\u2502                     \u25bc             \u25bc                            \u2502\n\u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502\n\u2502              \u2502 Task LLM \u2502  \u2502 Judge LLM    \u2502                    \u2502\n\u2502              \u2502 (Sidebar)\u2502  \u2502 (OpenAI)     \u2502                    \u2502\n\u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2502\n\u2502                                                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.h2,{id:"two-llms",children:"Two LLMs"}),"\n",(0,s.jsx)(n.p,{children:"The experiment uses two separate LLMs for different purposes:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"LLM"}),(0,s.jsx)(n.th,{children:"Purpose"}),(0,s.jsx)(n.th,{children:"Configuration"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Task LLM"})}),(0,s.jsx)(n.td,{children:"Runs the actual Forseti agent (validation/classification)"}),(0,s.jsx)(n.td,{children:"Sidebar selection (Gemini, Claude, Mistral, Ollama)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Judge LLM"})}),(0,s.jsx)(n.td,{children:"Runs Opik's evaluation metrics (Hallucination, Moderation)"}),(0,s.jsx)(n.td,{children:"Admin dashboard (default: OpenAI gpt-4o-mini)"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"task-llm-sidebar",children:"Task LLM (Sidebar)"}),"\n",(0,s.jsx)(n.p,{children:"The task LLM is the same provider selected in the sidebar. It executes:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Charter validation (",(0,s.jsx)(n.code,{children:"charter_validation"})," span)"]}),"\n",(0,s.jsxs)(n.li,{children:["Category classification (",(0,s.jsx)(n.code,{children:"category_classification"})," span)"]}),"\n",(0,s.jsx)(n.li,{children:"Other agent features"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"judge-llm-opik",children:"Judge LLM (Opik)"}),"\n",(0,s.jsx)(n.p,{children:"The judge LLM runs Opik's built-in metrics. Configuration stored in Redis db=5:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Default configuration\n{\n    "provider": "openai",\n    "model": "gpt-4o-mini",\n    "api_key_env": "OPENAI_API_KEY"\n}\n'})}),"\n",(0,s.jsxs)(n.p,{children:["Configure via Admin dashboard: ",(0,s.jsx)(n.strong,{children:"Opik Judge LLM"})," section."]}),"\n",(0,s.jsx)(n.h2,{id:"available-metrics",children:"Available Metrics"}),"\n",(0,s.jsx)(n.h3,{id:"opik-built-in-metrics-llm-judges",children:"Opik Built-in Metrics (LLM Judges)"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Metric"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"hallucination"})}),(0,s.jsx)(n.td,{children:"Detects generated false information"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"moderation"})}),(0,s.jsx)(n.td,{children:"Checks adherence to content standards"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"answer_relevance"})}),(0,s.jsx)(n.td,{children:"Evaluates how well the answer fits the question"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"context_recall"})}),(0,s.jsx)(n.td,{children:"Measures retrieval of relevant context (RAG)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"context_precision"})}),(0,s.jsx)(n.td,{children:"Measures precision of retrieved context (RAG)"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"custom-metrics",children:"Custom Metrics"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Metric"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"charter_compliance"})}),(0,s.jsx)(n.td,{children:"Checks if is_valid matches expected output"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"confidence"})}),(0,s.jsx)(n.td,{children:"Checks if confidence meets threshold"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"output_format"})}),(0,s.jsx)(n.td,{children:"Measures output format compliance (0-1 scale)"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"output-format-metric",children:"Output Format Metric"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"output_format"})," metric evaluates how well LLM output matches the ideal charter validation format:"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Ideal Format:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\n  "is_valid": true,\n  "violations": [],\n  "encouraged_aspects": [\n    "Concrete and argued proposals",\n    "Constructive criticism",\n    "Questions and requests for clarification"\n  ],\n  "reasoning": "Clear explanation of why the contribution is valid...",\n  "confidence": 0.95\n}\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Scoring (0.0 to 1.0):"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"+0.2"})," per required field with correct type (5 fields = 1.0 max)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"-0.5"})," if reasoning contains error messages"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"-0.2"})," if reasoning is empty"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"-0.1"})," if confidence out of [0, 1] range"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"-0.1"})," if valid but no ",(0,s.jsx)(n.code,{children:"encouraged_aspects"})]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Example Scores:"})}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Output Type"}),(0,s.jsx)(n.th,{children:"Score"}),(0,s.jsx)(n.th,{children:"Reason"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Ideal format"}),(0,s.jsx)(n.td,{children:"1.00"}),(0,s.jsx)(n.td,{children:"All fields correct"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Error in reasoning"}),(0,s.jsx)(n.td,{children:"0.50"}),(0,s.jsx)(n.td,{children:'Contains "Validation error"'})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Missing fields"}),(0,s.jsx)(n.td,{children:"0.10-0.40"}),(0,s.jsx)(n.td,{children:"Partial structure"})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"workflow-components",children:"Workflow Components"}),"\n",(0,s.jsx)(n.h3,{id:"1-workflow_datasetpy",children:"1. workflow_dataset.py"}),"\n",(0,s.jsx)(n.p,{children:"Creates Opik datasets from various sources:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from app.processors.workflows import create_dataset_from_spans\n\nresult = create_dataset_from_spans(\n    experiment_type="charter_optimization",\n    dataset_name="charter-opt-20260204",\n    max_correctness=0.7,  # Spans with Correctness < 0.7\n    max_items=50,\n    task_provider="gemini",  # Track which LLM was used\n)\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Sources:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Opik spans (filtered by Correctness feedback score)"}),"\n",(0,s.jsx)(n.li,{children:"MockupStorage records (filtered by confidence)"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-workflow_experimentpy",children:"2. workflow_experiment.py"}),"\n",(0,s.jsxs)(n.p,{children:["Runs experiments using Opik's native ",(0,s.jsx)(n.code,{children:"evaluate()"})," API:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from app.processors.workflows import OpikExperimentConfig, run_opik_experiment\n\nconfig = OpikExperimentConfig(\n    experiment_name="charter-eval-20260204",\n    dataset_name="charter-opt-20260204",\n    experiment_type="charter_optimization",\n    task_provider="gemini",  # Sidebar LLM for Forseti\n    metrics=["hallucination", "moderation"],\n)\n\nresults = run_opik_experiment(config)\n'})}),"\n",(0,s.jsx)(n.h2,{id:"dataset-item-format",children:"Dataset Item Format"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\n  "id": "unique-id",\n  "input": {\n    "title": "Contribution title...",\n    "body": "Contribution body...",\n    "category": "economie",\n    "original_confidence": 0.8,\n    "original_is_valid": true,\n    "record_id": "span_019c27cb..."\n  },\n  "expected_output": {\n    "is_valid": true,\n    "confidence_threshold": 1.0\n  },\n  "tags": [],\n  "created_at": "2026-02-04T08:34:58Z"\n}\n'})}),"\n",(0,s.jsx)(n.h2,{id:"agent-feature-registry",children:"Agent Feature Registry"}),"\n",(0,s.jsxs)(n.p,{children:["Experiment types are defined in ",(0,s.jsx)(n.code,{children:"AGENT_FEATURE_REGISTRY"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'AGENT_FEATURE_REGISTRY = {\n    "charter_optimization": {\n        "agent": "forseti",\n        "feature": "charter_validation",  # Span name\n        "prompt_key": "forseti.charter_validation",\n        "dataset_prefix": "charter-optimization",\n    },\n    "category_optimization": {\n        "agent": "forseti",\n        "feature": "category_classification",\n        "prompt_key": "forseti.category_classification",\n        "dataset_prefix": "category-optimization",\n    },\n}\n'})}),"\n",(0,s.jsx)(n.p,{children:"To add a new optimizable feature:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Ensure the feature logs ",(0,s.jsx)(n.code,{children:"Correctness"})," feedback via ",(0,s.jsx)(n.code,{children:"tracer.log_span_feedback()"})]}),"\n",(0,s.jsxs)(n.li,{children:["Add ",(0,s.jsx)(n.code,{children:"added_to_dataset: False"})," metadata to the span"]}),"\n",(0,s.jsxs)(n.li,{children:["Register in ",(0,s.jsx)(n.code,{children:"AGENT_FEATURE_REGISTRY"})]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"pre-experiment-cleanup",children:"Pre-experiment Cleanup"}),"\n",(0,s.jsx)(n.p,{children:"Before running experiments, error traces can pollute optimization and diverge results. The workflow includes an optional cleanup step."}),"\n",(0,s.jsx)(n.h3,{id:"error-patterns-detected",children:"Error Patterns Detected"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:'"error"'})," - Generic errors"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:'"retries exhausted"'})," - API retry failures"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:'"rate limit"'})," - Rate limiting errors"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:'"validation error"'})," - LLM validation failures"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:'"timeout"'})," - Request timeouts"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:'"failed"'})," - Generic failures"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"using-cleanup",children:"Using Cleanup"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from app.processors.workflows.workflow_experiment import cleanup_error_traces\n\n# Dry run (preview only)\nresult = cleanup_error_traces(dry_run=True)\nprint(f\"Would delete {result['error_traces']} traces\")\n\n# Actual cleanup\nresult = cleanup_error_traces()\nprint(f\"Deleted {result['deleted']} error traces\")\n"})}),"\n",(0,s.jsx)(n.h3,{id:"cleanup-in-experiments",children:"Cleanup in Experiments"}),"\n",(0,s.jsxs)(n.p,{children:["Cleanup is enabled by default in ",(0,s.jsx)(n.code,{children:"OpikExperimentConfig"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'config = OpikExperimentConfig(\n    experiment_name="...",\n    dataset_name="...",\n    experiment_type="charter_optimization",\n    cleanup_errors=True,  # Default: True\n)\n'})}),"\n",(0,s.jsx)(n.h2,{id:"evaluation-flow",children:"Evaluation Flow"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"1. (Optional) Cleanup error traces\n        \u2193\n2. Load dataset from Opik\n        \u2193\n3. For each dataset item:\n   a. Run evaluation_task (Forseti with task_provider LLM)\n   b. Get output: {input, output, context, is_valid, confidence}\n        \u2193\n4. Opik evaluate() with scoring_metrics (judge_provider LLM)\n   - Hallucination metric\n   - Moderation metric\n   - output_format metric (custom)\n   - Other custom metrics\n        \u2193\n5. Results logged to Opik platform\n   - Per-item scores\n   - Aggregate statistics\n"})}),"\n",(0,s.jsx)(n.h2,{id:"admin-dashboard-integration",children:"Admin Dashboard Integration"}),"\n",(0,s.jsx)(n.p,{children:"The Admin tab includes:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Opik Judge LLM Configuration"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Provider selection (OpenAI, Anthropic)"}),"\n",(0,s.jsx)(n.li,{children:"Model selection (gpt-4o-mini, gpt-4o, claude-3-haiku)"}),"\n",(0,s.jsx)(n.li,{children:"API key status indicator"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Experiment Type Selection"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Dynamic from AGENT_FEATURE_REGISTRY"}),"\n",(0,s.jsx)(n.li,{children:"Shows agent and feature info"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Opik Span Statistics"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Total spans by type"}),"\n",(0,s.jsx)(n.li,{children:"Spans with Correctness below threshold"}),"\n",(0,s.jsx)(n.li,{children:"Already added to dataset count"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"scheduled-task",children:"Scheduled Task"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"task_opik_evaluate"})," runs periodically to handle async span ingestion:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Cron: Every 30 minutes, 7 AM - 10 PM\nOPIK_EVALUATE_CRON = "*/30 7-22 * * *"\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Workflow:"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Cleanup error traces (optional)"}),"\n",(0,s.jsx)(n.li,{children:"Search for recent spans not yet in dataset"}),"\n",(0,s.jsx)(n.li,{children:"Create dataset from new spans"}),"\n",(0,s.jsx)(n.li,{children:"Run Opik evaluate()"}),"\n",(0,s.jsx)(n.li,{children:"Mark spans as processed"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["See ",(0,s.jsx)(n.a,{href:"/fr/docs/app/scheduler/tasks/TASK_OPIK_EVALUATE",children:"Task: Opik Evaluate"})," for details."]}),"\n",(0,s.jsx)(n.h2,{id:"files",children:"Files"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"File"}),(0,s.jsx)(n.th,{children:"Purpose"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"app/processors/workflows/workflow_dataset.py"})}),(0,s.jsx)(n.td,{children:"Dataset creation"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"app/processors/workflows/workflow_experiment.py"})}),(0,s.jsx)(n.td,{children:"Experiment execution, cleanup, custom metrics"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"app/services/opik_config.py"})}),(0,s.jsx)(n.td,{children:"Judge LLM config (Redis db=5)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"app/services/tasks/__init__.py"})}),(0,s.jsx)(n.td,{children:"AGENT_FEATURE_REGISTRY"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"app/services/tasks/task_opik_evaluate.py"})}),(0,s.jsx)(n.td,{children:"Scheduled evaluation task"})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.comet.com/docs/opik/evaluation/metrics/overview",children:"Opik Evaluation Metrics"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.comet.com/docs/opik/evaluation/metrics/custom_metric",children:"Opik Custom Metrics"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.comet.com/docs/opik/evaluation/manage_datasets",children:"Opik Datasets"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(o,{...e})}):o(e)}},8453(e,n,i){i.d(n,{R:()=>l,x:()=>d});var r=i(6540);const s={},t=r.createContext(s);function l(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);