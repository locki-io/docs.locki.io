"use strict";(globalThis.webpackChunkdocs_locki_io=globalThis.webpackChunkdocs_locki_io||[]).push([[853],{5233(e){e.exports=JSON.parse('{"permalink":"/fr/blog/opik-workshop-1","editUrl":"https://github.com/locki-io/docs.locki.io/tree/main/blog/2026-01-14-OPIK-workshop.mdx","source":"@site/blog/2026-01-14-OPIK-workshop.mdx","title":"OPIK : AI Evaluation and Observability","description":"This lecture, led by Abby Morgan, an AI Research Engineer, introduces AI evaluation as a systematic feedback loop for transitioning prototypes to production-ready systems. It outlines the four key components of a useful evaluation: a target capability, a test set, a scoring method, and decision rules. The session differentiates between general benchmarks and specific product evaluations, emphasizing the need for observability in agent evaluation. It demonstrates using OPIK, an open-source tool, to track, debug, and evaluate LLM agents through features like traces, spans, \'LM as a judge\', and regression testing datasets.","date":"2026-01-14T00:00:00.000Z","tags":[{"inline":false,"label":"AI and Machine Learning","permalink":"/fr/blog/tags/ai-ml","description":"Articles on AI, machine learning, and related technologies"},{"inline":false,"label":"civitech","permalink":"/fr/blog/tags/civictech","description":"Citizen technologies and open source for the public good"},{"inline":false,"label":"Encode hackathon","permalink":"/fr/blog/tags/encode","description":"Information from the Encode hackathon"},{"inline":false,"label":"Observability","permalink":"/fr/blog/tags/observability","description":"Articles on observability practices and tools"}],"readingTime":17.25,"hasTruncateMarker":true,"authors":[{"name":"Jean-No\xebl Schilling","title":"Locki one / french maintainer","url":"https://github.com/jnschilling","socials":{"linkedin":"https://www.linkedin.com/in/jnschilling/","github":"https://github.com/jnschilling","newsletter":"https://www.stratej.fr"},"imageURL":"https://github.com/jnschilling.png","key":"jnxmas","page":null}],"frontMatter":{"slug":"opik-workshop-1","title":"OPIK : AI Evaluation and Observability","authors":["jnxmas"],"tags":["ai-ml","civictech","encode","observability"]},"unlisted":false,"prevItem":{"title":"OPIK : Agent & Prompt Optimization for LLM Systems","permalink":"/fr/blog/opik-workshop-2"},"nextItem":{"title":"Project scope - Victor + JNS","permalink":"/fr/blog/victor-intro-project-description"}}')},7450(e,t,n){n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>s,metadata:()=>i,toc:()=>c});var i=n(5233),o=n(4848),a=n(8453);const s={slug:"opik-workshop-1",title:"OPIK : AI Evaluation and Observability",authors:["jnxmas"],tags:["ai-ml","civictech","encode","observability"]},r=void 0,l={authorsImageUrls:[void 0]},c=[];function u(e){const t={p:"p",...(0,a.R)(),...e.components};return(0,o.jsx)(t.p,{children:"This lecture, led by Abby Morgan, an AI Research Engineer, introduces AI evaluation as a systematic feedback loop for transitioning prototypes to production-ready systems. It outlines the four key components of a useful evaluation: a target capability, a test set, a scoring method, and decision rules. The session differentiates between general benchmarks and specific product evaluations, emphasizing the need for observability in agent evaluation. It demonstrates using OPIK, an open-source tool, to track, debug, and evaluate LLM agents through features like traces, spans, 'LM as a judge', and regression testing datasets."})}function p(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(u,{...e})}):u(e)}},8453(e,t,n){n.d(t,{R:()=>s,x:()=>r});var i=n(6540);const o={},a=i.createContext(o);function s(e){const t=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(a.Provider,{value:t},e.children)}}}]);