<!doctype html>
<html lang="fr-FR" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">OPIK : AI Evaluation and Observability | Ò Capistaine Docs</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://docs.locki.io/fr/img/ocapistaine-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://docs.locki.io/fr/img/ocapistaine-social-card.jpg"><meta data-rh="true" property="og:url" content="https://docs.locki.io/fr/blog/opik-workshop-1"><meta data-rh="true" property="og:locale" content="fr_FR"><meta data-rh="true" property="og:locale:alternate" content="en_US"><meta data-rh="true" name="docusaurus_locale" content="fr"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="fr"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="OPIK : AI Evaluation and Observability | Ò Capistaine Docs"><meta data-rh="true" name="description" content="This lecture, led by Abby Morgan, an AI Research Engineer, introduces AI evaluation as a systematic feedback loop for transitioning prototypes to production-ready systems. It outlines the four key components of a useful evaluation: a target capability, a test set, a scoring method, and decision rules. The session differentiates between general benchmarks and specific product evaluations, emphasizing the need for observability in agent evaluation. It demonstrates using OPIK, an open-source tool, to track, debug, and evaluate LLM agents through features like traces, spans, &#x27;LM as a judge&#x27;, and regression testing datasets."><meta data-rh="true" property="og:description" content="This lecture, led by Abby Morgan, an AI Research Engineer, introduces AI evaluation as a systematic feedback loop for transitioning prototypes to production-ready systems. It outlines the four key components of a useful evaluation: a target capability, a test set, a scoring method, and decision rules. The session differentiates between general benchmarks and specific product evaluations, emphasizing the need for observability in agent evaluation. It demonstrates using OPIK, an open-source tool, to track, debug, and evaluate LLM agents through features like traces, spans, &#x27;LM as a judge&#x27;, and regression testing datasets."><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2026-01-14T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://github.com/jnschilling"><meta data-rh="true" property="article:tag" content="AI and Machine Learning,civitech,Encode hackathon,Observability"><link data-rh="true" rel="icon" href="/fr/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://docs.locki.io/fr/blog/opik-workshop-1"><link data-rh="true" rel="alternate" href="https://docs.locki.io/blog/opik-workshop-1" hreflang="en-US"><link data-rh="true" rel="alternate" href="https://docs.locki.io/fr/blog/opik-workshop-1" hreflang="fr-FR"><link data-rh="true" rel="alternate" href="https://docs.locki.io/blog/opik-workshop-1" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://docs.locki.io/fr/blog/opik-workshop-1","mainEntityOfPage":"https://docs.locki.io/fr/blog/opik-workshop-1","url":"https://docs.locki.io/fr/blog/opik-workshop-1","headline":"OPIK : AI Evaluation and Observability","name":"OPIK : AI Evaluation and Observability","description":"This lecture, led by Abby Morgan, an AI Research Engineer, introduces AI evaluation as a systematic feedback loop for transitioning prototypes to production-ready systems. It outlines the four key components of a useful evaluation: a target capability, a test set, a scoring method, and decision rules. The session differentiates between general benchmarks and specific product evaluations, emphasizing the need for observability in agent evaluation. It demonstrates using OPIK, an open-source tool, to track, debug, and evaluate LLM agents through features like traces, spans, 'LM as a judge', and regression testing datasets.","datePublished":"2026-01-14T00:00:00.000Z","author":{"@type":"Person","name":"Jean-Noël Schilling","description":"Locki one / french maintainer","url":"https://github.com/jnschilling","image":"https://github.com/jnschilling.png"},"keywords":[],"isPartOf":{"@type":"Blog","@id":"https://docs.locki.io/fr/blog","name":"Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/fr/blog/rss.xml" title="Ò Capistaine Docs RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/fr/blog/atom.xml" title="Ò Capistaine Docs Atom Feed"><link rel="stylesheet" href="/fr/assets/css/styles.cc893cbc.css">
<script src="/fr/assets/js/runtime~main.11305026.js" defer="defer"></script>
<script src="/fr/assets/js/main.7c795b50.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/fr/img/logo.svg"><link rel="preload" as="image" href="https://github.com/jnschilling.png"><div role="region" aria-label="Aller au contenu principal"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Aller au contenu principal</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Ouvrir/fermer la barre de navigation" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/fr/"><div class="navbar__logo"><img src="/fr/img/logo.svg" alt="OCapistaine Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/fr/img/logo.svg" alt="OCapistaine Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Ò Capistaine</b></a><a class="navbar__item navbar__link" href="/fr/docs/audierne2026/overview">Audierne2026</a><a class="navbar__item navbar__link" href="/fr/docs/usage/getting-started">Usage</a><a class="navbar__item navbar__link" href="/fr/docs/methods/triz">Methods</a><a class="navbar__item navbar__link" href="/fr/docs/workflows/consolidation">Workflows</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/fr/blog">Blog</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>Français</a><ul class="dropdown__menu"><li><a href="/blog/opik-workshop-1" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en-US">English</a></li><li><a href="/fr/blog/opik-workshop-1" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="fr-FR">Français</a></li></ul></div><a href="https://github.com/locki-io/ocapistaine" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Basculer entre le mode sombre et clair (actuellement system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Navigation article de blog récent"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><div role="group"><h3 class="yearGroupHeading_rMGB">2026</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/fr/blog/meeting-ocapistaine-3">Catch-up Call: Deployment Strategy</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/fr/blog/gemini-workshop-1">Technical Strategy: Google Gemini Integration</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/fr/blog/meeting-ocapistaine-2">Team Sync: Gemini Integration and Agent Workflows</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/fr/blog/meeting-ocapistaine-1">Let us choose the stack</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/fr/blog/meher-jnxmas-letscode">Lets us code</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/fr/blog/ocapistaine-hackathon-kickoff">Ò Capistaine Kick-off</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/fr/blog/opik-workshop-2">OPIK : Agent &amp; Prompt Optimization for LLM Systems</a></li><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/fr/blog/opik-workshop-1">OPIK : AI Evaluation and Observability</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/fr/blog/victor-intro-project-description">Project scope - Victor + JNS</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/fr/blog/encode-kickoff">Hackathon kickoff meeting</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2025</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/fr/blog/locki-in-2025">Locki Labs in 2025 - Introducing Valkyria</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2024</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/fr/blog/locki-journey-2023-2025">Locki Labs: From Hackathon to Production - Navigating Blockchain Evolution and Data Challenges</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2021</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/fr/blog/welcome">Locki blog introduction</a></li></ul></div></nav></aside><main class="col col--7"><article class=""><header><h1 class="title_f1Hy">OPIK : AI Evaluation and Observability</h1><div class="container_mt6G margin-vert--md"><time datetime="2026-01-14T00:00:00.000Z">14 janvier 2026</time> · <!-- -->18 minutes de lecture</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/jnschilling" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://github.com/jnschilling.png" alt="Jean-Noël Schilling"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/jnschilling" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp" translate="no">Jean-Noël Schilling</span></a></div><small class="authorTitle_nd0D" title="Locki one / french maintainer">Locki one / french maintainer</small><div class="authorSocials_rSDt"><a href="https://www.linkedin.com/in/jnschilling/" target="_blank" rel="noopener noreferrer" class="authorSocialLink_owbf" title="LinkedIn"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" preserveAspectRatio="xMidYMid" viewBox="0 0 256 256" style="--dark:#0a66c2;--light:#ffffffe6" class="authorSocialIcon_XYv3 linkedinSvg_FCgI"><path d="M218.123 218.127h-37.931v-59.403c0-14.165-.253-32.4-19.728-32.4-19.756 0-22.779 15.434-22.779 31.369v60.43h-37.93V95.967h36.413v16.694h.51a39.907 39.907 0 0 1 35.928-19.733c38.445 0 45.533 25.288 45.533 58.186l-.016 67.013ZM56.955 79.27c-12.157.002-22.014-9.852-22.016-22.009-.002-12.157 9.851-22.014 22.008-22.016 12.157-.003 22.014 9.851 22.016 22.008A22.013 22.013 0 0 1 56.955 79.27m18.966 138.858H37.95V95.967h37.97v122.16ZM237.033.018H18.89C8.58-.098.125 8.161-.001 18.471v219.053c.122 10.315 8.576 18.582 18.89 18.474h218.144c10.336.128 18.823-8.139 18.966-18.474V18.454c-.147-10.33-8.635-18.588-18.966-18.453"></path></svg></a><a href="https://github.com/jnschilling" target="_blank" rel="noopener noreferrer" class="authorSocialLink_owbf" title="GitHub"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 256 250" preserveAspectRatio="xMidYMid" style="--dark:#000;--light:#fff" class="authorSocialIcon_XYv3 githubSvg_Uu4N"><path d="M128.001 0C57.317 0 0 57.307 0 128.001c0 56.554 36.676 104.535 87.535 121.46 6.397 1.185 8.746-2.777 8.746-6.158 0-3.052-.12-13.135-.174-23.83-35.61 7.742-43.124-15.103-43.124-15.103-5.823-14.795-14.213-18.73-14.213-18.73-11.613-7.944.876-7.78.876-7.78 12.853.902 19.621 13.19 19.621 13.19 11.417 19.568 29.945 13.911 37.249 10.64 1.149-8.272 4.466-13.92 8.127-17.116-28.431-3.236-58.318-14.212-58.318-63.258 0-13.975 5-25.394 13.188-34.358-1.329-3.224-5.71-16.242 1.24-33.874 0 0 10.749-3.44 35.21 13.121 10.21-2.836 21.16-4.258 32.038-4.307 10.878.049 21.837 1.47 32.066 4.307 24.431-16.56 35.165-13.12 35.165-13.12 6.967 17.63 2.584 30.65 1.255 33.873 8.207 8.964 13.173 20.383 13.173 34.358 0 49.163-29.944 59.988-58.447 63.157 4.591 3.972 8.682 11.762 8.682 23.704 0 17.126-.148 30.91-.148 35.126 0 3.407 2.304 7.398 8.792 6.14C219.37 232.5 256 184.537 256 128.002 256 57.307 198.691 0 128.001 0Zm-80.06 182.34c-.282.636-1.283.827-2.194.39-.929-.417-1.45-1.284-1.15-1.922.276-.655 1.279-.838 2.205-.399.93.418 1.46 1.293 1.139 1.931Zm6.296 5.618c-.61.566-1.804.303-2.614-.591-.837-.892-.994-2.086-.375-2.66.63-.566 1.787-.301 2.626.591.838.903 1 2.088.363 2.66Zm4.32 7.188c-.785.545-2.067.034-2.86-1.104-.784-1.138-.784-2.503.017-3.05.795-.547 2.058-.055 2.861 1.075.782 1.157.782 2.522-.019 3.08Zm7.304 8.325c-.701.774-2.196.566-3.29-.49-1.119-1.032-1.43-2.496-.726-3.27.71-.776 2.213-.558 3.315.49 1.11 1.03 1.45 2.505.701 3.27Zm9.442 2.81c-.31 1.003-1.75 1.459-3.199 1.033-1.448-.439-2.395-1.613-2.103-2.626.301-1.01 1.747-1.484 3.207-1.028 1.446.436 2.396 1.602 2.095 2.622Zm10.744 1.193c.036 1.055-1.193 1.93-2.715 1.95-1.53.034-2.769-.82-2.786-1.86 0-1.065 1.202-1.932 2.733-1.958 1.522-.03 2.768.818 2.768 1.868Zm10.555-.405c.182 1.03-.875 2.088-2.387 2.37-1.485.271-2.861-.365-3.05-1.386-.184-1.056.893-2.114 2.376-2.387 1.514-.263 2.868.356 3.061 1.403Z"></path></svg></a><a href="https://www.stratej.fr" target="_blank" rel="noopener noreferrer" class="authorSocialLink_owbf" title="newsletter"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="authorSocialIcon_XYv3"><path stroke="none" d="M0 0h24v24H0z" fill="none"></path><path d="M1.2 12a10.8 10.8 0 1 0 21.6 0a10.8 10.8 0 0 0 -21.6 0"></path><path d="M1.92 8.4h20.16"></path><path d="M1.92 15.6h20.16"></path><path d="M11.4 1.2a20.4 20.4 0 0 0 0 21.6"></path><path d="M12.6 1.2a20.4 20.4 0 0 1 0 21.6"></path></svg></a></div></div></div></div></div></header><div id="__blog-post-container" class="markdown"><p>This lecture, led by Abby Morgan, an AI Research Engineer, introduces AI evaluation as a systematic feedback loop for transitioning prototypes to production-ready systems. It outlines the four key components of a useful evaluation: a target capability, a test set, a scoring method, and decision rules. The session differentiates between general benchmarks and specific product evaluations, emphasizing the need for observability in agent evaluation. It demonstrates using OPIK, an open-source tool, to track, debug, and evaluate LLM agents through features like traces, spans, &#x27;LM as a judge&#x27;, and regression testing datasets.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="takeaways">Takeaways<a href="#takeaways" class="hash-link" aria-label="Lien direct vers Takeaways" title="Lien direct vers Takeaways" translate="no">​</a></h2>
<ol>
<li class="">Introduction to Abby Morgan, an AI Research Engineer and Developer Advocate at Comet.</li>
<li class="">Housekeeping rules for the hackathon: use the public Discord channel for specific questions to ensure fairness and help others.</li>
<li class="">Recap of the previous session on AI evaluations: they turn prototypes into production-ready systems.</li>
<li class="">Evaluation is the feedback loop that enables systematic improvement, turning guesswork into a scientific process.</li>
<li class="">Evaluations help in making decisions (ship or roll back), debugging failures, and building trustworthy systems.</li>
<li class="">An evaluation is defined as a structured, repeatable measurement of system behavior against specific criteria.</li>
<li class="">Four key ingredients of a useful evaluation: a target capability, a test set reflecting the relevant world, a scoring method, and decision rules.</li>
<li class="">Evaluation outputs are not just numbers; they can include concrete examples, categorical slices, and error taxonomies.</li>
<li class="">Distinction between benchmarks and product evaluations: Benchmarks are for broad comparisons, while product evals are specific to your use case, tools, and workflows.</li>
<li class="">The importance of observability in evaluating agents: the full trace (context, prompts, tool calls) is crucial as agents don&#x27;t fail like traditional software.</li>
</ol>
<iframe width="100%" style="aspect-ratio:16 / 9" src="https://www.encodeclub.com/programmes/comet-resolution-v2-hackathon/events/intro-to-opik" title="OPIK workshop 1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin"></iframe>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="highlights">Highlights<a href="#highlights" class="hash-link" aria-label="Lien direct vers Highlights" title="Lien direct vers Highlights" translate="no">​</a></h2>
<ul>
<li class=""><code>&quot;Evaluation is essentially just the feedback loop that makes improvement thematic.&quot;-- Annie Morgan</code></li>
<li class=""><code>&quot;Evals are what turn random iteration or guesswork into more of a scientific process that allows you to improve in a systematic way.&quot;-- Annie Morgan</code></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="chapters--topics">Chapters &amp; Topics<a href="#chapters--topics" class="hash-link" aria-label="Lien direct vers Chapters &amp; Topics" title="Lien direct vers Chapters &amp; Topics" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-role-and-definition-of-ai-evaluation">The Role and Definition of AI Evaluation<a href="#the-role-and-definition-of-ai-evaluation" class="hash-link" aria-label="Lien direct vers The Role and Definition of AI Evaluation" title="Lien direct vers The Role and Definition of AI Evaluation" translate="no">​</a></h3>
<blockquote>
<p>An evaluation is a structured, repeatable measurement of system behavior against criteria we care about. This structured approach is key to distinguishing a true evaluation from a mere product demo, as it must be consistently runnable over time to interpret changes.</p>
</blockquote>
<ul>
<li class=""><strong>Keypoints</strong>
<ul>
<li class="">Evaluation turns a prototype into a production-ready system.</li>
<li class="">It provides a feedback loop for systematic improvement.</li>
<li class="">It helps make decisions like shipping or rolling back features.</li>
<li class="">An evaluation must be structured and repeatable.</li>
<li class="">It&#x27;s the difference between a product demo and a scientific process.</li>
<li class="">It helps build systems that people can trust in the real world.</li>
</ul>
</li>
<li class=""><strong>Explanation</strong>
The process of evaluation is what transforms a cool but unreliable prototype into something that can be confidently shipped and iterated upon in a real-world production environment. It provides the necessary feedback loop to make improvements systematic rather than random. It helps decide whether to ship new features, roll back changes, debug failures, and ultimately, build systems that users can trust. Without a structured evaluation process, it&#x27;s difficult to know if a system is actually improving or if observed successes are just cherry-picked examples or irrelevant outliers.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="four-ingredients-of-a-useful-evaluation">Four Ingredients of a Useful Evaluation<a href="#four-ingredients-of-a-useful-evaluation" class="hash-link" aria-label="Lien direct vers Four Ingredients of a Useful Evaluation" title="Lien direct vers Four Ingredients of a Useful Evaluation" translate="no">​</a></h3>
<blockquote>
<p>Most useful evaluations are composed of four essential ingredients: a target capability, a test set, a scoring method, and decision rules.</p>
</blockquote>
<ul>
<li class=""><strong>Keypoints</strong>
<ul>
<li class="">A target capability (e.g., fluency, relevance, toxicity).</li>
<li class="">A test set that reflects the specific world and edge cases you care about.</li>
<li class="">A scoring method, which can be human annotation or an automated metric.</li>
<li class="">Decision rules that dictate actions based on the evaluation scores (e.g., ship, roll back).</li>
</ul>
</li>
<li class=""><strong>Explanation</strong>
To create a useful evaluation, you need four components. First, a &#x27;target capability&#x27; which could be things like fluency, relevance, or toxicity. Second, a &#x27;test set&#x27; that accurately reflects the real-world scenarios you care about, including edge cases. This is where product evals differ from general benchmarks. Third, a &#x27;scoring method&#x27;, which can be either human annotation or an automated metric. Finally, &#x27;decision rules&#x27; which define what actions to take based on the evaluation scores, such as deploying a new feature or rolling it back.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="benchmarks-vs-product-evals">Benchmarks vs. Product Evals<a href="#benchmarks-vs-product-evals" class="hash-link" aria-label="Lien direct vers Benchmarks vs. Product Evals" title="Lien direct vers Benchmarks vs. Product Evals" translate="no">​</a></h3>
<blockquote>
<p>Benchmarks are standardized evaluations for broad comparisons across a wide range of use cases, whereas product evaluations are tailored to a specific use case, including its unique tools and workflows. Benchmarks are helpful for general comparisons, but are not a substitute for product-specific evaluations.</p>
</blockquote>
<ul>
<li class=""><strong>Keypoints</strong>
<ul>
<li class="">Benchmarks are standardized and cover a wide range of use cases.</li>
<li class="">Product evals are specific to your use case, tools, and workflows.</li>
<li class="">Benchmarks are for broader comparisons.</li>
<li class="">Product evals are for seriously evaluating your system for its specific purpose.</li>
<li class="">Product evals should use a test set that reflects the particular world you care about, including edge cases.</li>
</ul>
</li>
<li class=""><strong>Explanation</strong>
While the term &#x27;benchmark&#x27; is often used in AI evals, it&#x27;s important to distinguish it from a &#x27;product eval&#x27;. Benchmarks are generalized and standardized, designed to test a wide range of examples and use cases. They are useful for broader comparisons. On the other hand, a product evaluation is specific to your product&#x27;s use case. It should use a test set that reflects the world you care about, including specific edge cases, and incorporate your particular tools and workflows. For serious evaluation of your own system&#x27;s performance for its intended purpose, a product-specific eval is necessary.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="observability-as-the-foundation-for-agent-evaluation">Observability as the Foundation for Agent Evaluation<a href="#observability-as-the-foundation-for-agent-evaluation" class="hash-link" aria-label="Lien direct vers Observability as the Foundation for Agent Evaluation" title="Lien direct vers Observability as the Foundation for Agent Evaluation" translate="no">​</a></h3>
<blockquote>
<p>For AI agents, observability is the foundation of evaluation. Since agents don&#x27;t fail like traditional software, it&#x27;s crucial to look beyond the final output and observe the full trace of their operation. This includes the context retrieved, prompts used, tools called, and all intermediate steps. This observation is the first step in a cycle of improvement: observe, understand, evaluate, and then improve.</p>
</blockquote>
<ul>
<li class=""><strong>Keypoints</strong>
<ul>
<li class="">The final output of an agent is never the full story.</li>
<li class="">Observing the full trace is critical for evaluation.</li>
<li class="">The trace includes retrieved context, prompts, tool calls, and intermediate steps.</li>
<li class="">Agents do not fail like traditional software, so observation is key.</li>
<li class="">The improvement cycle is: observe -&gt; understand -&gt; evaluate -&gt; improve.</li>
<li class="">OPIK is designed to make observability and evaluation practical and simple.</li>
</ul>
</li>
<li class=""><strong>Explanation</strong>
Evaluating agents requires starting with observability because their final output alone doesn&#x27;t tell the whole story. Unlike traditional software, an agent&#x27;s failure or success is determined by a complex series of steps. Therefore, you must be able to see the full trace of its actions. This includes what context was retrieved, what prompts were generated and used, which tools were called, and all the intermediate results. Once you can observe this entire process, you can begin to understand what is happening. With understanding, you can then evaluate the agent&#x27;s performance against your criteria. Finally, with these evaluations and measurements of success, you can systematically begin to improve the agent. OPIC is a tool specifically designed to facilitate this process by making observability and evaluation practical and easy.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="integrating-opik-into-your-code">Integrating OPIK into Your Code<a href="#integrating-opik-into-your-code" class="hash-link" aria-label="Lien direct vers Integrating OPIK into Your Code" title="Lien direct vers Integrating OPIK into Your Code" translate="no">​</a></h3>
<blockquote>
<p>OPIK is an observability tool that can be easily integrated into your code to track, monitor, and evaluate the performance of LLM agents. It requires minimal code, often just one to three lines, to start logging agent interactions. The integration method might vary slightly depending on whether you&#x27;re using a direct integration like with OpenAI Agents or a more general method like the track decorator.</p>
</blockquote>
<ul>
<li class=""><strong>Keypoints</strong>
<ul>
<li class="">Import OPIK at the top of your file.</li>
<li class="">Integration can be as simple as one to three lines of code.</li>
<li class="">Direct integrations, like with OpenAI Agents, might have a specific syntax.</li>
<li class="">The &#x27;@track&#x27; decorator is another common method for integration.</li>
</ul>
</li>
<li class=""><strong>Explanation</strong>
To integrate OPIK, you first import it at the top of your script. Then, depending on the framework, you might use a specific call like the one shown for OpenAI Agents, or use a track decorator. For the demonstrated recipe generator agent, which uses OpenAI Agents, the integration was slightly different but still very simple. This minimal setup allows OPIK to automatically capture detailed information about each agent run.</li>
<li class=""><strong>Examples</strong>
<blockquote>
<p>A basic agent acting as a recipe generator. The user provides a list of ingredients, and the agent performs two LLM calls. The first LLM call suggests a recipe based on the ingredients (e.g., creamy orange tomato soup from tomatoes, cream, and oranges). The second LLM call researches the steps to create that specific recipe.</p>
</blockquote>
<ul>
<li class="">The user runs the script and is prompted for ingredients.</li>
<li class="">The user enters &#x27;tomatoes, cream, oranges&#x27;.</li>
<li class="">The first LLM processes these ingredients and suggests &#x27;creamy orange tomato soup&#x27;.</li>
<li class="">This recipe name is then passed to the second LLM.</li>
<li class="">The second LLM researches and outputs the detailed steps for making the soup.</li>
<li class="">All of this activity is logged as a single trace in the OPIC dashboard.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="understanding-traces-and-spans">Understanding Traces and Spans<a href="#understanding-traces-and-spans" class="hash-link" aria-label="Lien direct vers Understanding Traces and Spans" title="Lien direct vers Understanding Traces and Spans" translate="no">​</a></h3>
<blockquote>
<p>OPIC organizes logged data into traces and spans. A trace represents a single, complete end-to-end process or execution of your agent. A span is a smaller, individual step or operation within that trace. This hierarchical structure allows for both a high-level overview and a granular look at each component of the agent&#x27;s execution, which is crucial for debugging and evaluation.</p>
</blockquote>
<ul>
<li class=""><strong>Keypoints</strong>
<ul>
<li class="">A trace is the entire end-to-end agentic call.</li>
<li class="">A span is an individual step within the trace.</li>
<li class="">The dashboard allows you to view the overall trace and drill down into individual spans.</li>
<li class="">This structure helps isolate where failures or issues occur in a complex agent.</li>
</ul>
</li>
<li class=""><strong>Explanation</strong>
When an agent runs, the entire operation is captured as one trace. Within this trace, you can see individual spans corresponding to specific actions, like LLM calls or tool usage. The OPIC dashboard clearly displays the input and output for the entire agent call (the trace) and for each individual step (the spans). This helps identify exactly where in the process a failure or unexpected behavior occurs. For example, if an agent fails, you can inspect the spans leading up to the failure to understand the root cause.</li>
<li class=""><strong>Examples</strong>
<blockquote>
<p>In the recipe generator agent, the entire process from taking user ingredients to outputting a full recipe is one trace. Within that trace, the first LLM call (suggesting &#x27;chicken parmesan&#x27;) is one span, and the second LLM call (researching how to make it) is another span.</p>
</blockquote>
<ul>
<li class="">User interacts with the agent, triggering a run. This entire run is a &#x27;trace&#x27;.</li>
<li class="">The agent calls the first LLM to generate a recipe idea. This call is a &#x27;span&#x27;.</li>
<li class="">The agent then calls the second LLM to get the recipe steps. This second call is another &#x27;span&#x27;.</li>
<li class="">The dashboard shows the full trace and allows clicking into it to see the individual spans, each with its own inputs, outputs, and metadata.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="online-evaluations-and-lm-as-a-judge">Online Evaluations and &#x27;LM as a Judge&#x27;<a href="#online-evaluations-and-lm-as-a-judge" class="hash-link" aria-label="Lien direct vers Online Evaluations and &#x27;LM as a Judge&#x27;" title="Lien direct vers Online Evaluations and &#x27;LM as a Judge&#x27;" translate="no">​</a></h3>
<blockquote>
<p>OPIC allows for creating online evaluations that automatically score agent performance against predefined criteria every time an agent runs. A key feature is &#x27;LM as a judge,&#x27; where one LLM is used to evaluate the output of your agent&#x27;s LLM based on a custom prompt and scoring scale you provide. This enables automated, qualitative assessment of agent outputs.</p>
</blockquote>
<ul>
<li class=""><strong>Keypoints</strong>
<ul>
<li class="">Online evaluations automatically score agent calls based on created rules.</li>
<li class="">&#x27;LM as a judge&#x27; uses an external LLM to evaluate your agent&#x27;s output.</li>
<li class="">You must provide a detailed prompt to guide the judge LLM.</li>
<li class="">Users can manually annotate traces with their own scores to calibrate the &#x27;LM as a judge&#x27; evaluator.</li>
<li class="">You can compare human scores with LLM-generated scores to improve the evaluation prompt.</li>
</ul>
</li>
<li class=""><strong>Explanation</strong>
To set up an online evaluation, you navigate to the &#x27;Online Evaluation&#x27; section in the OPIC dashboard and create a new rule. You name the rule, provide API keys for the external judging LLM (e.g., from OpenAI, Anthropic, or OpenRouter), and select the model. You then write a detailed prompt that instructs the judge LLM on how to score the output, including the context, scoring scale, and criteria. Once the rule is created, every new trace will be automatically scored against this metric, and the scores will appear in your trace list.</li>
<li class=""><strong>Examples</strong>
<blockquote>
<p>For the recipe agent, three &#x27;LM as a judge&#x27; evaluation metrics were created to assess the generated recipes. The user set up a rule by providing a prompt to a judge LLM, defining a scoring scale and the context for what constitutes a good recipe suggestion. After setup, every time the recipe agent ran, it was automatically scored by these metrics.</p>
</blockquote>
<ul>
<li class="">Go to &#x27;Online Evaluation&#x27; and click &#x27;Create New Rule&#x27;.</li>
<li class="">Name the rule (e.g., &#x27;Recipe Coherence&#x27;).</li>
<li class="">Choose a provider and model for the judge LLM (e.g., OpenAI&#x27;s GPT-4).</li>
<li class="">Write a prompt for the judge: &#x27;You are an evaluator. Rate the following recipe on a scale of 1-5 for coherence...&#x27;</li>
<li class="">Save the rule. Now, all subsequent runs of the recipe agent will have a &#x27;Recipe Coherence&#x27; score.</li>
<li class="">You can then go into a trace and manually add a human score to compare against the &#x27;LM as a judge&#x27; score, which helps in refining the evaluation prompt.</li>
</ul>
</li>
<li class=""><strong>Considerations</strong></li>
<li class="">It&#x27;s good practice to use a healthy mix of heuristic evaluation metrics and &#x27;LLM as a judge&#x27; metrics.</li>
<li class="">You may need to iterate on the prompt given to the judging LLM to ensure its evaluations align with your definition of success.</li>
<li class="">Using one LLM to judge another can be problematic, so human oversight and comparison are valuable.</li>
<li class=""><strong>Special Circumstances</strong></li>
<li class="">If you suspect the &#x27;LM as a judge&#x27; is providing inaccurate scores, you should go into the traces, manually score them yourself, and compare your scores to the AI&#x27;s. Based on the discrepancies, you can tweak the prompt given to the judge LLM to make its ratings more closely aligned with human judgment.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="managing-and-testing-with-problematic-samples">Managing and Testing with Problematic Samples<a href="#managing-and-testing-with-problematic-samples" class="hash-link" aria-label="Lien direct vers Managing and Testing with Problematic Samples" title="Lien direct vers Managing and Testing with Problematic Samples" translate="no">​</a></h3>
<blockquote>
<p>OPIC provides features to isolate problematic agent runs and use them for regression testing. When an agent produces an error or an undesirable output, you can select these &#x27;problematic samples&#x27; from your traces and add them to a named dataset. This dataset can then be used to repeatedly test your agent after making code changes, ensuring that your fixes are effective and don&#x27;t introduce new issues.</p>
</blockquote>
<ul>
<li class=""><strong>Keypoints</strong>
<ul>
<li class="">Isolate traces where the agent fails or performs poorly.</li>
<li class="">Select these problematic samples in the dashboard.</li>
<li class="">Add them to a new or existing dataset.</li>
<li class="">Use this dataset to run regression tests on your agent after making code changes.</li>
<li class="">This helps verify that improvements are effective and don&#x27;t cause regressions.</li>
</ul>
</li>
<li class=""><strong>Explanation</strong>
To do this, you would go through your list of traces in the OPIC dashboard. You can filter or sort by low evaluation scores or errors to find the problematic runs. Select the checkboxes next to these traces. Then, use the option to &#x27;add to a dataset&#x27;. You can create a new dataset (e.g., &#x27;problematic samples&#x27;) or add to an existing one. Later, when you&#x27;ve modified your agent&#x27;s code, you can run the agent specifically against the inputs from this dataset to see if the outputs have improved.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="getting-started-with-opic-and-basic-tracing">Getting Started with OPIC and Basic Tracing<a href="#getting-started-with-opic-and-basic-tracing" class="hash-link" aria-label="Lien direct vers Getting Started with OPIC and Basic Tracing" title="Lien direct vers Getting Started with OPIC and Basic Tracing" translate="no">​</a></h3>
<blockquote>
<p>OPIC (Open-Source Project for Instrumenting and Correcting AI) is an open-source tool for LLM observability. It can be self-hosted locally for free, providing high customization, or used via a 100% free cloud tier for quick setup. The demonstration and initial setup guide focus on the free cloud tier.</p>
</blockquote>
<ul>
<li class=""><strong>Keypoints</strong>
<ul>
<li class="">OPIC is 100% open source and offers a 100% free cloud tier.</li>
<li class="">Setup involves installing the package and configuring it with an API key and workspace.</li>
<li class="">The &#x27;@track&#x27; decorator is the simplest way to add tracing to any Python function.</li>
<li class="">Direct integrations for frameworks like OpenAI provide deeper observability by wrapping the client object.</li>
</ul>
</li>
<li class=""><strong>Explanation</strong>
To get started with the free cloud tier, first install OPIC using &#x27;pip install opic&#x27;. Then, configure your account by running &#x27;open configure&#x27;. This command will prompt you for your API key, which can be found in your Comet account settings. You will also confirm the workspace you want to use. You can also set these as environment variables to avoid entering them manually each time. A project name can be specified using a code snippet provided in the quick start guide.</li>
<li class=""><strong>Examples</strong>
<blockquote>
<p>A simple &#x27;Hello World&#x27; trace example can be created using the &#x27;@track&#x27; decorator from OPIC. You import &#x27;track&#x27; from OPIC and place the decorator above the definition of the function you want to monitor, such as an LLM call. This allows you to track the function&#x27;s execution without rewriting your code. Every time the decorated function is run, it will automatically log to Comet, and a link to view the trace will be provided in the output.</p>
</blockquote>
<ul>
<li class="">Import the &#x27;track&#x27; decorator: <code>from opic import track</code></li>
<li class="">Place the decorator above your function definition: <code>@track def my_function(): ...</code></li>
</ul>
</li>
<li class="">Run the function. It will automatically log its execution, inputs, and outputs to OPIC.<!-- -->
<blockquote>
<p>OPIC offers direct integrations with popular frameworks like OpenAI. Instead of using the &#x27;@track&#x27; decorator, you import the specific integration module (e.g., <code>opic.integrations.openai</code>). Then, you wrap your framework client (e.g., the OpenAI client) in an OPIC track function. This provides deep observability into all system metrics and information from the LLM call with just one or two lines of code.</p>
</blockquote>
</li>
<li class="">Import the integration: <code>import opic.integrations.openai</code></li>
<li class="">Instantiate your original client: <code>client = OpenAI()</code></li>
<li class="">Wrap the client with the OPIC function: <code>client = opic.integrations.openai.patch(client)</code></li>
<li class="">Now, all calls made using this <code>client</code> object will be automatically tracked.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="debugging-and-evaluation-with-opic">Debugging and Evaluation with OPIC<a href="#debugging-and-evaluation-with-opic" class="hash-link" aria-label="Lien direct vers Debugging and Evaluation with OPIC" title="Lien direct vers Debugging and Evaluation with OPIC" translate="no">​</a></h3>
<blockquote>
<p>OPIC provides visibility into an agent&#x27;s execution flow, helping users debug errors and identify areas for improvement. It allows you to see exactly which step in a multi-step agent failed, avoiding the need to manually sift through code or error traces.</p>
</blockquote>
<ul>
<li class=""><strong>Keypoints</strong>
<ul>
<li class="">OPIC&#x27;s trace visualization helps pinpoint the exact step where an error occurred in an agent.</li>
<li class="">This reduces debugging time by narrowing the search space from the entire codebase to a specific component (e.g., one LLM call).</li>
<li class="">Automatic evaluation metrics can be set up to flag issues like toxicity that are hard to catch manually.</li>
<li class="">The UI allows filtering the entire dataset based on evaluation flags, aggregating all relevant examples for analysis.</li>
</ul>
</li>
<li class=""><strong>Explanation</strong>
When an agent call fails, OPIC&#x27;s trace view shows the sequence of operations (e.g., LLM calls, tool calls). If a step fails, the trace stops there. For example, if an agent fails at the &#x27;recipe suggester&#x27; step, you know the problem lies within that specific LLM call. This narrows down the debugging scope significantly. For more subtle issues, you can create evaluation metrics for things like toxicity. These metrics can automatically flag problematic runs, which you can then filter and analyze in the OPIC UI. This makes it much easier to consume a lot of information quickly and distill it down to find where things are going wrong, why, and how to fix them.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="managing-evaluation-datasets-and-metrics">Managing Evaluation Datasets and Metrics<a href="#managing-evaluation-datasets-and-metrics" class="hash-link" aria-label="Lien direct vers Managing Evaluation Datasets and Metrics" title="Lien direct vers Managing Evaluation Datasets and Metrics" translate="no">​</a></h3>
<blockquote>
<p>To maintain a relevant dataset for evaluations as an application evolves, OPIC allows for dynamic management of evaluation datasets. It is also highly recommended to use a combination of evaluation methods rather than relying on a single one.</p>
</blockquote>
<ul>
<li class=""><strong>Keypoints</strong>
<ul>
<li class="">Evaluation datasets in OPIC can be dynamically updated by adding or removing traces.</li>
<li class="">If a dataset becomes irrelevant due to application changes, you can edit it or create a new one from scratch.</li>
<li class="">It is recommended to use multiple evaluation metrics, such as combining heuristic logic with an LLM-as-a-judge, for more robust evaluation.</li>
<li class="">Users are not limited to one evaluation dataset and can maintain several for different testing scenarios.</li>
</ul>
</li>
<li class=""><strong>Explanation</strong>
As your application changes (e.g., tool call names, parameters), some traces in your evaluation dataset may become obsolete. In the OPIC UI, you can easily remove irrelevant traces from an existing dataset. You can also add new, more relevant traces. If a dataset has largely lost its relevance, you can create a completely new one from scratch. Users are not limited to a single evaluation dataset and can have dozens for different purposes. For the evaluation itself, you can and should use multiple metrics. For example, you can evaluate a model using a combination of heuristic logic and an LLM-as-a-judge.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="assignments--suggestions">Assignments &amp; Suggestions<a href="#assignments--suggestions" class="hash-link" aria-label="Lien direct vers Assignments &amp; Suggestions" title="Lien direct vers Assignments &amp; Suggestions" translate="no">​</a></h2>
<ul>
<li class="">Include the source code file (like a cursor) or basic setup steps in your final submission if you use AI tools like n8n, so judges can confirm the project works as intended.</li>
<li class="">For detailed or &#x27;deep divey&#x27; questions about OPIC, post them on the Discord server for a more thorough answer.</li>
<li class="">For questions about specific SDKs and the functional differences between various OPIC integration methods, post them on Discord.</li>
</ul></div><footer class="docusaurus-mt-lg"><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><b>Tags :</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" title="Articles on AI, machine learning, and related technologies" class="tag_zVej tagRegular_sFm0" href="/fr/blog/tags/ai-ml">AI and Machine Learning</a></li><li class="tag_QGVx"><a rel="tag" title="Citizen technologies and open source for the public good" class="tag_zVej tagRegular_sFm0" href="/fr/blog/tags/civictech">civitech</a></li><li class="tag_QGVx"><a rel="tag" title="Information from the Encode hackathon" class="tag_zVej tagRegular_sFm0" href="/fr/blog/tags/encode">Encode hackathon</a></li><li class="tag_QGVx"><a rel="tag" title="Articles on observability practices and tools" class="tag_zVej tagRegular_sFm0" href="/fr/blog/tags/observability">Observability</a></li></ul></div></div><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/locki-io/docs.locki.io/tree/main/blog/2026-01-14-OPIK-workshop.mdx" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Éditer cette page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Pagination des articles du blog"><a class="pagination-nav__link pagination-nav__link--prev" href="/fr/blog/opik-workshop-2"><div class="pagination-nav__sublabel">Article plus récent</div><div class="pagination-nav__label">OPIK : Agent &amp; Prompt Optimization for LLM Systems</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/fr/blog/victor-intro-project-description"><div class="pagination-nav__sublabel">Article plus ancien</div><div class="pagination-nav__label">Project scope - Victor + JNS</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#takeaways" class="table-of-contents__link toc-highlight">Takeaways</a></li><li><a href="#highlights" class="table-of-contents__link toc-highlight">Highlights</a></li><li><a href="#chapters--topics" class="table-of-contents__link toc-highlight">Chapters &amp; Topics</a><ul><li><a href="#the-role-and-definition-of-ai-evaluation" class="table-of-contents__link toc-highlight">The Role and Definition of AI Evaluation</a></li><li><a href="#four-ingredients-of-a-useful-evaluation" class="table-of-contents__link toc-highlight">Four Ingredients of a Useful Evaluation</a></li><li><a href="#benchmarks-vs-product-evals" class="table-of-contents__link toc-highlight">Benchmarks vs. Product Evals</a></li><li><a href="#observability-as-the-foundation-for-agent-evaluation" class="table-of-contents__link toc-highlight">Observability as the Foundation for Agent Evaluation</a></li><li><a href="#integrating-opik-into-your-code" class="table-of-contents__link toc-highlight">Integrating OPIK into Your Code</a></li><li><a href="#understanding-traces-and-spans" class="table-of-contents__link toc-highlight">Understanding Traces and Spans</a></li><li><a href="#online-evaluations-and-lm-as-a-judge" class="table-of-contents__link toc-highlight">Online Evaluations and &#39;LM as a Judge&#39;</a></li><li><a href="#managing-and-testing-with-problematic-samples" class="table-of-contents__link toc-highlight">Managing and Testing with Problematic Samples</a></li><li><a href="#getting-started-with-opic-and-basic-tracing" class="table-of-contents__link toc-highlight">Getting Started with OPIC and Basic Tracing</a></li><li><a href="#debugging-and-evaluation-with-opic" class="table-of-contents__link toc-highlight">Debugging and Evaluation with OPIC</a></li><li><a href="#managing-evaluation-datasets-and-metrics" class="table-of-contents__link toc-highlight">Managing Evaluation Datasets and Metrics</a></li></ul></li><li><a href="#assignments--suggestions" class="table-of-contents__link toc-highlight">Assignments &amp; Suggestions</a></li></ul></div></div></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Documentation</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/fr/docs/audierne2026/overview">Audierne2026</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://audierne2026.fr" target="_blank" rel="noopener noreferrer" class="footer__link-item">audierne2026.fr<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Project</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/fr/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/locki-io/ocapistaine" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://github.com/orgs/locki-io/projects/2" target="_blank" rel="noopener noreferrer" class="footer__link-item">Project Board<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2026 OCapistaine - Open Source Civic Tech. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>