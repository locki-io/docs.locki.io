# Scheduler Task Boilerplate Documentation

## Overview

The `_task_boilerplate()` function in `app/services/routines.py` provides standardized initialization, locking, and result handling for all scheduler tasks. This ensures consistent behavior across the task chain and simplifies task implementation.

## Purpose

The boilerplate handles:
1. **Task Identification**: Generates unique task IDs for logging/tracking
2. **Redis Locking**: Prevents concurrent execution of the same task
3. **Success Keys**: Tracks task completion to avoid duplicate runs
4. **Result Dictionary**: Standardized result structure with counters
5. **Skip Logic**: Automatically skips already-completed or running tasks
6. **Counter Naming**: Dynamic counter keys based on task name

## Function Signature

```python
def _task_boilerplate(task_name: str, date_string: str) -> tuple:
    """
    Shared boilerplate for task routines.

    Args:
        task_name: Full task identifier (e.g., "task_program", "task_careers")
        date_string: Date in DATE_FORMAT (DDMMYYYY, e.g., "03102025")

    Returns:
        tuple: (l: Redis conn, lock_key: str, success_key: str, result: dict, task_id: str)
    """
```

## Return Values

| Value | Type | Description |
|-------|------|-------------|
| `l` | Redis connection | APScheduler Redis connection (db=11) for locks/success keys |
| `lock_key` | str | Redis key for task locking (`lock:{task_name}:{date_string}`) |
| `success_key` | str | Redis key for completion tracking (`success:{task_name}:{date_string}`) |
| `result` | dict | Pre-initialized result dictionary with standard fields |
| `task_id` | str | Unique UUID for this task execution |

## Result Dictionary Structure

The boilerplate creates a result dictionary with:

```python
{
    "status": "success",              # success | skipped | failed | partial_success
    "date": date_string,              # Date being processed (DDMMYYYY)
    "errors": [],                     # List of error messages
    "warnings": [],                   # List of non-fatal warnings
    "failed_races": [],               # List of failed race identifiers
    "reason": None,                   # Reason for skip/failure (optional)
    "{short_name}_successes": 0,      # Number of successful operations
    "{short_name}_processed": 0,      # Total number of operations
}
```

### Counter Key Naming

Counter keys are dynamically generated by stripping the `task_` prefix:
- `task_program` → `program_successes`, `program_processed`
- `task_participants` → `participants_successes`, `participants_processed`
- `task_careers` → `careers_successes`, `careers_processed`
- `task_favorability` → `favorability_successes`, `favorability_processed`
- `task_tomorrow` → `tomorrow_successes`, `tomorrow_processed`

## Usage Pattern

### Basic Usage

```python
def task_example(date_string: str):
    """
    Example task using boilerplate.

    Args:
        date_string: Date in DDMMYYYY format

    Returns:
        dict: Task result with status and counters

    Raises:
        TaskError: For critical failures
    """
    # 1. Call boilerplate
    l, lock_key, success_key, result, task_id = _task_boilerplate(
        "task_example", date_string
    )

    # 2. Check if already skipped
    if result["status"] == "skipped":
        return result

    try:
        # 3. Do your work here
        for item in items_to_process:
            try:
                # Process item
                result["example_processed"] += 1
                result["example_successes"] += 1

            except Exception as e:
                error_msg = f"Failed processing {item}: {str(e)}"
                logger.error(error_msg)
                result["errors"].append(error_msg)
                result["example_processed"] += 1
                continue

        # 4. Check for failures
        if result["errors"]:
            error_msg = f"Task failed with {len(result['errors'])} errors"
            logger.error(error_msg)
            result["status"] = "failed"
            alog.display(result, title="Example Routine Failed")
            raise TaskError("failed", error_msg)

        # 5. Mark success
        l.set(success_key, "completed", ex=86400)
        alog.display(result, title="Example Routine Succeeded")
        return result

    except TaskError:
        raise  # Re-raise TaskErrors cleanly
    except Exception as e:
        logger.error(f"Unexpected error: {e}", exc_info=True)
        result["status"] = "failed"
        if not result["errors"]:
            result["errors"].append(f"Unexpected error: {str(e)}")
        alog.display(result, title="Example Routine Failed")
        raise TaskError("failed", f"Unexpected error: {str(e)}")
    finally:
        # 6. Always release lock
        l.delete(lock_key)
```

### Special Case: task_tomorrow

`task_tomorrow` is unique because it:
- **Locks on today's date** (when task runs)
- **Processes tomorrow's date** (actual data)
- **Combines multiple tasks** (program, participants, careers)

```python
def task_tomorrow():
    # Compute dates: lock on today, process tomorrow
    today = datetime.now(tz_timezone).strftime(DATE_FORMAT)
    tomorrow_obj = datetime.now(tz_timezone) + relativedelta(days=1)
    tomorrow = tomorrow_obj.strftime(DATE_FORMAT)

    # Use boilerplate with today's date for locking
    l, lock_key, success_key, result, task_id = _task_boilerplate(
        "task_tomorrow", today
    )

    # Override date to reflect processing date
    result["processing_date"] = tomorrow

    # Check skip early
    if result["status"] == "skipped":
        return result

    try:
        # Process tomorrow's data using `tomorrow` variable
        program = fetch_and_merge_program(tomorrow, force_update=True)

        # Use tomorrow-specific counters (from boilerplate)
        result["tomorrow_processed"] += 1
        result["tomorrow_successes"] += 1

        # ... rest of processing ...

        l.set(success_key, "completed", ex=86400)
        alog.display(result, title="Tomorrow Routine Summary")
        return result

    except TaskError:
        raise
    except Exception as e:
        error_msg = f"Critical failure in task_tomorrow for {tomorrow}: {str(e)}"
        logger.error(error_msg, exc_info=True)
        result["status"] = "failed"
        if not result["errors"]:
            result["errors"].append(error_msg)
        alog.display(result, title="Tomorrow Routine Failed")
        raise TaskError("failed", error_msg)
    finally:
        l.delete(lock_key)
```

## Skip Logic

The boilerplate automatically handles two skip scenarios:

### 1. Task Already Completed

```python
if l.exists(success_key):
    logger.info(f"Skipping {task_name}: already completed for {date_string}")
    result["status"] = "skipped"
    result["reason"] = f"Already completed on {date_string}"
    alog.display(result, title=f"{short_name.capitalize()} Routine Skipped")
    return l, lock_key, success_key, result, task_id
```

### 2. Task Already Running

```python
if not l.set(lock_key, task_id, ex=REDIS_LOCK_TIMEOUT, nx=True):
    logger.info(f"{task_name} already running for {date_string}")
    result["status"] = "skipped"
    result["reason"] = "Already running"
    alog.display(result, title=f"{short_name.capitalize()} Routine Skipped")
    return l, lock_key, success_key, result, task_id
```

**Important**: Always check if result status is "skipped" after calling boilerplate:

```python
l, lock_key, success_key, result, task_id = _task_boilerplate("task_name", date_string)

# Early return if already skipped
if result["status"] == "skipped":
    return result
```

## Redis Key Patterns

### Lock Keys
- Format: `lock:{task_name}:{date_string}`
- TTL: 300 seconds (`REDIS_LOCK_TIMEOUT`)
- Purpose: Prevent concurrent execution
- Always deleted in `finally` block

### Success Keys
- Format: `success:{task_name}:{date_string}`
- TTL: 86400 seconds (24 hours)
- Purpose: Track task completion
- Set after successful execution
- Cleared by `task_clear_log` at start of day

## Error Handling Pattern

### TaskError
Use `TaskError` for controlled failures:

```python
raise TaskError("failed", error_message)
```

### Exception Handling Template

```python
try:
    # Main work
    pass

except TaskError:
    raise  # Clean re-raise, already logged

except redis.RedisError as e:
    logger.error(f"Redis error: {e}", exc_info=True)
    result["status"] = "failed"
    result["errors"].append(f"Redis error: {str(e)}")
    alog.display(result, title="Task Failed")
    raise TaskError("failed", f"Redis error: {str(e)}")

except Exception as e:
    logger.error(f"Unexpected error: {e}", exc_info=True)
    result["status"] = "failed"
    if not result["errors"]:
        result["errors"].append(f"Unexpected error: {str(e)}")
    alog.display(result, title="Task Failed")
    raise TaskError("failed", f"Unexpected error: {str(e)}")

finally:
    l.delete(lock_key)  # ALWAYS release lock
```

## Best Practices

### 1. Always Check Skip Status

```python
l, lock_key, success_key, result, task_id = _task_boilerplate("task_name", date)

if result["status"] == "skipped":
    return result  # Early return
```

### 2. Use Correct Counter Names

```python
# task_careers → careers_*
result["careers_processed"] += 1
result["careers_successes"] += 1

# task_tomorrow → tomorrow_*
result["tomorrow_processed"] += 1
result["tomorrow_successes"] += 1
```

### 3. Track All Operations

```python
for race in races:
    try:
        # Process race
        result["careers_processed"] += 1  # Always increment

        if success:
            result["careers_successes"] += 1  # Only on success
        else:
            result["failed_races"].append(race_id)

    except Exception as e:
        result["errors"].append(str(e))
        result["careers_processed"] += 1  # Still count as processed
        result["failed_races"].append(race_id)
```

### 4. Use Appropriate Titles

```python
# Success
alog.display(result, title="Careers Routine Succeeded")

# Failure
alog.display(result, title="Careers Routine Failed")

# Skip
alog.display(result, title="Careers Routine Skipped")
```

### 5. Always Release Lock

```python
finally:
    l.delete(lock_key)  # Critical - prevents deadlocks
```

## Common Mistakes

### ❌ Wrong Counter Names

```python
# BAD: Using generic names instead of task-specific
result["processed"] += 1
result["successes"] += 1

# GOOD: Using counter names from boilerplate
result["careers_processed"] += 1
result["careers_successes"] += 1
```

### ❌ Not Checking Skip Status

```python
# BAD: Continuing after skip
l, lock_key, success_key, result, task_id = _task_boilerplate("task_name", date)
# ... do work anyway ...

# GOOD: Early return on skip
l, lock_key, success_key, result, task_id = _task_boilerplate("task_name", date)
if result["status"] == "skipped":
    return result
```

### ❌ Forgetting Lock Release

```python
# BAD: No finally block
try:
    # work
    l.delete(lock_key)  # Only happens on success

# GOOD: Always release
try:
    # work
finally:
    l.delete(lock_key)  # Always happens
```

### ❌ Wrong Date for task_tomorrow

```python
# BAD: Using wrong date variable
program = fetch_and_merge_program(date_string, ...)  # date_string is today!

# GOOD: Using tomorrow variable
program = fetch_and_merge_program(tomorrow, ...)
```

## Integration with Task Chain

Tasks are orchestrated in `app/services/scheduler.py`:

```python
task_chain = [
    {
        "id": "task_careers",
        "func": task_careers,
        "args": [today],
        "depends_on": ["task_participants"],
    },
    {
        "id": "task_favorability",
        "func": task_favorability,
        "args": [today],
        "depends_on": ["task_careers"],
    },
    {
        "id": "task_tomorrow",
        "func": task_tomorrow,
        "args": [],  # No args - computes dates internally
        "depends_on": ["task_prepare_tomorrow"],
    },
]
```

Each task:
1. Runs only if dependencies succeeded
2. Uses boilerplate for initialization
3. Skips if already completed or running
4. Reports results via alog.display()
5. Raises TaskError on failure
6. Sets success key on completion

## Logging and Display

### Task Start
```python
logger.info(f"Starting {task_name} routine task {task_id} in process {os.getpid()}")
```

### Task Skip
```python
logger.info(f"Skipping {task_name}: already completed for {date_string}")
alog.display(result, title=f"{short_name.capitalize()} Routine Skipped")
```

### Task Success
```python
logger.info(f"Completed {task_name} for {date_string}: {successes}/{processed}")
alog.display(result, title=f"{short_name.capitalize()} Routine Succeeded")
```

### Task Failure
```python
logger.error(f"{task_name} failed: {error_msg}")
alog.display(result, title=f"{short_name.capitalize()} Routine Failed")
```

## Summary

The task boilerplate provides:
- ✅ Consistent initialization across all tasks
- ✅ Automatic locking and skip logic
- ✅ Standardized result structure
- ✅ Dynamic counter naming
- ✅ Clear logging patterns
- ✅ Error handling framework

Use it for every scheduler task to ensure reliability, consistency, and maintainability.
